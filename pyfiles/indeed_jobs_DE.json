[
  {
    "title": "Data Engineer",
    "company": "Sprout Social",
    "location": "Remote in Chicago, IL",
    "salary": "$133,056 - $199,584 a year",
    "url": "https://www.indeed.com/rc/clk?jk=51d87df22e512e8b&bb=dfT8O1z5LXWQ4TkbjdY1dMKEpWgK1x3_EIkZ1jtM32UveqWFL71cewRJiNwGOjdEuXUj4rK8uYoVN3z-5Ud2gEaIwkeGyM3bFCyF29ualfHPFH2QU2JKt9x2F834BxEbwdeUIFpx6tWFdz0Of3tacg%3D%3D&xkcb=SoAY67M3sjH-L1xUJJ0NbzkdCdPP&fccid=9d8433ffeb95e981&vjs=3",
    "description": "Sprout Social is looking for a Data Engineer to join our Data Foundations team. This team builds the internal data infrastructure, pipelines, and products that empower analytics, data science, and business stakeholders across Sprout. While our software engineers are focused on delivering customer-facing platform features, our data engineers specialize in ensuring data is reliable, well-modeled, and accessible to fuel smarter decisions and internal innovation.\nWhy join Sprout Social's Data Engineering team?\nSprout Social empowers businesses worldwide to harness the immense potential of social media in today's digital-first world. Processing over one billion social messages daily, our platform delivers insights and actionable intelligence to more than 30,000 brands. These insights guide strategic decisions, drive growth, and foster deeper connections with customers.\nOur Data Foundations team plays a critical role in this by enabling Sprout's internal stakeholders—analytics, product, finance, sales, and beyond—to work with trustworthy, scalable, and reusable data. You'll be helping build the pipelines, curated datasets, and data products that unlock value across the business and extend Sprout's data-driven culture.\nWhat you'll do\nDesign and maintain ETL/ELT pipelines and data workflows that enable reliable, timely, and scalable data flows.\nCollaborate with analysts, scientists, and business stakeholders to deliver curated datasets and internal data products.\nImplement best practices in schema design, data modeling, and metadata management.\nOwn and evolve internal data infrastructure for quality, monitoring, and discoverability.\nPartner with software engineering teams where application data intersects with internal pipelines—ensuring business-critical data is clean, structured, and usable.\nWhat you'll bring\nWe're looking for a data engineer with a strong foundation in data infrastructure and a passion for enabling others to succeed through high-quality data.\nThe minimum qualifications for this role include:\n2+ years of professional experience in data engineering or at least 2 years of hands on experience building, deploying and maintaining production-grade data infrastructure and pipelines\nDemonstrated proficiency in SQL (e.g. MySQL, PostgreSQL) and data modeling; with experience in a variety of business domains. Proficiency in Python.\nExperience working in relational and non-relational databases\nHands on experience with ELT + transformation frameworks (e.g., dbt) and with orchestrators (e.g., Airflow, dbt. dagster).\nExperience building internal data products (curated datasets, semantic layers, or reusable modeling frameworks).\nProven experience applying standard software development practice to data engineering, including testing, version control, code reviews, incident management, incident CI/CD, documentation, and observability for data.\nPreferred qualifications for this role include:\nExperience building and maintaining data infrastructure using transformations with dbt or similar tools(and managing semantic layers for Business intelligence (BI) dashboards (e.g. Tableau, Hex, Looker).\nExperience implementing data quality frameworks, testing methodologies, and monitoring practices to ensure data integrity.\nHands-on experience with event-driven or streaming frameworks (Kafka or NSQ, Kinesis, Pub/Sub).\nProven ability to collaborate withcross-functional teams and effectively communicate complex technical concepts to non-technical stakeholders.\nDirect experience with cloud infrastructure (AWS, GCP, or Azure) and implementing cost optimization strategies for data platforms.\nHow you'll grow\nWithin 1 month, you'll plant your roots, including:\nComplete Sprout's New Hire onboarding program and meet peers across Data Foundations, Data Science, Engineering.\nLearn the team's existing data stack, pipelines, and modeling frameworks.\nShadow teammates to understand how internal stakeholders use curated datasets today.\nPartner with your manager to scope your first pipeline or modeling task to own.\nWithin 3 months, you'll start hitting your stride by:\nDelivering your first production-ready pipeline, dataset, or internal data product.\nCollaborating with analysts and data scientists on requirements for reusable modeling layers.\nGaining deeper familiarity with Sprout's data warehouse(s) and orchestration environment—and starting to suggest improvements.\nWithin 12 months, you'll make this role your own by:\nTaking technical ownership of a set of pipelines or data products relied on by stakeholders across the business.\nProposing and implementing improvements to our pipelines for scalability, reliability, or cost efficiency.\nActing as a mentor to newer members of the Data Foundations team while continuing to grow your own expertise.\nHelping shape the team's roadmap and long-term data foundations strategy.\nOf course what is outlined above is the ideal timeline, but things may shift based on business needs and other projects and tasks could be added at the discretion of your manager.\nOur Benefits Program\nWe're proud to regularly be recognized for our team, product and culture. Our benefits program includes:\nInsurance and benefit options that are built for both individuals and families\nProgressive policies to support work/life balance, like our flexible paid time off and parental leave program\nHigh-quality and well-maintained equipment—your computer will never prevent you from doing your best\nWellness initiatives to ensure both health and mental well-being of our team\nOngoing education and development opportunities via our Grow@Sprout program and employee-led diversity, equity and inclusion initiatives.\nGrowing corporate social responsibility program that is driven by the involvement and passion of our team members\nBeautiful, convenient and state-of-the-art offices in Chicago's Loop and downtown Seattle, for those who prefer an office setting\nWhenever possible, Sprout wants to provide our team with the flexibility to work in the location that makes the most sense for them. Sprout maintains a remote workforce in many places in the United States. However, we are not set up in all states, so please look at the drop-down box in our application to see whether your state is listed. Few roles require an office setting. If your position requires a physical presence in a Sprout office, it will be evident in the job listing and your offer letter.\n\nIndividual base pay is based on various factors, including work location, relevant experience and skills, the responsibility of the role, and job duties/requirements. In the United States, we have two geographic pay zones. You can confirm the pay zone for your specific location with your recruiter during your interview process. For this role, our current base pay ranges for new hires in each zone are:\nZone 1 (New York, California, Washington): $133,056 (min), $166,320 (mid), $199,584 (max) USD annually\nZone 2 (All other US states): $121,000 (min), $151,200 (mid), $181,400 (max) USD annually\nThe listed ranges represent the full earning potential in this position. Starting salaries for well-qualified new hires are typically around the midpoint of the range. These ranges were determined by a market-based compensation approach; we used data from trusted third-party compensation sources to set equitable, consistent, and competitive ranges. We also evaluate compensation bi-annually, identify any changes in the market and make adjustments to our ranges and existing employee compensation as needed.\nBase pay is only one element of an employee's total compensation at Sprout. Every Sprout team member has an opportunity to receive restricted stock units (RSUs) under Sprout's equity plan. Employees (and their dependents) are covered by medical, dental, vision, basic life, accidental death, and dismemberment insurance, and Modern Health (a wellness benefit). Employees are able to enroll in Sprout's company's 401k plan, in which Sprout will match 50% of your contributions up to 6% with a maximum contribution. Sprout offers \"Flexible Paid Time Off\" and ten paid holidays. We have outlined the various components to an employee's full compensation package here to help you to understand our total rewards package.\nSprout Social is proud to be an Equal Opportunity Employer and an Affirmative Action Employer. We do not discriminate based on identity- race, color, religion, national origin or ancestry, sex (including sexual identity), age, physical or mental disability, pregnancy, veteran or military status, unfavorable discharge from military service, genetic information, sexual orientation, marital status, order of protection status, citizenship status, arrest record or expunged or sealed convictions, or any other legally recognized protected basis under federal, state, or local law. Learn more about our commitment to diversity, equity and inclusion in our latest DEI Report.\nIf you require a reasonable accommodation for any part of the interview process or to submit your application, please email us at accommodations@sproutsocial.com. Include the nature of your request and your preferred contact information. We'll do everything we can to support your success during our recruitment process while upholding your privacy. Please note that only inquiries regarding accommodations will receive a response from this email address; other inquiries will not be addressed (e.g., you send your resume but are not requesting an accommodation).\nFor more information about our commitment to equal employment opportunity, please click here (1) Equal Opportunity Employment Poster (2) Sprout Social's Affirmative Action Statement (3) Pay Transparency Statement.\nAdditionally, Sprout Social participates in the E-Verify program in certain locations, as required by law.\n#LI-REMOTE\nSprout Social Inc. and its subsidiaries process personal data submitted through your application to assess your qualifications for employment and to inform our hiring decision and, where applicable, for required governmental reporting. For more information, please review Sprout's Global Applicant Privacy Notice."
  },
  {
    "title": "Data Engineer",
    "company": "AES Drilling Fluids LLC",
    "location": "Midland, TX 79707",
    "salary": "Full-time",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0D38RiyXjzyvoa5n5mI43Mn1tdqlIXTn97ZbUeNWag3OwiZ4caMYPw-kVigN_niHf1vd9Nw9HoLHMYsDoMAmGZeshJrylnA-qbumytjUb9Zej_7c_QJN5JAd2JNK9zofRNV1nBr1I8zgmGuVShY9yOSaQdl9vyYrVPxE1vAHFhbFhkZJmcFpGEE-5L2ImjCeolHBa60KHJXgBNxtGuDSMoDweQgAqkcLSDNbgIQO5KU4DD91g93dOzhJJ96U2p1yxgURNCrrpWqBB_hqEEXE-Dgmrmlh4W4NWUjecEm-PoslZo5dn7fM1dh5lw7bpaLAgE_ncrthjSpjJrIEq3rIWrF0JwCYpP10vfSAZm_C80d4s432v4t6nXMv9qCwtuUcYN_BhfzZuccytaFQmUFi79jLViTk_e5MI96sYEVgCvTy7-Rf1YPkakG28u-pzPg_g_8ZLr9oIeBqu1e8RKfxc8pVVp8LwnZWH2dEC7weRtgH3Rb_oSpJ3WU-9YQQUu52vutzl7-IUsUjn4_mySrQ4OX_Tj1sVpIA5733zdPYR0teZEWMeAYluSzvwAXYFQKiZwUMRVMhT4xwBkNRwkBVbh7-U_gPhhZhWRuuTTQsNfOlNnPC97C03KojXMd03hNm4UK95NT8mIOVEGm8ATzj0N-ANhqEAeHZasSFHQzWg-POTpF-v-5aHfg26bL4TpaSy5t4aXGICN2vVw3G03VfIt28URk-9WW8e3Pw5u-YAhQFA==&xkcb=SoCi6_M3sjH-L1xUJJ0HbzkdCdPP&camk=4HOcmqOLYrBtrtAfGS7yiw==&p=8&fvj=0&vjs=3",
    "description": "Description:\nAES Completion Services has an opportunity for an ambitious and innovative Data Engineer to join our Engineering team in Midland, TX. This is a new role for the team that supports daily operations by ensuring accurate data collection, entry, and validation across inventory and accounting systems. This role manages high-volume data in Excel, oversees field invoicing, and assists with reporting to maintain accuracy and compliance. Key duties include reconciling data, identifying discrepancies, understanding data relationships, and translating findings for the Accounting team, especially Accounts Payable. Strong attention to detail, organization, and collaboration with field and office staff are essential in this fast-paced oilfield service environment.\nFantastic benefits!\nAffordable medical / dental / vision / dependent\nEmployer paid life insurance\nVacation / sick pay / generous holidays\n401K (6% match)\nWork Location: Midland, TX\nRESPONSIBILITIES:\nData Management & Analysis\n1. Enter, validate, and maintain operational and financial data with precision.\n2. Develop and manage Excel spreadsheets, pivot tables, and formulas to support reporting needs.\n3. Perform quality checks to ensure accuracy and consistency of field and office data.\nInvoicing Oversight\n1. Performs administrative functions, such as reviewing and writing reports, approving expenditures, enforcing rules, and making decisions about the purchase of materials or services and updates AES executive management concerning procurement and personnel needs.\n2. Monitor invoicing timelines to ensure prompt and correct billing to customers.\n3. Collaborate with field supervisors and accounting teams to resolve discrepancies\nOperational Support\n1. Assist engineers and field personnel by preparing data reports and summaries.\n2. Support compliance with company standards, customer requirements, and industry best practices.\n3. Provide ad hoc analysis and reporting as required by management Effectively uses tools to manage and build customer relationships, including entertainment, business calls, technical support, conferences, and other appropriate outreach.\nRequirements:\nBachelor’s degree in Business, Engineering Technology (or related degree) is preferred.\nStrong proficiency in Microsoft Excel (advanced formulas, pivot tables, data validation).\n3-5 years of experience in oilfield, data management, or invoicing preferred.\nExcellent attention to detail and ability to spot errors in large datasets.\nStrong organizational and communication skills with the ability to work across teams.\nKnowledge of oilfield operations and terminology is a plus.\nAES Drilling Fluids is an equal opportunity employer. All persons shall have the opportunity to be considered for employment on the basis of their qualification for the job in question without regard to their race, color, religion, sex, national origin age, disability, military/veteran status, genetic characteristics, or any other characteristic protected by applicable federal, state or local law.\nAES Drilling Fluids regrets that it is unable to sponsor employment Visas or consider individuals on time-limited Visa status for this position."
  },
  {
    "title": "Senior Data Engineer – Cloud Data Warehousing (Investments Technology)",
    "company": "Liberty Mutual",
    "location": "Hybrid work in Boston, MA",
    "salary": "$112,000 - $194,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DepOg7TDxGKZPUDK5aMJwXQn2YTYIL3PVVUU2ENsp1lRUjI0nRYCSPuDtxtiPgr7QYVp28B-cNiGxccb6_2XKOQFktpxcmjIrT3-2CmvcKMmGjXAUSe-NYgCeWrHJV9BdSvCk4NghNW31lZAJv-yyGwjr8U3oaC-hpr44gLXTyOVkPqhBH2SQiweobxAvnhXntAPiEstgj5gYfePL-sGrPeciwoNC8THI3xjePKhu1qURlDVO6bMap7P8GDlHHJlNJvNWoxPHRXcsRt1L7fSIAbv7IHJ3GLnOehxjdKTpB0pRw_i2RJs62FpHCXSULDdNIi_zHon5H_mXP_G0d2jJHHQpgPZzQpo7W7qHy9Zmq2l69hvmKk1pAWwnBoXsuNeK3m0ino3oGBaQnmUSqvV9wyQc7HByzUTWnW2M5V48h4eu286o66SVCvNkUFnvcTZjleI7O9-tMymIadmC0xJTMy1DtQL03rFrxpD9YACSWOHPKCcY37JOaytYQ5svw8W5i_ABgna3AsXTrz4mkXUl3kTGBjJGVDYHQLPpjqp4Dgcii_VFpUfjDtO6XIuZDK7l5P_r3AYhB0ZuPMaq14Jx6xMCJTYaQT8OooOR-m_mdtoBCTGgj-UmSayHX2-9HG8SHzHP7j990NFxOsRzTkJXR-LdSVu8XW9HhlEi1enD79mz3pbEvwFpTE8OLBwrVZ9Mmu3p3firDoQ0P3AKZMe0StjWb79znyQZpDpzaMySGCS4EjQeww3kY1PbDa81x_AU3VNyvy6_Jjo7ryMQv1m7yLEY27qSYfW4ZEhHEyaOdhVsqDsg4PsodkebJRDYsiTosd15I6E-JgrgWBlbRMhjZOMohsDCftIu50kIqkROYdD5EEv-PlQAENg0oW1YFW8hW0A24HjNilIhVdydwm9oBS2gXbeoyKSXXiqAf41HjOLWHP9YIpSo2PD2araEiODsAlw52zov1GQ==&xkcb=SoCY6_M3sjH-L1xUJJ0BbzkdCdPP&camk=nUmJqO2E8rhk7fk8d7xYKA==&p=6&fvj=0&vjs=3",
    "description": "Description\nAre you passionate about data and excited to shape the future of investing through innovative technology? Do you thrive in an inclusive, collaborative environment where your ideas are valued? Are you curious, adaptable, and eager to learn and innovate? If so, come build with us at Liberty Mutual Investments.\n\nLiberty Mutual Investments manages Liberty Mutual Insurance Group’s global financial assets across public and private domains, to create capital and generate income. With over $100 billion in AUM and staffed with 300+ investment, finance and operations professionals located in Boston, MA., Liberty Mutual Investments offers the best of both worlds; the look and feel of a boutique investment firm, and the reputation and financial strength of a Fortune 100 company.\n\nNote: This role has a hybrid work arrangement (2x per week) in our Boston, MA office.\n\nJob Description\nAt Liberty Mutual Investments Technology, we are seeking a highly skilled Senior Data Engineer to join one of our core data teams that is building Data Warehouse solutions in Snowflake. In this role, you will design and build scalable ETL pipelines and leverage native Snowflake capabilities to deliver high-quality, cloud-based data products. Your work will be central to powering advanced investment insights, enabling data-driven decisions, and shaping the evolution of our modern data platform. This is a high-impact opportunity to apply your technical expertise, grow your career within a collaborative, forward-thinking team, and tackle meaningful challenges that influence the future of our investment portfolio\nKey Responsibilities\nWork in a Data and Analytics squad to design and build robust data pipelines that deliver high-quality insights to our investment teams.\nCollaborate with diverse teams of data engineers and product partners who believe in continuous learning, respectful collaboration, and inclusive data solutions.\nBuild and optimize scalable, reliable, high-performance automated data pipelines and solutions using Snowflake and AWS.\nDrive data quality, governance, and observability practices throughout the development lifecycle.\nStay current with industry trends and best practices in data engineering and recommend improvements to existing processes.\nContribute to a culture of learning by mentoring junior engineers, participating in code reviews, and promoting engineering excellence and best practices.\n\nQualifications\n\nQualifications\nA bachelor’s degree in technical or business discipline, or equivalent experience.\n5+ years of experience in data engineering, with designing, building, and optimizing scalable data pipelines and architecture.\nDesign and build data provisioning workflows/pipelines, extracts, data transformations, and data integrations using Snowflake cloud-native features in an AWS cloud environment.\nIntermediate experience using Python for data transformation, integration, and automation.\nHands-on experience with modern public cloud-based data platforms—Snowflake, AWS (e.g., S3, Glue, Lambda).\nProficiency with ETL/ELT pipelines and patterns for loading data warehouses and data lakes.\nHands-on experience building and deploying CI/CD pipelines (e.g., GitHub Actions, Bamboo, or similar tools).\nBonus Skills\nExposure to the Investments or Finance domain is highly desirable.\nFamiliarity with building and consuming APIs (e.g., REST, API Gateway).\nAwareness of security best practices, including access control, authentication, and secure processing.\n\nAbout Us\nPay Philosophy: The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.\n\nAs a purpose-driven organization, Liberty Mutual is committed to fostering an environment where employees from all backgrounds can build long and meaningful careers. Through strong relationships, comprehensive benefits and continuous learning opportunities, we seek to create an environment where employees can succeed, both professionally and personally.\n\nAt Liberty Mutual, we believe progress happens when people feel secure. By providing protection for the unexpected and delivering it with care, we help people embrace today and confidently pursue tomorrow.\n\nWe are dedicated to fostering an inclusive environment where employees from all backgrounds can build long and meaningful careers. By actively seeking employee feedback and amplifying the voices of our seven Employee Resource Groups (ERGs), which are open to all, we create an environment where every individual can make a meaningful impact so we continue to meet the evolving needs of our customers.\n\nWe value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits\n\nLiberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran's status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.\n\nFair Chance Notices\nCalifornia\nLos Angeles Incorporated\nLos Angeles Unincorporated\nPhiladelphia\nSan Francisco"
  },
  {
    "title": "Data Automation Engineer",
    "company": "Albers Aerospace",
    "location": "Hybrid work in Cincinnati, OH 45241",
    "salary": "$80,000 - $95,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Ae7325nMaQpSpsdgSgAt_0t_2iRNzQSZL8jvT8GU7KZClEpsEeYbZky4YjZLofjwM41aBF11GMPpHciRIgYCna9DtH8BInKzMbcqN_Xb4nhBngl58QJuq_Yj9NdHNnI3panCCEoL4VyaosLhJV9UWNDRy4FFNhPrTE3KnEKXCmzScHnej-lnQXJ68r7tlvS9sIiKwqNZELzg7txunHttKCrGMWWT5MUlEeVv-k4h7mo4Z5OCY2Cs1GkVf-zPO6TUtzF7f4MqISuZvaWLG6WDOjWjJocBs5fV_XvNZu_vcuycnCTba7tzCeCePnGD33CQ7AfrYmwN8BjEILh5oNPFjYTiMB9zbNjj3VEKiT8gehzTt2WNdD8Y2XgvyCtSCox826n0O0yPah4I7DuUlJp0Ou7pC0yJTyM5m8H0tXMu1wW8Y288fu1fZUnHUwSHud5P9dUVc2NaSjHGbmpkWbvT0eUDEJH_TXeLg04XzARUYPGrbV3VuhwGAuBk3i8opqflsAgIdjUa2-D9A_VtFtBR15xplEHVLEVkDtyikxu2d-QS_48xoRB6iH65-_ZXWl9PJnZS-A_TTTFyFgrj__t0SyIvPe7gjzOQmHOHebKJUjbLikYWim4R56nKXwtKIkCSlqHmsq0gHeIU7e3gnRwmQGWibZBFrgigQ_QZ5roXt3SGAKbIQdKDYsmAzyqCf09W9UerHdfslHTHe1-8vHAwWd3IVApqs2bVYsrbX0_94UVUcbBah9Lpad&xkcb=SoAY6_M3sjH-L1xUJJ0bbzkdCdPP&camk=UoKtGZLa3XIFVofASNGoxQ==&p=12&fvj=0&vjs=3",
    "description": "Albers Aerospace is a professional services company currently working for the U.S. Navy, U.S. Marine Corps, and commercial clients; competing as a Service Disabled Veteran Owned Small Business (SDVOSB) under several NAICS codes. Our core competencies include systems engineering, program management, logistics, production support, aircraft maintenance, and aviation/aerospace services. We were founded in 2015 and since our establishment, we have been attracting the best talent in our fields of interest enabling us to provide cutting-edge solutions and support to our warfighters. We understand and provide expert consultation on weapons systems acquisition programs, maintenance/modernization programs, and sustainment programs. We know defense and we know aircraft systems.\n*\nJob Title: Data Automation *Engineer\nDepartment: Engineering Solutions\nReports to: Director of Engineering\nLocation: Evendale, Ohio\nFull/Part Time: Full Time\nRemote work authorized: Hybrid\nFLSA Classification: Exempt\nCBA: N/A\nEffective Date: 25 Sep 2025\nSecurity Clearance Req: Secret Eligible\n*\n_______________________________________*\nAlbers Aerospace was founded on the principles of Integrity, Commitment, & Excellence – principles tested during the founders’ time in the military – Albers Aerospace is a coalition of business units and related entities operating with the goal of delivering industry leading products and services while endeavoring to make our employee’s lives better.\nGrowing rapidly (ranked numerous times in the Aggie 100), we have established ourselves as the company to solve problems, be a partner with, and make things happen. We are laser focused on solving problems and doing it in an “out of the box” manner. We love helping our clients and our teammates be all that they were made to be. We are passionate and aggressive about getting things done!\n*\nJob Summary:*\nThis position will integrate systems, build products, and develop software services as a member of a Software Engineering team focused on engineering tool chain connectivity, digital threads, and data automation. Alongside the other team members, this engineer will implement best practices for projects, provide novel solutions to hard problems, rapidly develop prototypes, and reduce ideas to practice.\nThis position will be highly collaborative with daily interface with other Albers team members as well as engineers from our customer company. This is an on-site position embedded at our customer’s Evendale, OH facility. You will develop new data automation solutions, produce user documentation, and assist in developing and presenting user training. You will work closely with Model-Based Systems Engineers in the military jet engine design and manufacturing domain.\nThe successful candidate must be a U.S. citizen and be eligible for a security clearance. Some travel (&lt;10%) may be occasionally required.\n*\nDuties/Responsibilities:*\nCollaborate to understand key process workflows to define gaps and prioritize integration solutions.\nBuild and test data automation tools. Work with customer IT stakeholders to qualify and deploy solutions.\nProduce user documentation and provide user training on developed solutions.\nDesign, document, develop, integrate, and test software according to industry coding standards.\nUnderstand data concepts, data integration practices, APIs, containers, etc.\nExecute complex software tasks with minimal oversight or direction.\nParticipate in requirements development, project estimation, software design/code reviews, algorithm architecture alignment reviews and software performance metrics.\nTroubleshoot and fix bugs\nVerify and comply with engineering documentation standards and test procedures.\n*\nSupervisory Responsibilities:*\nNone, but mentorship of more junior team memberships is expected. You will frequently collaborate with other engineers to explore, develop, deploy, and test solutions. All of our team members learn together and from each other.\n*_\nNote:*_ The duties and responsibilities described on this document are not necessarily a comprehensive list and additional tasks may be assigned to the employee from time to time; and the scope of the job may change as necessitated by business demands.\n*\nRequired Skills/Abilities:*\nGood communications and analytical skills required\nHigh work ethic and commitment\nExperience with modern source control tools and practices\nExperience configuring and using continuous build/integration facilities\n*\nPreferred Skills/Abilities:*\nBackground/Experience in propulsion engineering capabilities and tools (e.g., PLM – Teamcenter, Systems Engineering – CAD, Cameo, NX)\nFront-end development experience including React, Vue, or related frameworks\nExperience with Containers and Virtualization technologies – configuring, deploying, and managing\nExperience with Amazon Web Services (AWS) or Azure cloud services\nEstablishing and using dependency management, source control, issue tracking, build, and test facilities in a closed or high security environment\nExperience with Modeling, Simulation, and Analysis techniques and tools\nExperience with Modbus, Discrete & Analog Signals\nExperience developing industrial systems, including control and safety interlocks\n*\nEducation, Experience and Certifications Required:*\nMinimum:\nBachelor’s Degree in Computer Science, Computer/Electrical Engineering or related STEM discipline\nMust be a U.S citizen. Must possess or be able to obtain a Secret security clearance\nPreferred:\nGraduate Degree engineering in Computer Science, Computer/Electrical Engineering or related discipline\n1-3 years of relevant work experience\n*\nPhysical Requirements:*\nProlonged periods sitting at a desk and working on a computer.\nWhile primarily an office job, may require time at an integration facility or other location.\nMust be able to lift or push 25 pounds at times.\nAbility to traverse integration & test facility.\nCONUS travel may be required from time to time.\nWillingness to participate in integration and test of industrial systems, including cable, kitting, installation.\n*\nWhat We Offer:*\nTeammates at Albers Aerospace receive a robust benefits package which includes medical, dental and vision plans, matching 401k, 15 days of PTO, 12 paid holidays, employer paid term life, AD&D, long and short-term disability, and education and certification reimbursement.\n*\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities*\nThe contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractor’s legal duty to furnish information. 41 CFR 60-1.35(c)\n*\nThe pay range for this role is:*\n80,000 - 95,000 USD per year(AAEVN)\nBenefits:\n401(k) matching\nDental insurance\nEmployee assistance program\nEmployee discount\nFlexible schedule\nFlexible spending account\nHealth insurance\nHealth savings account\nLife insurance\nPaid time off\nParental leave\nProfessional development assistance\nReferral program\nTuition reimbursement\nVision insurance\nWork Location: In person"
  },
  {
    "title": "Data Engineer (Azure Data Factory)",
    "company": "Procentrix, Inc.",
    "location": "Remote in Herndon, VA 20171",
    "salary": "$130,000 - $150,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=a7123278734a55e0&bb=dfT8O1z5LXWQ4TkbjdY1dLQb-JBn7hi_QzArtj2En97e5BGlBUmtt45s8X67TgcpBhsV_OScsQ5laGGwd8Rc-RHQ05sjfAoD2-SMHM1fGzqffe0iAdiU-FF2dK_hHcsjZwWwVtwEDwD_iDenpNCmcA%3D%3D&xkcb=SoBF67M3sjH-L1xUJJ0DbzkdCdPP&fccid=7f39976b0e3fc83c&vjs=3",
    "description": "Company Description\n\nWe offer professional services and innovative solutions that streamline business and government.\n\nJob Description\n\nAs a Data Engineer at Procentrix, you will play a key role in modernizing our data ecosystem by leading efforts to migrate on-premise data sources—including Informix databases and binary objects—into Dataverse and Azure storage. You will design, implement, and maintain Azure Data Factory (ADF) pipelines that enable seamless data movement while ensuring reliability, scalability, and compliance with security standards.\nThis role requires direct interaction with customers to review and confirm requirements, validate data migration needs, and ensure proper configuration and connectivity of the Self-hosted Integration Runtime (SHIR) for secure on-prem to cloud integration. You will collaborate with stakeholders to ensure data quality, troubleshoot issues, and provide guidance on best practices for pipeline automation, scheduling, and error handling. Your work will include building and managing data pipelines, performing transformations, handling error detection and recovery, and ensuring data is delivered efficiently for use across applications and services. Additionally, you will be responsible for configuring integrations with Dataverse, managing blob storage for binary data, and implementing automation and scheduling to support ongoing operational needs.\nThe projected compensation range for this position is $130K - $150K annualized (USD). The final salary offered will generally fall within this range and is determined by various factors, including but not limited to the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as internal pay equity, location, contract-specific affordability and other organizational requirements.\nRequired Skills\nStrong hands-on experience with Azure Data Factory (ADF), including pipeline design, data flows, and integration runtime setup.\nExperience working with on-premise databases (e.g., Informix) and migrating data into cloud environments.\nProficiency in SQL for data transformation and schema alignment.\nKnowledge of Dataverse integration using API or OData connectors.\nExperience implementing error handling, logging, and retry policies in ADF pipelines.\nFamiliarity with Azure Blob Storage and management of binary/object data.\nExperience with scheduling and automation of pipelines in ADF.\nStrong problem-solving skills, attention to detail, and ability to work independently and as part of a team.\nDesirable Skills\nExperience with Microsoft Power Platform and working with Dataverse in enterprise environments.\nFamiliarity with Azure DevOps or similar CI/CD tools for pipeline deployment and management.\nKnowledge of SharePoint integration for binary data movement.\nBackground in cloud security and access control best practices.\nPrior experience supporting large-scale data migration or modernization projects.\n\nAdditional Information\n\nProcentrix is an Equal Opportunity employer and does not discriminate on the basis of race, color, religion, gender, national origin, age, marital or veteran status, the presence of a non-job related medical condition or handicap, or any other legally protected status."
  },
  {
    "title": "Business Intelligence and Data Solutions Engineer",
    "company": "Confidential",
    "location": "Plano, TX 75093",
    "salary": "$90,000 - $100,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BO7x1q_z_euVOEdqUORtA-zPWG-khplYjr3zrXcQiIrL6GAB1iaxCqo68jn8iFXtYQiJoPy5aj7QoBeEzQtDM7X4Xo_sT75OFOM7cTaja0sBmLHFqzgRpt9L_eX5LMlHT3A57sLC1TDRbBDZtQt3mARhAEtO5wg3r_qAGNSdHB-EeIiAIYv12tX25sz6SRSdbyYeQkpsADHMwKtwvYRvr4MWUl7p9-YKG3DopNQGrAP_HR3d22oJOD_FpdiQ0wsPqcL6yD1dE4Y8hgSp9DlhxwxyYZhz1xNF2YD6yxA5B83Xkv5VCMisf0bJG-VIwUwe9mK7Svh1FEhO0IaX8CM7G1YIyRQqZjj6UXn-wyPLBZgXZwjYwuReEXcrzFXBEuyAfR1hFGF-juiKBcURA9QibOpp6SkDL7g2FcOqo7AcyQIlO2dt8e64dbPCEDLzVfNI1eCSUrKwZpJM888KY-9FwddA7ovGvuz0NFhyGoOtv7Rg7blvmwQBJASgPKlEUdEEelfXwtl7Gvr8TJGPsmmHv-dSGJQ3TsOYB_vzZytFmC07rQMXgdSNqJj_-o0M5JW1r4hLe-TTkEfQY6jhJ0BSq2Gtf-5O9x1oACbgES4fmbFxTtG5V1cwjZgVyTF3sqRIOXY9DAHnwoONDDuVWpBsArHik0cZmjcC9Gfwc872mGN_mXlzglmlqnGaYiU9gnEXLuU0lhsaJdrjsC88JSfpeCsrJmifc457c=&xkcb=SoCL6_M3sjH-L1xUJJ0FbzkdCdPP&camk=UoKtGZLa3XLZZsBOHpVD9A==&p=10&fvj=0&vjs=3",
    "description": "We are seeking a highly skilled Business Intelligence and Data Solutions Engineer with a strong background in SQL, Power BI, and enterprise data modeling. The ideal candidate will combine technical expertise with business acumen to design, implement, and optimize scalable data solutions that drive decision-making across the organization.\nKey Responsibilities\nSQL Development & Optimization: Design and optimize queries using joins, CTEs, and window functions for high-performance analytics.\nPower BI Leadership: Build and manage enterprise-grade dashboards, reports, and semantic models using DAX, Power Query, and advanced data modeling techniques.\nData Modeling & Warehousing: Architect data warehouse solutions using Snowflake, Azure Synapse, and SQL Server to support financial, operational, and executive reporting needs.\nBusiness Insight & Stakeholder Engagement: Partner with finance, operations, and executive leadership to gather requirements, define KPIs, and translate business needs into actionable BI solutions.\nAutomation & Advanced Analytics: Implement automation pipelines and ML solutions using Azure Data Factory, Power Automate, Python, and R; integrate APIs to streamline reporting and forecasting processes.\nGovernance & Performance Management: Manage BI governance, security (RLS), and Power BI Premium capacity to ensure reliability, scalability, and compliance.\nTeam Leadership: Mentor and guide data analysts and engineers, providing technical oversight while driving best practices in BI and ML development.\nQualifications\nProven track record delivering enterprise-scale BI solutions across finance, sales, and operations.\nAdvanced expertise in SQL (Joins, CTEs, window functions, optimization).\nDeep proficiency in Power BI (DAX, Power Query, dashboards, semantic modeling, RLS).\nHands-on experience with Snowflake, Azure Synapse, and data warehouse architecture.\nSkilled in automation and machine learning using Python, R, Power Automate, and Azure services.\nStrong communication skills with experience leading requirements sessions and delivering executive-ready insights.\nExperience managing BI platforms, governance frameworks, and ML lifecycle management.\nBachelor’s or Master’s degree in a technical field.\nJob Type: Full-time\nPay: $90,000.00 - $100,000.00 per year\nBenefits:\n401(k) matching\nDental insurance\nEmployee assistance program\nHealth insurance\nHealth savings account\nLife insurance\nPaid time off\nProfessional development assistance\nReferral program\nTuition reimbursement\nVision insurance\nApplication Question(s):\nAre you a US Resident or US Citizen?\nExperience:\nPower BI: 4 years (Required)\nMicrosoft SQL Server: 3 years (Preferred)\nETL: 3 years (Preferred)\nAzure: 3 years (Preferred)\nWork Location: In person"
  },
  {
    "title": "Data Engineer - Mid level (5 years experience) - NewYork",
    "company": "Hire Orbitt",
    "location": "Hybrid work in New York, NY 10017",
    "salary": "$100,000 - $150,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=f0d4751e18400ce6&bb=dfT8O1z5LXWQ4TkbjdY1dO4_sx_3wgAFdXFJ4HrDUAqE-Mun74AgNMVMeHjG8hUrNaCoQgpdXemkW-x9m6bTVXBp1PrIgMNA_Cwmt7OoWpbldKK-UK8hLQ0utEyHwV162fASv7ujfqNgU1LrWxgfWA%3D%3D&xkcb=SoDx67M3sjH-L1xUJJ0CbzkdCdPP&fccid=d98e503c1b6a0173&cmp=Hire-Orbitt&ti=Data+Engineer&vjs=3",
    "description": "Job Summary\nWe are looking for a Data Engineer to join our Data & Analytics team.\nThis role is ideal for someone with at least 5 years of experience as Data Engineer who is eager to learn, contribute, and grow into a strong data engineering professional.\nThe successful candidate will have strong skills in SQL and Python, be familiar with ETL concepts, and have the ability to work with stored procedures in SQL Server.\nExposure to Snowflake and Tableau is a plus.\nExperience in AI/ML concepts or projects will also be considered a plus.\nReporting to: Head of Data Architecture and Reporting Strategy\nKey Responsibilities\nDevelop, optimize, and maintain SQL Server stored procedures, views, and queries.\nDesign, build, and maintain ETL/ELT pipelines to move and transform data from multiple sources.\nWrite and maintain Python scripts for data wrangling, automation, and reporting.\nEnsure data quality, integrity, and consistency across platforms.\nCollaborate with analysts and business stakeholders to analyze and interpret data.\nAssist in data warehouse development and pipeline optimization within Snowflake.\nDocument technical processes, workflows, and best practices.\nQualifications\nBachelor’s degree in Computer Science, Information Systems, Data Analytics, or equivalent experience.\nStrong proficiency in SQL, with the ability to write and troubleshoot complex queries.\nHands-on experience with Python for data processing and automation.\nSkilled in ETL concepts, data modeling, and data management best practices.\nKnowledge of SQL Server stored procedures and optimization techniques.\nFamiliarity with Snowflake or other cloud-based data warehouses (a plus).\nExperience with Tableau or other BI tools (a plus).\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and collaboration skills.\nPreferred Experience\nExposure to cloud platforms such as AWS, Azure, or GCP.\nExperience with Git or other version control systems.\nExposure to AI/ML concepts, frameworks, or projects (a plus).\nUnderstanding of data governance and documentation best practices.\nJob Type: Full-time\nPay: $100,000.00 - $150,000.00 per year\nBenefits:\n401(k)\nDental insurance\nHealth insurance\nPaid time off\nVision insurance\nWork Location: Hybrid remote in New York, NY 10017"
  },
  {
    "title": "Data Automation Engineer",
    "company": "Rebel Convenience Stores",
    "location": "Upland, CA 91786",
    "salary": "$90,000 - $120,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CeGd7aKpTAVC4t6ylrV_IeD4YaOlvod3go8SnCN846B-2i7xJ7g3lr0byYIud4klgvg_v9O5DQI5a88al4WCvVPmz6MtfThn6R4GymiEEdfX5zB_B0JI7IBtmVqUWYSGlUF2BPZ0LcdOMY--a90N1MEsLT0NCSUZgq3iL2vJxCeeWZBkXCXGzrjRGSRnPFx06qQNxd-ZB4ycfjutrBgTcIFhLobLcNHVyupMnjylGm8QTp-JDWwQqSncNuwYaTwGFx72Y9NMPKH5XmvYl-ZODGy3pZNHPb87pRyyviFmIjkB4yEgk6M3ZYUV33Tf20kJOKlbx6RSTlna2A_VHT_myZryjz84JjnJKx3eXA3-1_M5gyU6lNZGg03vUocfbkG-3Stk0hfTDdb2Xv-4CZK9dE-sEBRMENLtjv7eVwZCoCtSKmJxz3H1hNdv2FN4y85Qk7z4O8sdzKAGWYR7OMuB5KSDLYgQByfMsxzgx_maJ7blyLd81RZdD1GV7ZYKM9eO2CKe3MbWv9J8zIH-E0SdJ1Mg7osXQF-yJcmzTSudpSkK_PNjq77jBD9br5Zr6OLXBSj27dDPh3XeYXwPwGzq_OIFRQ8ZFdNUWTqOgWl9X9bIGQpoWGvslS35pcS5jotO_FHo6c_1cMqSrxW2H2IjzBytV-4hDdST1-WNBnR2phoeoe_yoetq2CfOplL0w5omc4n8qD3uDMTzrEU1CbMDxq&xkcb=SoDF6_M3sjH-L1xUJJ0PbzkdCdPP&camk=UoKtGZLa3XKXiB-AY1Xq9w==&p=0&fvj=0&vjs=3",
    "description": "Innovate. Automate. Transform.\nRebel Convenience Stores is seeking a Data Automation Engineer to join our IT team at our Upland, CA corporate office. This role is responsible for designing, developing, and maintaining integrations and automations across enterprise systems to improve efficiency, scalability, and business performance. If you thrive in building seamless data flows and innovative solutions that make processes smarter, we’d love to meet you.\nTHIS IS NOT A REMOTE OR HYBRID POSITION. CANDIDATES WILL ONLY BE CONSIDERED IF YOU ARE ABLE TO COMMUTE TO OUR CORPORATE OFFICE IN UPLAND, CA.\nWhat You’ll Do\nBuild and maintain data pipelines, APIs, and middleware to connect enterprise systems.\nDesign and manage ETL processes for smooth data integration.\nAutomate workflows and system tasks with software automation tools and scripts.\nDevelop, test, and maintain RESTful and SOAP APIs for secure, scalable integrations.\nMonitor system and integration performance, identify bottlenecks, and optimize workflows.\nEnsure integrations align with security standards and compliance requirements.\nDesign solutions for cloud, hybrid, and on-premise environments.\nCreate and maintain clear documentation for integrations, workflows, and system architectures.\nWhat You Bring\nBachelor’s degree in Computer Science, Software Engineering, Information Systems or equivalent experience.\n3+ years of hands-on experience in systems integration, data automation, or data engineering.\nProficiency with ETL tools, data transformation techniques, and scripting/programming (Python, JavaScript, SQL).\nStrong experience with APIs, REST/SOAP web services, and enterprise application integrations (ERP, CRM, HRIS, etc.).\nFamiliarity with authentication standards (OAuth, SAML, JWT) and security best practices.\nProblem-solving mindset, with strong analytical and technical skills.\nAbility to collaborate with cross-functional teams to design and deliver solutions.\nWhy Join Rebel Convenience Stores?\nWork on impactful projects that directly improve business operations and scalability.\nCompetitive compensation and performance-based growth opportunities.\nComprehensive benefits package (health, dental, vision, retirement plans).\nBe part of an innovative IT team driving technology transformation.\nA supportive environment where your technical expertise is valued.\nApply now to join Rebel Convenience Stores as a Data Automation Engineer and help us build smarter, faster, and more connected systems!\nJob Type: Full-time\nPay: $90,000.00 - $120,000.00 per year\nBenefits:\n401(k)\nDental insurance\nDisability insurance\nHealth insurance\nLife insurance\nPaid time off\nVision insurance\nEducation:\nBachelor's (Required)\nExperience:\nSystems Integration: 3 years (Required)\nSoftware Automation: 3 years (Required)\nData Engineering: 3 years (Required)\nLanguage:\nEnglish (Required)\nAbility to Commute:\nUpland, CA 91786 (Required)\nWork Location: In person"
  },
  {
    "title": "Data Analytics Engineer",
    "company": "Spring-Green Enterprises Inc. & Subsidiaries",
    "location": "Hybrid work in Naperville, IL 60564",
    "salary": "$80,000 - $90,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0A8g-KRKynU8DeOWtkKq9MzTbursXtihu9Xmb6xlnK7kfWEukwL-f7_jTPCf-hn57s-3nqr80SdgkxwCpiMD63da0KSg2DS6ZICQvYa4W6IVf8T15hngAFMRRBMaSjsSpkNHJ9iLxjanrxFxTdRz6MI9RgLj27bO8oysDQ2qf9MpqVOShHVZq6iK2568iEa9v148B5VbOZiTaoFV8FwBZp5YCNpdpDUNEHGFJFc2YEXvGi5KiBnIFcosqa9zxSYx_9Y-TsCMg5CScj85fYztaWtrmriiZbiaNU4tUszS10LCmHeTdWwoo_DbWL2vDeOn_ulw1jGXx_zVi8EQFRHLCVv6nAOjv02lq0XZcq2j4-LjIWuetFHBu6XJOw_RUNRL_AGZ02Ggyyeuk5eBWoPcbI6xslCtaJSAygfj9U-PPWK3IUowljxSl8OPgwGMdhA5cHr1av1Y-Z-gGYQPW0mu1p1_cujRaxNqjh9tHdx6mNYG_E1QavlBFGx9hkLnq4NMkCf2BmwzVq5hSnP9kEBqR518wxHi14agGfTia6EznOnDCSd_L2A0XrfvaUKsk91y6IAeZioYnI0aFkb_-hQ886xKghay_PLu7aXJD2EKX7tDBNre8kp6xYgggIJGm8JhIpOy6JOHa7tuRP4yqr9gvrz1qa8wSoV1uG55OsIHqlOEcdOsfaWHJ9Q-4I1JefXSy64C2eo9I4TEOihsO0JvKag&xkcb=SoBx6_M3sjH-L1xUJJ0ObzkdCdPP&camk=UoKtGZLa3XJ3Ghy_hBxiZw==&p=1&fvj=0&vjs=3",
    "description": "SGE Marketing- IT Services Inc. (a Spring-Green Enterprise company) is seeking a talented and self-motivated individual to become an integral part of our IT team. The Data Analytic Engineer is a full-time position located in Naperville, IL. This position will offer a flexible hybrid work arrangement with sometime spend working in the office. We offer competitive benefits including medical, dental, vision, life, and disability insurance, 401(k) participation, and paid holidays and vacation.\nAnnual salary range\n$80k - $90k based on experience.\n\nWe encourage personal and professional growth, join our culture with core values of Customer Focus, Pursuing Common Goals, Family, Integrity, Innovation, and Perseverance.\n\nSummary\nAs a Data Analytics Engineer, you'll be a key player on our team, reporting directly to the Manager of Data & Integrations. You'll use your sharp technical skills and proficiency with analytics tools to tackle a variety of projects. This role involves working with large datasets, developing detailed reports, and building complex data models. You'll be instrumental in creating and maintaining custom reports and dashboards in PowerBI, giving our users the insights they need. We're looking for someone who can confidently design and build data structures that are optimized for powerful reporting.\n\nEducation and/or Experience\nBachelor's degree in information technology or related field.\n3+ years related working experience in data engineering, report development, or data analytics\nExcellent knowledge of SQL and reporting building concept\nWorking knowledge of Microsoft BI technology (SSRS, SSIS, Power BI)\nSnowflake experience a plus.\nStrong verbal and written communication skills\nAbility to learn new technology\nCore Technical Skills\nMicrosoft T-SQL\nSSIS\nSSRS\nPower BI\nPython/Scripting\nEqual Opportunity Employer\n\nWe are an Equal Opportunity Employer welcoming candidates from all backgrounds and industries to apply. We encourage personal and professional growth. Come join our culture with core values of Customer Focus, Pursuing Common Goals, Family, Integrity, Innovation, and Perseverance.\nnpeit0jkLd"
  },
  {
    "title": "Senior Data Engineer - Real Time Analytics",
    "company": "Vanguard",
    "location": "Malvern, PA",
    "salary": "",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BWQs_M7ZA8XLbIFWVw-PYcVVEPryqVLyWhKaEKPskHy2YkbHyHJDwB5vIJ0eSmX6aAgeiYNWfyl7Ard7x5ad0QClAJOqPQhJ-qeKOSgU4vRmxvsyTiU98UL-4UUVXNivpVENKaGDYpenXXs6D1nZzqYGSSByq44IEYQVWSxw2xKHkJbxlM7kVUkJQZVUiFb9eWweCBrDkadm51OxlPJGsH5erDKBQMc2Jl_Va8VWZqlUpb6VU5YzMapb2huGij8o3USo6c_ud4ZnZsFkjhudYhrWTV_4vS_CuEu7mbNrFLJBPsr8DCszHOrDjyy_Fvr_l3kK_EKJ58bYAesKeOb-feEoC0SmDN7_bdu1XxJFtW79Y2xr0SmQrLiy2Gm8pUNCxjsQEiEvs5PHBlNEKyT7g6z0CFhN7eGEa9RE0_U7CfAef-OGKCjwSgMKIZCMfxApDhFlq9RGMLI6c9FPSrX5NWSlNXRjVuZ97HZlLfyxMZR4hhSb1vHizIxYay_K2WF27KgLH_VzpH9qh6sciAqeZf1JQghGWc8s5Qmrdq2DRjYJACDgzIlKQ1qA7MsiB3v5_g0c4hFvxTCBfCgE93l8AAxVmUiclSTeZX-o-oa_3sd4JfWt1AzWV5zOvNGY5lDwTAn2JOeGAG4ODR-6K7oyPCc96eKDrAuv8ntaTn_l8v3kRR5V28IOXDRI7kdJi1yNE4qDqpBkEIIrell0lA2rrnG0GCjnHcOkhUoVdlRqsHKQIV7_EMyzCehkK_iV2vdTgggjLRrPbnRTxogEc1d-otG_jIqh3wK6T8UKtS3Yb_dzAq8w_TkN5tXTbnDqe5jPNIJc2xBzUG6Vzjsu0AdcLonOpLBUttX3ijaVqUEmVInj7f8ZGyhMH6hPewBX75rCHqBhCSM-E_tecI2SvTxG3p1BB5uGSo69W5S_FYATyvKcZjUEQYuGYgaHaSrlgXlU2rOf0RnP9JBTiJphiuXIpr8T8HGnlAkSrYtelbEPXI3uIuD67o4_apN6kjtPgY7uyhq-75DRpQrL7EPLP9h0QGuyaw_53T4E9RBGh4ogPlra3zBypaKXV0pCCDC4ikzIq2XT3p2Anm2mrVSkPAZSHSN02V6SWgkVtoxdyKmCgDOrAugeIEXQgp7G1pIL9D-3V2KuYhbkRYXh9aqLYENMhOWYzEbrPYjMFxWvkg_qiaXZW3ymjOWcgjvY708PUo1YwDE4GZivfXg9OGQmHSKylzQ-mwfn1cnw7KnvhHnHunQmIVr1jCb9JfaUDMEUoj2MDG7C8XZ2MjqJEoftTFkZhoW0Y-daFFmtvc1ji6FAi4Toa_1iaBzH51NMjiI7pq8Te6fE4qocVMYQ==&xkcb=SoAW6_M3sjH-L1xUJJ0GbzkdCdPP&camk=G42KULXkIwNaEmQcdlbbiw==&p=9&fvj=0&vjs=3",
    "description": "In today’s world data and data analytics is critical to all investment systems supporting front, middle and back office. In this role you will be working as part of a full stack team that supports Vanguard’s Quantitative Equity Group’s (QEG) cloud enabled investment research capabilities, including signal generation, optimized back testing capabilities, proprietary software packages, and data and analytics platforms. These platforms are used in the portfolio management of QEG’s active, factor and alternate strategy funds. You will also learn how the global investment industry and primary market trading works while working with complex and modern technologies in a truly global organization. If you are looking for a challenge and want hands-on experience with exciting, cloud technologies and AWS services as well as work closely with highly talented IT individuals, business partners to build and increase investment acumen, this is the job for you. We are looking for a developer, passionate about technology and adopting modern software engineering practices to join us in building next generation quantitative research platforms in the cloud. In this role, you will work within a full stack team alongside our Business Partners and Product Owner and an extremely motivated team of developers committed to working in an Agile environment. You will help us in doing POCs to validate patterns proposed by our architecture and platform teams and help deploy AWS applications to production. A passion for mentoring and coaching others is highly desirable. You will also help upskill the department towards adopting cloud technologies by facilitating and participating in tech talks. Ideal candidate will have a strong focus on delivering code that is observable, resilient, and secure.\nRequired Skill set: (Please don’t apply if you do not have these skills and experience)\nProven experience as a Data Engineer\nFamiliarity with real-time data processing and analytics.\nStrong proficiency in Python programming for data engineering tasks. Experience using Python libraries and frameworks.\nStrong hands-on experience with real-time data streaming technologies such as AWS Kinesis, Apache Kafka, Apache Flink, or similar technologies.\nSolid understanding of SQL and experience working with relational databases (e.g., Aurora)).\nExperience with data warehousing concepts and tools (e.g., Amazon Redshift, Snowflake).\nExperience with data lake and Lakehouse concepts and tools (e.g., Apache Iceberg, Hive, etc).\nKnowledge of data modeling, ETL processes, and data integration best practices.\nAt least 5 to 6 years of total experience and a minimum of 2 to 3 years of experience with the above skills.\nFamiliarity with Agile Development principles\nExperience with software engineering fundamentals including object-oriented design, data structures, dependency injection, testable code, and algorithms.\nExperience with software engineering tools, such as Eclipse, Git, and others.\nAble to write clean, maintainable code, and read code created by others.\nHighly collaborative, fast learner, willing to jump in and help wherever needed.\nEnthusiasm for learning and experimenting with latest trends and advancements in real-time data engineering and analytics technologies.\nWhat it takes:\nUndergraduate degree in a related field or the equivalent combination of training and experience\nSoftware Development experience\nStrong written and oral communication skills\nStrong, demonstrated analysis and problem-solving skills.\nStrong planning and organizational skills\nAbility to implement instrumentation to gather business-specific metrics around consumer usage patterns.\nSpecial Factors\nSponsorship\nVanguard is not offering visa sponsorship for this position.\nAbout Vanguard\nAt Vanguard, we don't just have a mission—we're on a mission.\nTo work for the long-term financial wellbeing of our clients. To lead through product and services that transform our clients' lives. To learn and develop our skills as individuals and as a team. From Malvern to Melbourne, our mission drives us forward and inspires us to be our best.\nHow We Work\nVanguard has implemented a hybrid working model for the majority of our crew members, designed to capture the benefits of enhanced flexibility while enabling in-person learning, collaboration, and connection. We believe our mission-driven and highly collaborative culture is a critical enabler to support long-term client outcomes and enrich the employee experience."
  },
  {
    "title": "Data and BI Engineer",
    "company": "American Food & Vending ⏐ American Dining...",
    "location": "New York State",
    "salary": "$110,000 - $120,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=6c01e27a2f97b4b6&bb=dfT8O1z5LXWQ4TkbjdY1dG7aHC-iQXYX_KNJbAbTjFYI3OuQWFoM_K7uN_vAQkdMaYnHz_O3eUT2jR8lmTOeqQkAbzqCMNOR6IsFWkhnoUOQglmYZj6tPj3ybt3ma5MjwGJUHs0oZTQAnCcs_TiVwA%3D%3D&xkcb=SoCs67M3sjH-L1xUJJ0MbzkdCdPP&fccid=dd616958bd9ddc12&vjs=3",
    "description": "Pay: $110000 per year - $120000 per year\nMust be a US Citizen.\n\nSummary:\nWe are seeking a highly skilled and versatile Data & BI Engineer to join our data-driven team. This role combines responsibilities across data engineering, ETL development, business intelligence and data analysis. The ideal candidate will have hands-on experience with ETL pipelines, BI tools, cloud platforms, Python, and Snowflake, and will be responsible for designing, developing, and maintaining scalable data solutions that support analytics and business decision-making. They will collaborate closely with cross-functional teams to ensure the availability, reliability, and performance of our data systems and solutions.\n\nResponsibilities:\nDesign, build, and optimize robust ETL pipelines to ingest, transform, and load data from diverse sources (Structured and Unstructured).\nBuild and maintain integrations with internal and external data sources and APIs.\nDevelop and maintain data models, schemas, and metadata for analytical and operational use.\nImplement data validation, testing, cleansing, and quality checks to ensure data integrity.\nWork with orchestration tools for workflow automation and monitoring data pipeline operation.\nDesign and develop user-friendly and intuitive visualizations/dashboards to communicate insights effectively using Power BI or similar BI tools.\nCollaborate with stakeholders to gather requirements and translate them into BI solutions.\nMonitor and troubleshoot BI and ETL processes to ensure performance and reliability.\nProvide insights through data visualization and performance metrics.\nWork with cloud platforms such as Azure for data storage and processing.\nUtilize Snowflake for data warehousing, Write complex SQL queries and stored procedures for data extraction, transformation, and loading processes.\nImplement CI/CD pipelines and version control using tools like Git or Azure DevOps.\nDocument technical designs, workflows, and best practices to facilitate knowledge sharing and maintain system documentation\n\nQualifications:\n3+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL tools, and BI/analytic tools\n2+ years of Experience with BI tools such as Power BI, Tableau, or QlikView.\nHands-on experience with Snowflake, including Snowpipe, Streams, Stored Procedures and various AI and ML features.\nStrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\nStrong understanding of database technologies, management systems, and data modeling techniques.\nProficiency in Python.\nStrong SQL skills and experience with relational and cloud databases.\nFamiliarity with cloud technology\nFamiliarity with Agile/Scrum methodology\nFamiliarity with AI/Machine Learning and Advanced analytics is plus\nStrong communication, problem solving and collaboration skills.\nAbility to manage multiple priorities simultaneously.\nKnowledge of data governance, security, and compliance best practices.\n\nEducation:\nBachelor’s or master’s degree in computer science, Information Systems, Business Analytics or related field.\n\nAbout Us:\nAmerican Food & Vending began vending operations in Ithaca, NY, over eighty years ago and has become one of the largest privately held hospitality service partners in the United States.\n\nAs a family-owned boutique company, we have evolved into a dining and catering service provider. In 2012, the second and third family generations expanded the company nationally by adding our dining division, American Dining Creations. Today we provide service to guests in 24 states, over 50 cities, and employ more than 2,000 employees.\n\nAmerican Food & Vending and American Dining Creations strongly focus on service, technology, partnerships, and hospitality, which is evident in everything we do. Our national ranking as one of the Top 50 Contract Management Companies highlights our commitment to excellence.\n\nWe are a company of culinary enthusiasts. Our award-winning chefs are always exploring innovative, creative, and on-trend dining and refreshment solutions. As part of our combined services, we provide café dining, mobile ordering, catering, events, grab-and-go programs, vending and refreshment solutions, advanced micro markets, coffee services, pantry services, water filtration and purification.\n\nWe are committed to enhancing the guest experience and partnering with our communities to provide outstanding service, innovation, sustainability, and customization.\n\nAmerican Food & Vending Offers:\nWeekly Pay\n401K with company match\nEmployee Referral Bonus Program\nEmployee Assistance Program\nEligible employees offered Medical, Prescription, Dental, and Vision Plans\n\nAmerican Food & Vending is an Equal Opportunity Employer.\n\nChat with Scout, our virtual hiring assistant to apply for this position right from your mobile device. Text JOB to 315-803-6049 to get started!"
  },
  {
    "title": "Senior Software Engineer, Data",
    "company": "Credit Acceptance",
    "location": "Remote",
    "salary": "$130,047 - $190,735 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AsbktAG5_d7zZpvjjXSmUmcD4iMcRvongZk4cc4SyVxl1iNPhZYD-Wv7IpeTcySjMu0dB2xAOl4NXxPgKQv3isaDKIfhHq1bMa5cDBs-Kf9Emo8epUpeh3Q3QQbsL_CJ4uI-4U5H-HI7yzqcCDP0Gp8EFIpE4uevJwH7Kg1llKK4bpP89Wyl5RMgSXjsdOns4lXvoi1l9G9G4NGPMcfcGNz73xgWwQ5UwYqLeYuQbmw2Kj5iOtMeaWNm7_yturXqY4ax7G-P92bL7TAajGqaxczpEfNkHIN3TV3BgSqkZ8gtUWcP7OuvfpR2utpZjyXx7kDpmSDiihT12G_6BlbKFRi_HSGFvaiSzNuvl-wNH2fM5grfFSx_wiL6xLfATRPysaonejYqnhGs3sNgn7raWhQcjFpzB3uUfrC5FrFYqXZ5xNu8-zrb1gUdVmeLNyll2rK4IBqSByo5mpewmX4ihZdcy9HJo8DSu32AW05NR4sJEyovuOllsujiUfH5wljN3TnBt2mZJM3e1mWdvTKu4DRxUU3I98pNrCOGqQaBcnOUYyHcqJsQ53zCIZL5CcmI0LchNk54e4i66UDrrrosOjmstvIpiqSR50WegiBytHIMKAAkJk7HJeZP_cMyO9WB8teXhB-evwugTv370edbN8ldfK8MmLIzB-pHqiIvFgPq4APUqca-oAetElqfqEdNQ7IKV_HXvXTx-OHwRDaRmPj_Yz0a4_VqU=&xkcb=SoCs6_M3sjH-L1xUJJ0abzkdCdPP&camk=4HOcmqOLYrBXgGRMyqxztw==&p=13&fvj=0&vjs=3",
    "description": "Credit Acceptance is proud to be an award-winning company with local and national workplace recognition in multiple categories! Our world-class culture is shaped by dedicated Team Members who share a drive to succeed as professionals and together as a company. A great product, amazing people and our stable financial history have made us one of the largest used car finance companies nationally.\nOur Engineering and Analytics Team Members utilize the latest technology to develop, monitor, and maintain complex practices that help optimize our success. Our Team Members value being challenged, are encouraged to express their ideas, and have the flexibility to enjoy work life balance. We build intrinsic value by partnering with all functions of our business to support their success and make strategic business decisions. We focus on professional development and continuous improvement while enjoying a casual work environment and Great Place to Work culture!\nAs a Senior Software Engineer, Data, you will independently design and build scalable, reliable, and observable data pipelines and models across batch and streaming architectures, supporting structured, semi-structured, and unstructured data. You will lead complex initiatives, ensuring architectural standards are met and performance is optimized. Serving as a domain expert, you will balance hands-on execution with strategic foresight, mentor engineers, and drive cross-functional improvements that elevate the data platform.\nOutcomes and Activities:\nThis position will work from home; occasional planned travel to an assigned Southfield, Michigan office location may be required. However, this position is permitted to work at a Southfield, Michigan office location if requested by the team member\nDesigns and builds domain-level, end-to-end data pipelines across batch and streaming architectures that process structured, semi-structured, and unstructured data with a strong emphasis on data quality, scalability, reliability, and observability\nBalances project execution with architectural foresight and leads cross-functional initiatives that elevate the data platform\nPartner with stakeholders to understand data requirements and implement effective data integration strategies\nWrite unit tests and validate software against acceptance criteria\nApply and advocate for team coding, documentation, and testing standards\nConduct impact analysis to assess changes across applications\nDevelop a strong understanding of business processes to align technical solutions with business needs\nExperiment with new ideas, validate assumptions, and recommend solutions\nParticipate in code reviews and communicate application changes\nDocument code and projects for maintainability and support. This includes reading, writing, and reviewing design documents\nTroubleshoot production issues and propose effective solutions\nContribute to sprint commitments and actively participate in Agile practices\nEngage in continuous learning to improve design, code quality, and domain knowledge\nCompetencies: The following items detail how you will be successful in this role.\nCustomer Empathy: Customer Empathy is the ability to understand the perspectives, pain points, and experiences of customers. It involves actively putting oneself in the customer’s shoes, comprehending their needs and challenges, and using that understanding to provide a better, more customer-centric experience.\nEngineering Excellence: Engineering Excellence is about bringing great craftsmanship and thought leadership to deliver an outstanding product that delights customers and solves for the business. This involves the pursuit and achievement of high standards, best practices, innovation, and superior solutions.\nOne Team: A One Team mindset refers to a collaborative approach across the organization, where individuals work together seamlessly, without boundaries, as a single, cohesive team. Shared goals, open communication and mutual support create a sense of collective purpose. This enables teams to navigate challenges and pursue shared objectives more effectively.\nOwner’s Mindset: Owner’s Mindset involves adopting a set of behaviors that reflect a sense of responsibility, accountability, strategic thinking, and a proactive approach to managing your domain. As an owner, you understand the business and your domain(s) deeply and solve for the right outcome for the domain(s) and the business.\nRequirements:\nBachelor’s degree in Computer Science, Information Systems, or related field; or equivalent work experience\nMinimum 5 years of software engineering experience, with recent experience in cloud data platforms\nWorking knowledge of Databricks (e.g., Delta Lake, Unity Catalog, DLT, Auto Loader)\nSolid grasp of programming (Python, SQL, Pyspark, etc), data modeling, and database management (SQL/NoSQL)\nApplies technical knowledge to process all data formats — structured, semi-structured (e.g., JSON, Parquet), and unstructured (e.g., logs, text, clickstream)\nExperience with Agile/SCRUM and Waterfall methodologies\nExperience designing scalable batch and streaming data pipelines\nStrong understanding of data modeling, schema design, and lakehouse principles\nFamiliarity with data governance, lineage, and quality frameworks\nExperience working on enterprise-class applications\nPreferred:\nExperience with Apache Spark\nHands-on experience with AWS data services (e.g., S3, Glue, Lambda, MSK)\nCapable in batch or streaming data processing using technologies such as Spark, Kafka, Flink, and DLT\nProficient in CI/CD pipelines, automated testing, code quality enforcement, and environment management for production-grade data systems\nExpert in orchestration and transformation frameworks such as Airflow, dbt, and Dagster, along with cloud-native platforms like Databricks\nFinancial services or FinTech industry experience\nKnowledge and Skills:\nOperates with full autonomy on large-scale, complex data projects. Go to for a data domain\nAnticipates edge cases, performance bottlenecks, and data quality risks without supervision\nMentors engineers across the team, improves systems through code and process leadership, and drives initiatives to completion independently\nAbility to challenge the status quo and contribute innovative solutions.\nStrong collaboration skills and openness to diverse perspectives\nAbility to build relationships across the organization\nAbility to connect technical work to business value\nStrong communication skills, both verbal and written, across all levels\nTarget Compensation : A competitive base salary range from $130,047 - $190,735. This position is eligible for an annual variable cash bonus, between 7.5 - 15%. Final compensation within the range is influenced by many factors including role-specific skills, depth and experience level, industry background, relevant education and certifications.\nCandidates who reside in the following major metropolitan areas may be eligible for a premium on top of the posted range based on their specific zone: San Francisco, Seattle, Boston, New York City, Los Angeles and San Diego.\nINDENGLP\n#zip\n#LI-Remote\nBenefits\nExcellent benefits package that includes 401(K) match, adoption assistance, parental leave, tuition reimbursement, comprehensive medical/ dental/vision and many nonstandard benefits that make us a Great Place to Work\nOur Company Values:\nTo be successful in this role, Team Members need to be:\nPositive by maintaining resiliency and focusing on solutions\nRespectful by collaborating and actively listening\nInsightful by cultivating innovation, accumulating business and role specific knowledge, demonstrating self-awareness and making quality decisions\nDirect by effectively communicating and conveying courage\nEarnest by taking accountability, applying feedback and effectively planning and priority setting\nExpectations:\nRemain compliant with our policies processes and legal guidelines\nAll other duties as assigned\nAttendance as required by department\nAdvice !\nWe understand that your career search may look different than others. Our hiring team wants to make sure that this would be a fit not just for us, but for you long term. If you are actively looking or starting to explore new opportunities, send us your application!\nP.S .\nWe have great details around our stats, success, history and more. We’re proud of our culture and are happy to share why – let’s talk!\nRequired degrees must have been earned at institutions of Higher Education which are accredited by the Council for Higher Education Accreditation or equivalent.\nCredit Acceptance is dedicated to providing a safe and inclusive working environment for all. As part of our Culture of Compliance, we are proud to be an Equal Opportunity Employer and value our culturally diverse workforce. All qualified applicants will receive consideration for employment regardless of the person’s age, race, color, religion, sex, gender, sexual orientation, gender identity, national origin, veteran or disability status, criminal history, or any other legally protected characteristic.\nCalifornia Residents: Please click here for the California Consumer Privacy Act (CCPA) notice regarding the personal information Credit Acceptance may collect from you."
  },
  {
    "title": "Senior Data Engineer",
    "company": "Freddie Mac",
    "location": "McLean, VA 22102",
    "salary": "$130,000 - $196,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DoIGMrbrreZo4ANtxWb4CunTORMijY9VfSyNyMdc1zSgYkWlS2JlLkez8E4DKkyUuDYZMqy2p7CZ2d8R0owC94q-VJ6qngFbfuf1tC6gJPfu4J2ZgxksA1OFqOTGnp_eLoOcSk9mAfC8NaTpCc5ebNtALXOJo90NoEYOxPJxJBFIy1mW2r_04MW3NaSKtyxI8d3bKon7jEfNF7B2VuGsiaWJ9UVgJdZXZQn3Z-KVwlO4y1NF1vNcFmy6tDf5JwNJJIDfBUhq2qCKMUd4aZIJAjHrW9RPyhOcwEA3sh3IdFxOscD4MA5H8JdJBS6vPj1qH43MbY0zytKBCVKsfg7l9ZvJdJv1runBiHZ5aiOeFFD5V5DhepAOIwJDRcBRYzdQa9VJGiD3UFCXdbuLCZsJF5x3lKVFGmz5uutyuvct2hLUAUlFx2eTny1PYTIvO_S2ZIWhZd5wjTid57_38bTNlTJBwKxF_lVV-zoi5c0SZkZTFSpyp8MIUdZW3-sdtF6-JEWcU1TSf7CKVrkBwsM4JisOzEYSmqBKinaR3Wquz3ty-Z34pvJGrtlRJUFJHySItRCfJmD-gMyHeajhRhzoxkaTR0cXD2cm3Lc1u9yL6eRB23Rw923ix0b_cyC-lFWifgTvSYm4zchZWZxJd1k2ifuI1WbW_2f7JEnDB0OOKQVGnd9x6OGFddYUEETyzz_NwqBGlLa7-auHa4_pOtyn4MZXZD0BTEnN5yFDfJj4k6grLzROVEsingN6tILjzkiEg=&xkcb=SoA_6_M3sjH-L1xUJJ0EbzkdCdPP&camk=UoKtGZLa3XI2_3FJtSRZ1w==&p=11&fvj=0&vjs=3",
    "description": "At Freddie Mac, our mission of Making Home Possible is what motivates us, and it’s at the core of everything we do. Since our charter in 1970, we have made home possible for more than 90 million families across the country. Join an organization where your work contributes to a greater purpose.\nPosition Overview:\nWe are seeking a highly skilled Senior Software Engineer to join our team and enhance our internal data platform. This role requires expertise in modern cloud-based data infrastructure to support data-driven decision-making and modeling across the organization. The ideal candidate will possess a strong background in data engineering, software engineering, and AWS familiarity.\nOur Impact:\nWe manage a critical internal data platform supporting key business operations, including prepayment model development, trading analytics, and securitization.\nWe collaborate with various teams to understand their data requirements and design systems that align with their business objectives.\nWe ensure our systems are robust, scalable, fault-tolerant, and cost-effective.\nYour Impact:\nDesign, build, maintain and support ETL/ELT data pipelines using AWS Services (e.g. AWS EMR) and Snowflake\nMaintain data ingestion libraries written in Java and Python\nCollaborate with data producers, data scientists / modelers and data consumers to understand their requirements and design innovative solutions to empower them\nDesign and develop new code, review existing code changes, and implement automated tests.\nActively seek opportunities to continuously improve the technical quality and architecture to improve the product’s business value.\nImprove the product’s test automation and deployment practices to enable the team to deliver features more efficiently.\nOperate the data pipelines in production including release management and production support.\nQualifications:\nAt least 5 years of experience developing production software\nStrong Python skills with at least two years of experience writing production code\nAt least two years of experience in data engineering, including Apache Spark\nAt least one year of experience with Snowflake\nExposure to AWS and a willingness to learn more\nBachelor’s degree in computer science or equivalent experience\nExperience writing automated unit, integration, regression, performance and acceptance tests\nSolid understanding of software design principles\nKeys to Success in this Role:\nPassionate about hands-on software development\nA desire to work on all aspects of the software development lifecycle: requirements gathering, design, development, testing and operations\nStrong collaboration and communication skills (both written and verbal)\nDesire to continuously improve the team’s technical practices\nAbility to quickly learn, apply and deploy new technologies to solve emerging problems\nCurrent Freddie Mac employees please apply through the internal career site.\nWe consider all applicants for all positions without regard to gender, race, color, religion, national origin, age, marital status, veteran status, sexual orientation, gender identity/expression, physical and mental disability, pregnancy, ethnicity, genetic information or any other protected categories under applicable federal, state or local laws. We will ensure that individuals are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\nA safe and secure environment is critical to Freddie Mac’s business. This includes employee commitment to our acceptable use policy, applying a vigilance-first approach to work, supporting regulatory mandates, and using best practices to protect Freddie Mac from potential threats and risk. Employees exercise this responsibility by executing against policies and procedures and adhering to privacy & security obligations as required via training programs.\nCA Applicants: Qualified applications with arrest or conviction records will be considered for employment in accordance with the Los Angeles County Fair Chance Ordinance for Employers and the California Fair Chance Act.\nNotice to External Search Firms: Freddie Mac partners with BountyJobs for contingency search business through outside firms. Resumes received outside the BountyJobs system will be considered unsolicited and Freddie Mac will not be obligated to pay a placement fee. If interested in learning more, please visit www.BountyJobs.com and register with our referral code: MAC.\nTime-type:Full time\nFLSA Status:Exempt\nFreddie Mac offers a comprehensive total rewards package to include competitive compensation and market-leading benefit programs. Information on these benefit programs is available on our Careers site.\nThis position has an annualized market-based salary range of $130,000 - $196,000 and is eligible to participate in the annual incentive program. The final salary offered will generally fall within this range and is dependent on various factors including but not limited to the responsibilities of the position, experience, skill set, internal pay equity and other relevant qualifications of the applicant."
  },
  {
    "title": "Sr Data Engineer, Test Automation, AI/ML Systems - Remote",
    "company": "Optum",
    "location": "Remote in Boston, MA 02112",
    "salary": "$89,900 - $160,600 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BWpgndjYL01xMr26yTqLCDFb3b3BUdaEwmMV3w6lpNxUsIZLXemk5fkggRKKneAdOr3zXBpEtq4LigbV0HtAZnAWWAvuLj2nzsoYxzVldnVhgiueyQS68LAptzxQLsP1OveMsIucVeVWjzynVX0eKb2dVEAg3kt9KN5pfkL0uKipre1BPXSNH6-31PMSPT0s38BuuB16AeEROkmbFCUaP06vCSfANpfEAlv2NMGgNAC4jajBXNQoXjwdMs4S7UTMCt3lOfLC8-ZtReDzuVQ4Hj1q6Qn-DKBHErb29l5G88111w_xPDM7kwcSte6A46P_BSETeQoJPXtaO21zAQ01RPpt78l6tK04U_5tfWBaJzjW1hsoC0Jx1fkDljljAC3ItP0qpkbKMLO8iee5s22GnBY71vtArSOVGMNU0wJwq1rxZEo6-E7Yz1ZGK5_dt5byxs5kQlexq4G5bUypu5flbw3AAsqD0Nqf-o-faaGRMJcoM_ScvjJP0ES7XEF5QZMamx7n-exH8aXr43k5Dy2acwM1Uha16_qBq4wzn5VDr7S26eUnTAu7Dzp1tZHHiAjdZzHsTdT1HbdXRhdl3H_-8hm2TcLZGFu_APHzgCLEU51i34q63DXb8mzoATJIxxGqlaNmlbVr_jOLoRHepdCzzolVHvMwnPaAF6VZUi-KQnyK6Oi6EHWZTw_NQqLtjAA1nBkU59zCqFYNybHLnmO5t_r2qlPh0A-KAcJRERKm7ZjRqlcY5vYorna26K_eimr-AN9ycTiYtxJhrKvzeS5StqDvcV0a9U_qaq1kDtoYqzxlzBNfjafCYV&xkcb=SoAs6_M3sjH-L1xUJJ0AbzkdCdPP&camk=nUmJqO2E8riwP1e3qbvzzw==&p=7&fvj=0&vjs=3",
    "description": "Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health equity on a global scale. Join us to start Caring. Connecting. Growing together.\n\nOptum Insight partners with payers, providers, governments and life sciences companies to simplify and enhance clinical, administrative and financial processes through software-enabled services and analytics, while advancing value-based care. Our differentiated products, technology insights, clinical expertise and analytics support the entire health system — ultimately delivering better experiences for consumers.\n\nOptum Insight Technology and Engineering is a critical function in Optum Insight driving the innovation and value we provide our customers and partners. This team is focused on products, solutions, platform/enabling capability development, product development lifecycle, engineering excellence and connectivity to Optum Technology.\n\nYou’ll be a part of our AI Data Engineering team. We often deliver the entire AI solution, from healthcare data integration, AI model deployment, processing, results and analytics, to the applications used for reviewing clinical data and AI results. Our projects often encompass leveraging and building cutting edge Generative AI technologies from scratch.\n\nWe have the data and resources to make an impact on a scale. When our solutions are deployed, they process millions of clinical data elements and benefit millions of patients.\n\nWe are a globally distributed and diverse team with the shared passion to improve patient outcomes, improve healthcare operations and streamline payments. We pay attention to the details to make sure we deliver quality, the first time.\n\nYou’ll enjoy the flexibility to work remotely * from anywhere within the U.S. as you take on some tough challenges.\n\nPrimary Responsibilities:\nDesign and implement automated test suites for AI/ML workflows that enables Optum to derive information from patient data in a scalable, reliable, and cost-effective manner using AI\nAnalyze clinical data and determine the best designs on how to verify the correct processing of this data\nPerform functional, regression, integration, and system testing to validate software quality\nIdentify, create, and track reproducible defects using bug-tracking tools, collaborating with developers for resolution\nConduct performance and load testing to identify bottlenecks and measure throughput\nIntegrate automated tests to our AWS and GCP accounts to achieve continuous integration (CI/CD)\nAs an advocate for product quality, ensure that team members adopt agile SCRUM methodologies, perform unit tests, code reviews\nReview and contribute to engineering specifications to ensure that each feature/user story can be tested in an automated fashion\nWrite integration test plans with end-end test automation as a goal\nMentoring a couple of mode Junior engineers who may write test cases\nDesign, develop, and deploy AI-powered solutions using no-code, low-code, and advanced platforms, translating business needs into scalable applications that enhance products, workflows and decision-making\n\nYou’ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\nRequired Qualifications:\nMaster’s degree in computer science, Engineering, or a related technical field\n4+ years of work experience in AI/ML engineering with solid proficiency in Python programming\n2+ years of experience as an SDET using test automation tools like Selenium, Cypress or similar tools\nExperience in at least one project using API testing tools like Selenium, Pytest, Postman or similar\nDelivered at least one product hosted on a cloud platform (AWS, GCP, or Azure)\n\nPreferred Qualifications:\nAWS experience such as RDS, ALB, Redis Cache, Secret Manager, EC2, IAM\nExperience in deploying generative AI systems (LLMs, Prompts, Agentic systems)\nExperience with Rally\nExperience integrating with healthcare data systems and working with clinical data formats\nSolid communication and collaboration skills, with the ability to thrive in fast-paced and ambiguous environments\nSolid analytical skills and ability to debug issues in complex systems\nExperience with CI/CD pipelines and infrastructure-as-code tools (e.g., Terraform, CloudFormation, GitHub actions, Azure DevOps)\n\nAll employees working remotely will be required to adhere to UnitedHealth Group’s Telecommuter Policy.\n\nPay is based on several factors including but not limited to local labor markets, education, work experience, certifications, etc. In addition to your salary, we offer benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements). No matter where or when you begin a career with us, you’ll find a far-reaching choice of benefits and incentives. The salary for this role will range from $89,900 to $160,600 annually based on full-time employment. We comply with all minimum wage laws as applicable.\nApplication Deadline: This will be posted for a minimum of 2 business days or until a sufficient candidate pool has been collected. Job posting may come down early due to volume of applicants.\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone. We believe everyone–of every race, gender, sexuality, age, location and income–deserves the opportunity to live their healthiest life. Today, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes. We are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes — an enterprise priority reflected in our mission.\n\nUnitedHealth Group is an Equal Employment Opportunity employer under applicable law and qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status, or any other characteristic protected by local, state, or federal laws, rules, or regulations.\n\nUnitedHealth Group is a drug - free workplace. Candidates are required to pass a drug test before beginning employment.\n#OptumTechPJ"
  },
  {
    "title": "Lead Data Engineer, Converse Tech (Beaverton, Boston or Atlanta Location)",
    "company": "CONVERSE",
    "location": "Atlanta, GA 30318",
    "salary": "Full-time",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CLAFIAocNJ4Wfrhm82ciahvtobYpoaf72Vrs-CtZeLT_ILOwoZihKlzfNjHCtQsEoANuXKgGYDNxKVD5AtL2dKz1ZsBzparEm1w5rvzpVLCipvYh11nZW3bGpgvRfkhX5Agyi6XWanXlfCSENyirR_9rk4KuVqlbTVsm8gzgPG9ePb6WC-17O6DK_E2Flmydq3AtOVRUjh4IJyxaCzvHAT6WPm02T1ks6BW4-D3CMd8_HprntW4aeVixJkihWX1PXWPjx8wy-oNY0bz7oe-qYSfyZsktZro7o00Wft4JfgoE_CVJ1uIqTIGGYA8SLUJ9f7mMYXvV9f-YtT4OjmirR0as3yQKB0uEGjBPoydrABXSX39Z6G_TvHkW_LntIdar8QaUeRFq7W-YWdhoYC9bGe-v28AT_QeZ11XbBdo1UrBeoesLQ3SXiKUUP99lIJx-bzvGZz01D-kJ7-rP0AlullxNfFEnQtsE5C7raUHb_8-tES_OVWONcN4sxPmF_jLMV6t7bZTytwVOzKGjmGiHokGDkYBZZOlWEjQApOFb2h-Q8Bf4UlLSIirAbMORemR6OLv5idhBQxKOnhlIF1iVyOS9B_DFtBrtHVe6uR_uwLyU32d72HZNYClh_n6xJBxztVmhcjm_KFNBeDHdHizx499O-8TBy9df8=&xkcb=SoAx6_M3sjH-L1xUJJ0ZbzkdCdPP&camk=UoKtGZLa3XK3McxbXcXWcw==&p=14&fvj=0&vjs=3",
    "description": "WE ARE LOOKING FOR CANDIDATES IN THE BEAVERTON, BOSTON OR ATLANTA LOCATION\nWHO YOU’LL WORK WITH\nYou will report to the Data Engineering Manager and work hand-in-hand with the data engineering squad and product management team along with a variety of dedicated Converse and Nike partners focused on technology solution delivery, data architecture, and analytics platforms. You will join a highly motivated, global team that will be a driving force in building Data and Analytic solutions for the Converse Enterprise.\nWHO WE ARE LOOKING FOR\nWe are looking for someone who thrives in the ever-evolving world of data & analytics. The right candidate can quickly pick up new programming languages, technology concepts, and frameworks. They have strong problem-solving skills and an understanding of data structures and algorithms. They thrive in a hard-working team environment and have an ability to lead, influence and communicate effectively with team members and business stakeholders alike. Experience must include:\nBachelor's degree in computer science, data science, software engineering or related field. Will accept any suitable combination of education, experience and training\nMinimum 6 years of relevant work experience in designing and implementing innovative data engineering capabilities and end to end solutions.\nAdvanced Experience with data modeling, warehousing and building ETL pipelines; Must have experience with any ETL tools, preferably Matillion and/or PySpark\nExpert in building/operating highly available, distributed systems of data extraction, ingestion, processing of large data sets and delivering end to end projects independently\nAdvanced experience building cloud scalable, real-time and high-performance data lake solutions, preferably Databricks, Snowflake, AWS\nAdvanced Experience with big data technologies such as: Hadoop, Hive, Spark, EMR and orchestration tools like Airflow\nAdvanced experience in SQL and modern scripting or programming languages, such as Python, Shell\nExperience in CI/CD Pipeline for Code deployment: preferred GitHub, Jenkins, terraform, Databricks Assets Bundles\n\nWHAT YOU’LL WORK ON\nAs a Lead Data Engineer on the Converse Enterprise Data & Analytics team, you will be primarily accountable for setting development standards and frameworks for the Converse Data Engineering team. This role will take a hands-on approach, implementing standard development processes that will be used across the data engineering squads to increase the team’s velocity and the overall quality of data pipelines. This role will also be responsible for designing and developing critical data pipelines and streamlining our data architecture while ensuring data quality and reliability throughout the data lake and data warehouse. Role responsibilities are:\nDefine and communicate the requirements for technical environments and determine the technical scope for projects, and provide technical estimates or capacity planning; Translate product backlog items into engineering designs and logical units of work\nAssess a well-defined problem and lead the development of a technical solution that meets the needs of the business and aligns with architectural standards\nDrive collaboration with architecture, platform teams or other teams on integration needs/designs; create advanced technical designs; approve proof of concept efforts and reviewing results; ensure high quality solutions are implemented, and engineering standard methodologies are defined and followed.\nDesign and implement data products and features in collaboration with product owners, data analysts, and business partners using Agile / Scrum methodology.\nDefine and apply appropriate data acquisition, processing and consumption strategies for given technical scenarios; Design and implement distributed data processing pipelines using tools and languages prevalent in the big data ecosystem; Profile and analyze data for the purpose of designing scalable solutions\nDrive and implement the technical strategies of new data projects and the optimization of existing solutions; Troubleshoot complex data issues and perform root cause analysis to proactively resolve product and operational issues\nBuild utilities, user defined functions, libraries, and frameworks to better enable data flow patterns; implement complex automated routines using workflow orchestration tool\nDrive collaborative reviews of design, code and test plans and dataset implementation by other data engineers in support of maintaining data engineering standards\nIdentify and remove technical bottlenecks for your engineering squad; and provide leadership, guidance and mentorship to other data engineers in adopting best standards and practices\nAnticipate, identify and tackle issues concerning data management to improve data quality; Build and incorporate automated unit tests and participate in integration testing efforts.\nUtilize and advance the software engineering best practices, including source control, code review, testing, and continuous integration and delivery (CI/CD) on the required cloud infrastructure and workspace configurations, Source files, such as notebooks and Python files, that include the business logic etc.\nThese are the characteristics that we strive for in our own work. We would love to hear from candidates who embody the same:\nDesire to work closely with your teammates to come up with the best solution to a problem\nDemonstrate experience and ability to deliver results on multiple projects in a fast-paced, agile environment\nExcellent problem-solving and interpersonal communication skills;\nStrong desire to learn, share knowledge with others and coach the team.\nIt is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability.\nWe offer a number of accommodations to complete our interview process including screen readers, sign language interpreters, accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as needed. If you discover, as you navigate our application process, that you need assistance or an accommodation due to a disability, please complete the Candidate Accommodation Request Form.\nIt is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability."
  },
  {
    "title": "",
    "company": "",
    "location": "",
    "salary": "",
    "url": "https://www.indeed.com/viewjob?jk=cdef0123456789ab",
    "description": "None"
  },
  {
    "title": "Data Engineer",
    "company": "CGI Group, Inc.",
    "location": "Indianapolis, IN 46262",
    "salary": "$61,900 - $108,300 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CmPt6JXytAhZscz-5ZOP53MMQ49Xi4hmwETo1lvmuAlRL2OAxsFRorduI7SZmWktVx1AP_9CJjdeBTw5C8c6RTCmsIeeBHH72U93e9MbhhfV1V56LDQLPcj62vdyZZlaHrfYiuLagPfASu3_f_CA8J9vqYMptudNjVQXqjjsSrP5wo9Sn_xFcO0Wp9xifLKHZDAI0NuQVFcl49O27APur3F1EEaTskGTXwqu9a9W1jDLBdBYvq5zUcGgScI5qVVmPjYuFppPbZzja0eegI0_NV_GykWG6hcN-BDbn4l6RFkxUGxMCU1qHiqx6mosSXXM8FbVvdlz6KY2m-vCXTGPqIMjWogkMe2WTZUL5Z1E_mb8zx-HKeMwhFK9wdYvOUDAR3awWdvGj4ARDeAC0xy3HNZNzrqrvY2idtOkxRGtodVRjJkPOb2-_vCGR02xXSswc9hpmujYohLdODUYdl9BLktTUgQnZToUUYd0PHlF5jSitw8tdN_V3-5KquLo-MWVF2exbz7uL-KKvMTrRrIsXwQxskXR2cJNSnGEG-uUC9532SWrWH8iNlBcnK6jKpMREv-iDlcQI_nDl5qfsmlY9UnudbcS4OgP7S62edTLEOUkf4CiF63a0cakz9Ocm0qp0zK-PTRDr2POtW9RW6mDLbA5zvZEARJmE=&xkcb=SoD-6_M3sjH0q33zQp0LbzkdCdPP&camk=4HOcmqOLYrD3MDZWBKj-EA==&p=0&fvj=0&vjs=3",
    "description": "U.S. - The best version of me\nBy playing this video you consent to Google/YouTube processing your data and using cookies Learn more.\nPosition Description:\nWe are seeking a motivated Data Engineer to support the modernization of our our clients' insurance agency and policy admin data platforms. This role offers the opportunity to work with cloud technologies (AWS or Azure), build scalable data pipelines, and integrate key systems such as Applied Epic, Ivans, EzLynx, AL3 data integration, Guidewire, and Salesforce FSC. In this role you will contribute to real-time analytics, predictive modeling, and customer insights, while collaborating with cross-functional teams to deliver high-impact data solutions\n\nThis is a Full Time Employment opportunity, based in Indianapolis, IN. And Portsmouth, RI.\nYour future duties and responsibilities:\nParticipate in the transition from legacy systems to modern cloud-based data platforms (e.g., AWS, Azure).\nMigrate key insurance data (policy) from Applied Epic and Guidewire into unified data warehouses/lakes for advanced analytics.\nArchitect Insurance Data Ecosystem\nDesign scalable data architectures that support future-state personal lines insurance workflows, omnichannel engagement, and predictive analytics.\nIntegrate Applied Epic, Guidewire, and Salesforce FSC data for 360 customer and policyholder insights.\nPrepare structured datasets for actuarial modeling, underwriting automation, and customer segmentation using ML and AI tools.\nCollaborate with data scientists to deploy models into production environments and monitor model performance over time.\nBuild real-time and event-driven data pipelines to support dynamic pricing, and customer notifications.\nLeverage tools like Kafka, AWS Kinesis, or Pub/Sub for real-time processing across policyholder journeys.\nImplement automated data cataloging, lineage tracking, and access controls aligned with NAIC, GDPR, and CCPA requirements.\nUse metadata management and data observability tools to ensure future scalability, trust, and transparency.\nEnable APIs and data layers to support partnerships and embedded insurance models.\nCollaborate with third-party Insurtech platforms, MGAs, and distribution partners through secure data integrations.\nDevelop reusable, curated datasets and data marts that empower underwriters, and business analysts.\nCreate training content and documentation to promote self-service data usage across business units.\nIntegrate Salesforce FSC with internal insurance systems to deliver hyper-personalized customer communications and retention strategies.\nBuild data layers that power cross-sell/up-sell recommendations and lifecycle marketing based on claims or policy behavior.\nAdopt agile methodologies for iterative data delivery and implement CI/CD pipelines for data deployments.\nParticipate in automation in testing, monitoring, and documentation of data products and pipelines.\nRequired qualifications to be successful in this role:\n35 years of professional experience as a Data Engineer, preferably in the insurance or financial services industry.\nStrong domain knowledge of Personal Lines Insurance (Auto, Homeowners, Renters, etc.), including policy lifecycle, Renewal, billing, and underwriting.\nHands-on experience working with Applied Epic and Guidewire (Policy Center, Billing Center, or Claim Center).\nFamiliarity with Salesforce Financial Services Cloud (FSC) or core Salesforce CRM data models (nice to have but preferred).\nUnderstanding Insurance data structures, workflows, and APIs specific to insurance operations and customer engagement.\nProficient in SQL and one or more programming languages like Python, Scala, or Java for data transformation and scripting.\nExperience building ETL/ELT pipelines using tools such as dbt, Talend, Informatica, or custom frameworks.\nWorking knowledge of cloud-based data platforms (e.g., AWS Glue/Redshift, Azure Data Factory/Synapse, GCP BigQuery).\nExposure to streaming and event-driven architecture (e.g., Kafka, Kinesis, Spark Streaming) is a plus.\nAbility to design and implement insurance data models across policy, claims, customer, and finance domains.\nExperience with dimensional modeling, normalization/denormalization, and building data marts for reporting/analytics.\nKnowledge of data governance best practices including data quality, lineage, access control, and metadata management.\nFamiliarity with regulatory and compliance frameworks (e.g., GDPR, CCPA, HIPAA, NAIC standards).\nExperience enabling BI tools such as Power BI, Tableau, or Looker by delivering clean, high-quality data sets.\nAbility to collaborate with analytics and reporting teams to support KPIs and business insights.\nStrong communication skills to collaborate with business stakeholders, product owners, and cross-functional IT teams.\nAbility to work in agile development environments and participate in sprint planning, retrospectives, and stand-ups.\n\n#LI-TSCH1\n\nOther Information:\n\nCGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $61,900.00 - $108,300.00.\n\nCGIs benefits are offered to eligible professionals on their first day of employment to include:\nCompetitive compensation\nComprehensive insurance options\nMatching contributions through the 401(k) plan and the share purchase plan\nPaid time off for vacation, holidays, and sick time\nPaid parental leave\nLearning opportunities and tuition assistance\nWellness and Well-being programs\nSkills:\nData Engineering\nETL\nPersonal Lines Insurance\nService Cloud\nSQL\nWhat you can expect from us:\nTogether, as owners, lets turn meaningful insights into action.\n\nLife at CGI is rooted in ownership, teamwork, respect and belonging. Here, youll reach your full potential because\n\nYou are invited to be an owner from day 1 as we work together to bring our Dream to life. Thats why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our companys strategy and direction.\n\nYour work creates value. Youll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise.\n\nYoull shape your career by joining a company built to grow and last. Youll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons.\n\nCome join our teamone of the largest IT and business consulting services firms in the world.\n\nQualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status or responsibilities, reproductive health decisions, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics to the extent required by applicable federal, state, and/or local laws where we do business.\n\nCGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned.\n\nWe make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.\n\nAll CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. Dependent upon role and/or federal government security clearance requirements, and in accordance with applicable laws, some background investigations may include a credit check. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.\n\nCGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGIs legal duty to furnish information."
  },
  {
    "title": "Data and BI Engineer",
    "company": "American Food & Vending ⏐ American Dining...",
    "location": "New York State",
    "salary": "$110,000 - $120,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=6c01e27a2f97b4b6&bb=DthfB3g5WpsJd_sByUBtezMzD8OrmkH1PMNP3WGfngHTVhDdt9Xdhd60PBW9cpn3oAPqrLrR5djH-LalPAJeOk1UkevfpmSnEvfBmaMBoHZaHlKGkeJJxk-KN4RZJX5IjNmXk8DIgiZypvPMJYOq0A%3D%3D&xkcb=SoAw67M3sjH0q2XzQp0KbzkdCdPP&fccid=dd616958bd9ddc12&vjs=3",
    "description": "Pay: $110000 per year - $120000 per year\nMust be a US Citizen.\n\nSummary:\nWe are seeking a highly skilled and versatile Data & BI Engineer to join our data-driven team. This role combines responsibilities across data engineering, ETL development, business intelligence and data analysis. The ideal candidate will have hands-on experience with ETL pipelines, BI tools, cloud platforms, Python, and Snowflake, and will be responsible for designing, developing, and maintaining scalable data solutions that support analytics and business decision-making. They will collaborate closely with cross-functional teams to ensure the availability, reliability, and performance of our data systems and solutions.\n\nResponsibilities:\nDesign, build, and optimize robust ETL pipelines to ingest, transform, and load data from diverse sources (Structured and Unstructured).\nBuild and maintain integrations with internal and external data sources and APIs.\nDevelop and maintain data models, schemas, and metadata for analytical and operational use.\nImplement data validation, testing, cleansing, and quality checks to ensure data integrity.\nWork with orchestration tools for workflow automation and monitoring data pipeline operation.\nDesign and develop user-friendly and intuitive visualizations/dashboards to communicate insights effectively using Power BI or similar BI tools.\nCollaborate with stakeholders to gather requirements and translate them into BI solutions.\nMonitor and troubleshoot BI and ETL processes to ensure performance and reliability.\nProvide insights through data visualization and performance metrics.\nWork with cloud platforms such as Azure for data storage and processing.\nUtilize Snowflake for data warehousing, Write complex SQL queries and stored procedures for data extraction, transformation, and loading processes.\nImplement CI/CD pipelines and version control using tools like Git or Azure DevOps.\nDocument technical designs, workflows, and best practices to facilitate knowledge sharing and maintain system documentation\n\nQualifications:\n3+ years of relevant experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL tools, and BI/analytic tools\n2+ years of Experience with BI tools such as Power BI, Tableau, or QlikView.\nHands-on experience with Snowflake, including Snowpipe, Streams, Stored Procedures and various AI and ML features.\nStrong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\nStrong understanding of database technologies, management systems, and data modeling techniques.\nProficiency in Python.\nStrong SQL skills and experience with relational and cloud databases.\nFamiliarity with cloud technology\nFamiliarity with Agile/Scrum methodology\nFamiliarity with AI/Machine Learning and Advanced analytics is plus\nStrong communication, problem solving and collaboration skills.\nAbility to manage multiple priorities simultaneously.\nKnowledge of data governance, security, and compliance best practices.\n\nEducation:\nBachelor’s or master’s degree in computer science, Information Systems, Business Analytics or related field.\n\nAbout Us:\nAmerican Food & Vending began vending operations in Ithaca, NY, over eighty years ago and has become one of the largest privately held hospitality service partners in the United States.\n\nAs a family-owned boutique company, we have evolved into a dining and catering service provider. In 2012, the second and third family generations expanded the company nationally by adding our dining division, American Dining Creations. Today we provide service to guests in 24 states, over 50 cities, and employ more than 2,000 employees.\n\nAmerican Food & Vending and American Dining Creations strongly focus on service, technology, partnerships, and hospitality, which is evident in everything we do. Our national ranking as one of the Top 50 Contract Management Companies highlights our commitment to excellence.\n\nWe are a company of culinary enthusiasts. Our award-winning chefs are always exploring innovative, creative, and on-trend dining and refreshment solutions. As part of our combined services, we provide café dining, mobile ordering, catering, events, grab-and-go programs, vending and refreshment solutions, advanced micro markets, coffee services, pantry services, water filtration and purification.\n\nWe are committed to enhancing the guest experience and partnering with our communities to provide outstanding service, innovation, sustainability, and customization.\n\nAmerican Food & Vending Offers:\nWeekly Pay\n401K with company match\nEmployee Referral Bonus Program\nEmployee Assistance Program\nEligible employees offered Medical, Prescription, Dental, and Vision Plans\n\nAmerican Food & Vending is an Equal Opportunity Employer.\n\nChat with Scout, our virtual hiring assistant to apply for this position right from your mobile device. Text JOB to 315-803-6049 to get started!"
  },
  {
    "title": "Senior Data Engineer",
    "company": "Spring & Bond",
    "location": "Remote in New York, NY",
    "salary": "$120,000 - $160,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=b617c51cc5d7cf10&bb=DthfB3g5WpsJd_sByUBte0bCY1I8hXeatKOCdSuyNta7wrpAOTfrSW3wqDr8uo8ffK94d383LwYiUd0vVAKmS8yLHQ05ykjNUcvz5JlFGRwUxnxQ9MAnb7MoiP_bB02WLyCbHmtlPnGQSwToE-B4sg%3D%3D&xkcb=SoAj67M3sjH0q2XzQp0ObzkdCdPP&fccid=811397025b0dcb85&vjs=3",
    "description": "About us:\nSpring & Bond is a digital media agency and consultancy specializing in helping pharmaceutical and medical device manufacturers create robust, omnichannel media strategies for both healthcare professionals (HCPs) and consumer audiences. We emphasize transparency and client empowerment through comprehensive services, including customer journey planning, media strategy and activation, technology evaluation, in-house capability development, and training.\nWhat You'll Do:\nWe are looking for a Senior Data Engineer to help drive the design, build, and scale of our data infrastructure. This role will be responsible for architecting and maintaining multi-cloud data platforms, optimizing ETL pipelines, and ensuring data quality and availability across the organization. As a senior member of the Data Engineering team, you will not only deliver hands-on technical solutions but also mentor junior engineers, guide best practices, and help shape the direction of our data engineering capabilities.\nYour Responsibilities:\nArchitect and optimize robust, scalable ETL pipelines and workflows to ingest, transform, and deliver data from multiple sources.\nEnsure data integrity and governance through validation frameworks, schema design, and proactive monitoring.\nLeverage advanced SQL and programming skills to extract, transform, and analyze data efficiently.\nDevelop automation using Python, Pandas, AWS Lambda, and related tools to improve operational efficiency.\nDesign and manage data platforms, including Snowflake, Athena, Redshift, BigQuery, and other cloud-based solutions.\nCollaborate cross-functionally with analysts, strategists, and business leaders to translate business requirements into technical solutions.\nMentor and coach junior data engineers, setting standards for code quality, documentation, and best practices.\nLead vendor and partner integrations, ensuring seamless data delivery and alignment with internal standards.\nCommunicate effectively with both technical and non-technical stakeholders, providing clear insight and direction.\nDrive innovation, identifying opportunities to improve performance, scalability, and usability of our data systems.\nYour Qualifications:\nBachelor’s degree in Computer Science, Data Engineering, or related field—or equivalent experience.\n5–7+ years of experience in data engineering, with proven success building and scaling data systems.\nExpert knowledge of SQL and Python, including performance optimization and data manipulation libraries (e.g., Pandas).\nStrong experience with AWS data products (S3, Redshift, Athena, Lambda) and familiarity with multi-cloud environments.\nDeep understanding of data modeling, schema design, and best practices for large-scale data solutions.\nDemonstrated ability to design secure, reliable pipelines and troubleshoot complex issues.\nExperience leading projects and mentoring other engineers.\nExcellent communication, critical thinking, and problem-solving skills.\nComfortable managing multiple priorities and delivering in a fast-paced environment.\n\nGreat-to-Haves:\nExperience with data transformation and productivity tools (dbt, Jinja, etc.).\nFamiliarity with Snowflake (Notebooks, Worksheets, VS Code plugin).\nKnowledge of data visualization platforms such as Looker Studio or similar BI tools.\nExposure to digital marketing analytics and measurement frameworks.\nBenefits:\nRemote-first team environment\nCoverage for medical, dental, and vision insurance for you and your dependents\nDisability insurance plan\nMatching 401K\nParental leave\nOther fun health & wellness perks\nSpring & Bond is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.\nSpring & Bond is a woman-owned business.\nNot everyone will match the above qualifications 100%. If your experiences don’t perfectly align, but you think you’d be a great addition to our team, we’d still love to hear from you.\nCompensation Range: $120K - $160K"
  },
  {
    "title": "Sr. Data Engineer - Top Secret",
    "company": "Gridiron IT",
    "location": "Huntsville, AL 35812",
    "salary": "$150,000 - $170,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CTHA6cd59lXtQJ-DuZtBHQsSjOn019HaVEc20FtZol16Ry6pW_HxgBH8El4_gqC7sUOfMZdoVMFAhwHQjozTZgQgcPmwTQCFdY1zfC9c4ZS8iDDkvDgQJeFjALH8EIDUZN00WfjXJFflI3ZzgilXFT6vppASxUvGemwh0IW-L1yfmYw6P6u7T1sDf61ccn3efFffvfbX_pLLkPrmSN3aNKO4QV4mV6BTgM5U5ala3wZ0jfIPuMtExwqdZNj3nNHYZEhxJ1p-1sYpx33voZQ_0N1xpEDHa6OTHqN0hMQpiU6WdQqzoSyQdr-N1DCSYHwS9EVJnrkXeBwNfFWBlKYUJdbZRYPDCPag25VqKngN2CJcjVqBuTVOD2KbrGdHulZzAgwU-jhBfsswNvtKAlehagK-l_byxACHSWJ_BuEGtG0dtY8JUgR5Dn_Oy5533a2PWgt_yGgwdxiWjQm-V9tXK7ITvZK9LWOfYYtGZ296Ok7J-evuOm0KTEFXiI0882BfpiekU58SXMBs8Pcvhln1vDqjPtUGi6nwaRZ1Rf8YdwCArYt1888_KdfdEqB8dSBnRVUBHXjTwBiWO9JR9_vwFSKQjethn5LoQKGfd-3cSzSAF_8M4AuCCfC2ndFrBKATpGQT8egLCL87PPgf31l8Uup5ptLeubgCw=&xkcb=SoDX6_M3sjH0q33zQp0JbzkdCdPP&camk=4HOcmqOLYrBWePrUXQswag==&p=2&fvj=0&vjs=3",
    "description": "Ever-expanding technology like IoT, machine learning, and artificial intelligence means that there’s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing data can yield pivotal insights when it’s gathered from disparate sources. We need an experienced data engineer like you to help our clients find answers in their data to impact important missions—from fraud detection to cancer research to national intelligence.\nAs a Data Engineer, you’ll help build advanced technology solutions and implement data engineering activities on some of the most mission-driven projects in the industry. You’ll deploy and develop pipelines and platforms that organize and make disparate data meaningful.\nHere, you’ll work with and guide a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You’ll use your experience in analytical exploration and data examination while you manage the assessment, design, building, and maintenance of scalable platforms for your clients.\nQualifications\nExperience in programming clean, secure, and efficient Java using established coding principles for scripting, data analysis, automation, and potentially data warehousing\nExperience with ETL tools, such as Informatica\nExperience performing tuning on large datasets in Oracle databases to optimize performance\nExperience writing scripts to automate processes in the development workflow, such as building, deploying, or managing Java applications\nExperience with CI/CD practices to automate builds, testing, and deployments\nExperience writing PL or SQL scripts and stored procedures for data manipulation, parsing, and processing within Oracle databases\nAbility to work in a complex enterprise environment and determine the best solution required for business and customer needs\nAbility to manage tasks and projects independently with minimal guidance and direction\nTop Secret clearance\nBachelor's degree and 10+ years of experience as a data engineer in a large volume enterprise system or 14+ years of experience as a data engineer in a large volume enterprise system in lieu of a degree\nAdditional Qualifications\nExperience designing and creating data warehousing solutions\nExperience with DevOps tools and principles for collaboration and automation\nExperience working in various SDLC models such as Waterfall, Agile, Iterative, or Spiral\nExperience delivering solutions across all phases of the development lifecycle\nKnowledge of Informatica's role in data warehousing and business intelligence\nKnowledge of the benefits and usage of stored procedures, especially for performance optimization and security\nKnowledge of Oracle database concepts, including partitioning for very large databases (VLDBs)\nTS/SCI clearance with a polygraph\nClearance:?\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Top Secret clearance is required.?\nCompensation and Benefits\nSalary Range: $150,000 - $170,000 (Compensation is determined by various factors, including but not limited to location, work experience, skills, education, certifications, seniority, and business needs. This range may be modified in the future.)\nBenefits: Gridiron offers a comprehensive benefits package including medical, dental, vision insurance, HSA, FSA, 401(k), disability & ADD insurance, life and pet insurance to eligible employees. Full-time and part-time employees working at least 30 hours per week on a regular basis are eligible to participate in Gridiron’s benefits programs.\nGridiron IT Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status or disability status.\nGridiron IT is a Women Owned Small Business (WOSB) headquartered in the Washington, D.C. area that supports our clients' missions throughout the United States. Gridiron IT specializes in providing comprehensive IT services tailored to meet the needs of federal agencies. Our capabilities include IT Infrastructure & Cloud Services, Cyber Security, Software Integration & Development, Data Solution & AI, and Enterprise Applications. These capabilities are backed by Gridiron IT's experienced workforce and our commitment to ensuring we meet and exceed our clients' expectations.\nJob Type: Full-time\nPay: $150,000.00 - $170,000.00 per year\nBenefits:\nDental insurance\nHealth insurance\nVision insurance\nWork Location: In person"
  },
  {
    "title": "Data Engineer - SQL",
    "company": "Perceptive Recruiting, LLC",
    "location": "Hybrid work in Greenville, SC",
    "salary": "",
    "url": "https://www.indeed.com/rc/clk?jk=45a8af5da9aa06e1&bb=DthfB3g5WpsJd_sByUBte222WqZeHo1pp0Nz7xCOFNTA1ISQYI7LGNdAKFjzl1Tx82b47tjNn93lr4T9raik_HMwrxMCzPjYj-onmd5Gig15MaSYDQCB64XbBmZxxNWEOAYJ6_30S4DAs0rmjj-ooA%3D%3D&xkcb=SoC-67M3sjH0q2XzQp0NbzkdCdPP&fccid=f8a36b5e705ca58b&vjs=3",
    "description": "Data Engineer position open in Greenville, SC for an Engineer or Developer with ETL pipline experience in a MS\nSQL environment. This is a direct-hire, hybrid position (3 days onsite) with a data engineering team. Visa Sponsorship NOT available.\n\n\nWhat you will work on: Python, Snowflake, MS SQL Server, ETL or SSIS, Azure Data Factory, Apache Airflow and more!\n\nRequirements:\n2+ years of data engineering or software development with a strong emphasis on data - SQL, Python, ETL or SSIS required.\nExperience with data migrations and managing large datasets\nStrong T/SQL scripting abilities, including stored procedures, views, table joins/queries, database design, normalization, and de-normalization techniques\nExperience with Python and scripting/monitoring platforms\nPreferred: Apache Airflow, SSIS, Azure Data Factory or AWS services (cloud)\n\nPerceptive Recruiting, LLC, headquartered in Greenville, SC, has 27+ years of technology recruitment experience. We build relationships with our clients so we can find you the right cultural and technical fit for your next role. We provide guidance every step of the interview process to give you the best chance of success. Looking for your next opportunity?\nConnect with us today! Perceptive Recruiting is an equal opportunity employer."
  },
  {
    "title": "Senior Lead Data Engineer",
    "company": "Capital One",
    "location": "Plano, TX 75023",
    "salary": "Full-time",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C3j_zLGvpMLCdiZ0WC46XqVTA1VMZzOzKXPhAXwYlrNQzIroA7S00QBkcNNfzTUOnyXr_zOt44e0LQls-cFNoB34DAwyzuU3Lvyg5eCOxYu4SSJM-W5Q4_EJxDunuWDTtRWvkPkxJLlkzFgVwQKGRXwiGDKKGAadINm68mknaIeZF14nKm_xSzJZ5tJeB2hWFdOfkDpmYw3_sS8LNaQvgDjIa8WZxUG81WQoyh9KoRjSpT0GaVYglh6evf4uVyKBmZ-roKlbQOpelUhyRODMmt2NWcZPr8ZH36VJX_GqG4GlKFbQIm5jB7sn146YrJ4cM-EFTQ3f0PQTI6DAMOmd5wlEookXeTnsUs7cNlIRgK1DjQaN0282xn8OyRkpffnKFHQMzh9aJ-q7LkzeqgGS0t0WapT-oJLw6ZeCoKFi8gqRpzfF3xezESVz6qSo_-Q_a-yXjwAyomiYbcmEbkymmg-oE7SmpQFGv0VeH97Nwf2ESXBlph2mATtdG3kLJ17nNJVJ9pyWgOfM6MmUVYXp7Hyy_wAuV9JF3iYkZBbN1xF_2LvbWwPGezuBQEOz3HZaF9GQm3cOmuy5eh5CacJhXEvqVAuHocLkpSwVF3q7RwQhhNyVfG8o82QufqHYKNTA4YENeqBlG2qjSlvOFrstQNOq1IcAHIPh7x2tR0kf3O21SXzdezFycfIzUq9dZOPHpPXlQBzAWwrmx2bh5zJqW60x8WGFHWdLM5zRmtjkM822cHOv2qj8q7AAmS70vCrX-8AuudesbC-WKNoKuSiap698QvplrDqCUaMw9TjsIuIdbCYI3egyyh&xkcb=SoBj6_M3sjH0q33zQp0IbzkdCdPP&camk=ethIe0s0hefG8K9KHaueHA==&p=3&fvj=0&vjs=3",
    "description": "Senior Lead Data Engineer\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\nWhat You’ll Do:\nCollaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\nLead a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\nUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\nShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\nCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\nPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\nBasic Qualifications:\nBachelor’s Degree\nAt least 6 years of experience in application development (Internship experience does not apply)\nAt least 2 years of experience in big data technologies\nAt least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)\nPreferred Qualifications:\nMaster's Degree\n9+ years of experience in application development including Python, SQL, Scala, or Java\n4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\n5+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n4+ year experience working on real-time data and streaming applications\n4+ years of experience with NoSQL implementation (Mongo, Cassandra)\n4+ years of data warehousing experience (Redshift or Snowflake)\n4+ years of experience with UNIX/Linux including basic commands and shell scripting\n2+ years of experience with Agile engineering practices\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\nMcLean, VA: $225,400 - $257,200 for Sr. Lead Data Engineer\nPlano, TX: $204,900 - $233,800 for Sr. Lead Data Engineer\nRichmond, VA: $204,900 - $233,800 for Sr. Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\nThis role is expected to accept applications for a minimum of 5 business days.\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."
  },
  {
    "title": "Data Engineer (Azure Data Factory)",
    "company": "Procentrix, Inc.",
    "location": "Remote in Herndon, VA 20171",
    "salary": "$130,000 - $150,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=a7123278734a55e0&bb=DthfB3g5WpsJd_sByUBtewnORlEoTc_8AOlKFmNjlDMdoF60kA-8ZDBlf5oRcugmpXm7TklmlWlCuYTkP-8_2OFU75-iSmoT7hH9bxuwKFNpe2dXrsTkXOM0UWI27BL3rxHGO2EpFFeOVDGilUJD5Q%3D%3D&xkcb=SoCt67M3sjH0q2XzQp0JbzkdCdPP&fccid=7f39976b0e3fc83c&vjs=3",
    "description": "Company Description\n\nWe offer professional services and innovative solutions that streamline business and government.\n\nJob Description\n\nAs a Data Engineer at Procentrix, you will play a key role in modernizing our data ecosystem by leading efforts to migrate on-premise data sources—including Informix databases and binary objects—into Dataverse and Azure storage. You will design, implement, and maintain Azure Data Factory (ADF) pipelines that enable seamless data movement while ensuring reliability, scalability, and compliance with security standards.\nThis role requires direct interaction with customers to review and confirm requirements, validate data migration needs, and ensure proper configuration and connectivity of the Self-hosted Integration Runtime (SHIR) for secure on-prem to cloud integration. You will collaborate with stakeholders to ensure data quality, troubleshoot issues, and provide guidance on best practices for pipeline automation, scheduling, and error handling. Your work will include building and managing data pipelines, performing transformations, handling error detection and recovery, and ensuring data is delivered efficiently for use across applications and services. Additionally, you will be responsible for configuring integrations with Dataverse, managing blob storage for binary data, and implementing automation and scheduling to support ongoing operational needs.\nThe projected compensation range for this position is $130K - $150K annualized (USD). The final salary offered will generally fall within this range and is determined by various factors, including but not limited to the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as internal pay equity, location, contract-specific affordability and other organizational requirements.\nRequired Skills\nStrong hands-on experience with Azure Data Factory (ADF), including pipeline design, data flows, and integration runtime setup.\nExperience working with on-premise databases (e.g., Informix) and migrating data into cloud environments.\nProficiency in SQL for data transformation and schema alignment.\nKnowledge of Dataverse integration using API or OData connectors.\nExperience implementing error handling, logging, and retry policies in ADF pipelines.\nFamiliarity with Azure Blob Storage and management of binary/object data.\nExperience with scheduling and automation of pipelines in ADF.\nStrong problem-solving skills, attention to detail, and ability to work independently and as part of a team.\nDesirable Skills\nExperience with Microsoft Power Platform and working with Dataverse in enterprise environments.\nFamiliarity with Azure DevOps or similar CI/CD tools for pipeline deployment and management.\nKnowledge of SharePoint integration for binary data movement.\nBackground in cloud security and access control best practices.\nPrior experience supporting large-scale data migration or modernization projects.\n\nAdditional Information\n\nProcentrix is an Equal Opportunity employer and does not discriminate on the basis of race, color, religion, gender, national origin, age, marital or veteran status, the presence of a non-job related medical condition or handicap, or any other legally protected status."
  },
  {
    "title": "Ataccama Data Quality Engineer - W2",
    "company": "Narvee Technologies",
    "location": "Remote",
    "salary": "$70 - $95 an hour",
    "url": "https://www.indeed.com/rc/clk?jk=74e3620f88ec25f1&bb=DthfB3g5WpsJd_sByUBte5S_xjd_uCmcHescrmdlgcvK4QXo1hIzSaXUBye9-eH9MkAB968beSsfw1AHNLzF51ziRDbVdWQ02pGqgzTumFE13BJ5x70XFwkkjSFnEhwypjUtJHkKHRJLhpKB127IEw%3D%3D&xkcb=SoAZ67M3sjH0q2XzQp0IbzkdCdPP&fccid=ad06d08ec18a4388&cmp=Narvee-Technologies&ti=Data+Engineer&vjs=3",
    "description": "Ataccama Data Quality Engineer\nLocation:\n(Example: Dallas, TX / Remote / Hybrid)\nRole Overview:\nWe are seeking an experienced Data Quality Engineer with expertise in Ataccama ONE to design, implement, and maintain enterprise-level data quality solutions. The role involves profiling, cleansing, standardizing, and monitoring large-scale data across multiple domains, ensuring compliance with organizational and regulatory standards.\nKey Responsibilities:\nDesign and develop data quality frameworks and rules using Ataccama ONE.\nPerform data profiling, anomaly detection, and root cause analysis to identify data quality issues across structured and unstructured datasets.\nImplement data cleansing, standardization, and enrichment workflows in Ataccama.\nConfigure data quality dashboards and monitoring solutions to track KPIs and data health.\nCollaborate with data governance, data stewards, and business teams to define data quality rules and standards.\nIntegrate Ataccama with enterprise data platforms (e.g., Snowflake, Databricks, Redshift, Azure, AWS).\nAutomate validation processes to ensure data consistency across ETL, streaming, and cloud data pipelines.\nWork closely with compliance teams to align with regulatory frameworks (GDPR, HIPAA, SOX, etc.).\nDocument data quality processes, maintain metadata, and contribute to continuous improvement initiatives.\nRequired Skills & Experience:\n3–7+ years of experience as a Data Quality Engineer or similar role.\nStrong hands-on experience with Ataccama ONE (DQ, MDM, RDM modules).\nProficiency in SQL for data validation, profiling, and reporting.\nExperience with data integration/ETL tools (Informatica, Talend, Apache NiFi, etc.).\nSolid understanding of cloud platforms (AWS, Azure, or GCP) and data lakes/warehouses.\nKnowledge of data governance and stewardship best practices.\nFamiliarity with data modeling, master data management (MDM), and metadata management.\nStrong problem-solving skills and ability to analyze large datasets for quality issues.\nJob Type: Contract\nPay: $70.00 - $95.00 per hour\nExpected hours: 40 per week\nWork Location: Remote"
  },
  {
    "title": "Senior Data Engineer",
    "company": "Canyon Associates",
    "location": "Bridgewater, NJ 08807",
    "salary": "$130,000 - $145,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=70af83cc389955bc&bb=DthfB3g5WpsJd_sByUBte0djL-X2k7Y3snrdn0xoei619dfBM08M4YkBtywpU1aPmPyGIOJVzqlwKfZ-xD1_HjiadvoT0MZK_cMf9dr1cFS9nWRd56BFHLSPZBq5dyWx3SFSZ7AGlfE_Yjs4G-WqLg%3D%3D&xkcb=SoDj67M3sjH0q2XzQp0DbzkdCdPP&fccid=c385eae2b1f77a6e&cmp=canyon-associates&ti=Data+Engineer&vjs=3",
    "description": "7 plus years experience with Python, Scala, or Java\nexperience in Oracle OLTP database design\nexperience building ETL/ELT data pipelines\nExperience developing API's REST, SOAP\nOLAP / Data Warehouse design and development\nDatabricks data intelligence experience is a must\nPowerBI/Cognos/or Tableau experience\nJob Type: Full-time\nPay: $130,000.00 - $145,000.00 per year\nBenefits:\n401(k)\n401(k) matching\nDental insurance\nEmployee discount\nFlexible spending account\nHealth insurance\nHealth savings account\nPaid time off\nParental leave\nVision insurance\nWork Location: Hybrid remote in Bridgewater, NJ 08807"
  },
  {
    "title": "Data Engineer",
    "company": "ELEVATE ME, INC.",
    "location": "California City, CA",
    "salary": "$103,141 - $111,654 a year",
    "url": "https://www.indeed.com/rc/clk?jk=e8b31d3c2c3ac938&bb=DthfB3g5WpsJd_sByUBtexaezN7_4_1xumXdjWuRJwL6w5mzFv6lQgBJYGlIdHcBqYjZkNXeK99pRcoIcudczKffdeqWzyAlrYksuvoaPso3ACgTiwN7aaW3beAlv4-oUmAm715XzHSd43snhKM0Dg%3D%3D&xkcb=SoBX67M3sjH0q2XzQp0CbzkdCdPP&fccid=911af661273d2200&cmp=ELEVATE-ME%2C-INC.&ti=Data+Engineer&vjs=3",
    "description": "Job Overview\nWe are seeking a skilled Data Engineer to join our dynamic team. In this role, you will be responsible for designing, building, and maintaining scalable data pipelines that facilitate the collection and processing of large datasets. You will work closely with data scientists and analysts to ensure that our data infrastructure supports advanced analytics and business intelligence initiatives. The ideal candidate will have a strong background in data engineering principles and a passion for working with big data technologies.\nDuties\nDevelop and maintain robust data pipelines using tools such as Talend and Apache Hive.\nCollaborate with cross-functional teams to gather requirements and understand data needs.\nImplement data models that support analytics and reporting requirements.\nOptimize SQL queries for performance and efficiency.\nEnsure the integrity, quality, and security of data throughout the lifecycle.\nUtilize cloud platforms like AWS to deploy scalable data solutions.\nAnalyze large datasets to identify trends, patterns, and insights that drive business decisions.\nStay updated with industry trends in big data technologies and analytics.\nExperience\nProficiency in programming languages such as Java and VBA.\nStrong experience with SQL for database management and querying.\nFamiliarity with linked data concepts and frameworks.\nExperience working with big data technologies including Hadoop or Spark is a plus.\nKnowledge of analytics tools and methodologies to support decision-making processes.\nAbility to vaticinate future trends based on current data patterns is advantageous.\nPrevious experience in a similar role or relevant projects is preferred.\nJoin us in leveraging the power of data to drive innovation and improve business outcomes. If you are passionate about building efficient data systems and enjoy working in a collaborative environment, we encourage you to apply.\nJob Types: Full-time, Permanent\nPay: $103,141.00 - $111,654.00 per year\nBenefits:\nHealth insurance\nWork Location: On the road"
  },
  {
    "title": "Lead Data Engineer",
    "company": "Capital One",
    "location": "Plano, TX 75023",
    "salary": "Full-time",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0C3j_zLGvpMLCdiZ0WC46XqVTA1VMZzOzKXPhAXwYlrNQzIroA7S00QBkcNNfzTUOmSTcxPURc3ZWqOCx7hYFpEqa59WkKRavxjeoDf4HvvKv-36RQDrkCwDzY9gKh94LDxnbC8adVexLhNI7WJus6Y67Kh2XemAI_9INdobQ3I_55aiDdJgTROETaps4UGgwILwMrPw2Puodi13GeFxPhJdwL2u3c1SWxvFm0dIloctoziIpakv1Ao9T_hFeaCPUbWEQGvQjgjIVEzM-0XSfFABgDsc7w6utYQurd-iOkGwXPz6r6sDH-2uRW-JTR3VkXdK7tcmQcPJPvWhf0SyFWPuVYLYZsvJdNVlqyydqSDQD-3oH4fUH1FaQ0wH3Pc75EsFMtMOPsbci9ZbSoowVugyHF1RwsT4wA_tvM5Fx7W8ECs1RcmMGI4BX2fRRgmPBNzLDXx-WP4ONlubGb_PbpATwt13IwUjdSUsTalfZ_Laza7J35yDaQT3FE1UkN7zfzKGsCtkfCcYCWFgVroABKYMUJ965dFjTo3Uqc2TNv99sRmIKUJiI4MgH2nYKsXeiAPDQArDAtNiK29TlKiNFMFTIrV2DapKSM2133DzF4rKDjfiYiRRXIRNpGsao954VezMpjH53NIFXWshZZm8ekn45H1WP7gpberq3jBAtHPFeVCoauOmvWfLxcw06ecGmPY0E9RgK2-BgvC8XCzZVOPjuH85UWCC4dqpE17mh_mbLfcbPWfWk5xHHAuB88Mt5Aoj5pAJ6kfEiNt3q_HRzMFR27PNJzsKZHI4tF_M9nZBDPRDeMAZXsX&xkcb=SoBK6_M3sjH0q33zQp0KbzkdCdPP&camk=ethIe0s0hefG8K9KHaueHA==&p=1&fvj=0&vjs=3",
    "description": "Lead Data Engineer\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Lead Data Engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\nWhat You’ll Do:\nCollaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\nWork with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\nUtilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\nShare your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\nCollaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\nPerform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\nBasic Qualifications:\nBachelor’s Degree\nAt least 4 years of experience in application development (Internship experience does not apply)\nAt least 2 years of experience in big data technologies\nAt least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)\nPreferred Qualifications:\n7+ years of experience in application development including Python, SQL, Scala, or Java\n4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\n4+ years experience with Distributed data/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n4+ year experience working on real-time data and streaming applications\n4+ years of experience with NoSQL implementation (Mongo, Cassandra)\n4+ years of data warehousing experience (Redshift or Snowflake)\n4+ years of experience with UNIX/Linux including basic commands and shell scripting\n2+ years of experience with Agile engineering practices\nAt this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer).\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\nMcLean, VA: $193,400 - $220,700 for Lead Data Engineer\nPlano, TX: $175,800 - $200,700 for Lead Data Engineer\nRichmond, VA: $175,800 - $200,700 for Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter.\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\nThis role is expected to accept applications for a minimum of 5 business days.\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City’s Fair Chance Act; Philadelphia’s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at RecruitingAccommodation@capitalone.com. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\nFor technical support or questions about Capital One's recruiting process, please send an email to Careers@capitalone.com\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC)."
  },
  {
    "title": "Azure Data Engineer",
    "company": "HSO",
    "location": "Remote",
    "salary": "Full-time",
    "url": "https://www.indeed.com/rc/clk?jk=ce84467b81e4af9d&bb=DthfB3g5WpsJd_sByUBte0x952OYzEDI2MivITROzja68C6itwkQH9ZvXeCSMfLjn7J6dGYAowF2Sj2-CT2Vi4muLTNZFIxXL6buYkgf-WS1TIdYk3GDiEQOr_jOW7kOkx7NmthG5sDXFKWV4F4HUw%3D%3D&xkcb=SoAK67M3sjH0q2XzQp0MbzkdCdPP&fccid=8f12b6b465612237&vjs=3",
    "description": "As an Azure Data Engineer you can expect to…\nDesign, build, and maintain data engineering solutions on Microsoft Azure and Fabric (Lakehouse, Warehouse, Data Factory, Synapse Analytics, Dataflows, Eventstreams).\nCoordinate sprints, daily stand-ups, and retrospective meetings as part of Agile delivery.\nCollaborate with business and technical teams to analyze, document, and validate requirements.\nPerform source system analysis and identify appropriate data ingestion and integration strategies for both batch and streaming pipelines.\nDevelop and optimize ETL/ELT solutions using Azure Data Factory pipelines, Synapse pipelines, or Fabric Data Pipelines.\nLeverage services such as Azure Data Lake Storage (ADLS), Synapse Analytics, SQL Database, Azure Functions, Event Hub, Synapse Spark, and Microsoft Fabric (Lakehouse, Warehouse, Notebooks, Pipelines) for data integration and transformation.\nWrite and optimize SQL, Python, and PySpark code for large-scale data processing.\nImplement data ingestion frameworks, incremental/delta loads, and medallion architecture patterns (Bronze/Silver/Gold).\nConnect and integrate diverse data sources (SQL, Oracle, ODBC, ERP, CRM, APIs, flat files, streaming events) into Fabric and Azure ecosystems.\nEnsure data quality, security, and governance in alignment with enterprise standards.\nContinuously research and adopt efficient methods for scalable data movement, transformation, and storage.\nWork independently and as part of a cross-functional team to deliver high-quality solutions.\nYou're great at…\nWorking in Microsoft Fabric and Azure Data Services to design, build, and optimize enterprise-grade data pipelines\nModern data engineering, advanced SQL/Spark skills, and working collaboratively in Agile environments\nSolving complex problems with creative solutions\nLearning new concepts quickly and thoroughly\nLearning new concepts quickly and thoroughly Promoting the mission and Shared Values of our company\nRequirements\nSound interesting? If so, you’ll have…\n3+ years of consulting experience in Data Engineering/Data Warehousing.\nStrong hands-on expertise with Azure Data Factory, Synapse Analytics, Fabric Lakehouse, and ADLS.\nProficiency in SQL, Python, and Apache Spark (PySpark/Synapse/Fabric Notebooks).\nExperience integrating batch and real-time data sources into Azure/Fabric.\nSolid understanding of Relational Databases, Analytical Databases, and NoSQL systems.\nStrong problem-solving ability and a self-starter mindset with the ability to learn new technologies quickly.\nExperience with PowerShell or Bash scripting for automation\nFamiliarity with CI/CD pipelines (Azure DevOps, GitHub Actions) for data engineering solutions\nKnowledge of Data Governance & Cataloging (Microsoft Preview)\nExperience with Synapse link/Fabric Link\nBenefits\nWe offer competitive pay with and performance-based bonus. Our employees also enjoy unlimited paid time off and a flexible and affordable benefits program designed to help you be and stay well, including: medical, dental & vision coverage, flexible spending accounts, health reimbursement account, and a 401(k) plan with a company match. Additionally, you’ll have the benefit of working alongside enthusiastic and energetic teammates in a dynamic and thriving environment.\nHSO is an Equal Opportunity Employer.\n\n#LI-FD1\n#LI-REMOTE"
  },
  {
    "title": "Data Engineer - Mid level (5 years experience) - NewYork",
    "company": "Hire Orbitt",
    "location": "Hybrid work in New York, NY 10017",
    "salary": "$100,000 - $150,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=f0d4751e18400ce6&bb=DthfB3g5WpsJd_sByUBteyhvnoPx9xDsxqhZZV5Ec0OJX3TiUq-g3naWuHYqEsFNnayQNxusfaLqZGDIqP_17WLii7_DfN7t1eII7sTxKqyU5AfOwlrbwtK3olDgeKmrUoeRLEpC81lgcj309ogw3Q%3D%3D&xkcb=SoCX67M3sjH0q2XzQp0PbzkdCdPP&fccid=d98e503c1b6a0173&cmp=Hire-Orbitt&ti=Data+Engineer&vjs=3",
    "description": "Job Summary\nWe are looking for a Data Engineer to join our Data & Analytics team.\nThis role is ideal for someone with at least 5 years of experience as Data Engineer who is eager to learn, contribute, and grow into a strong data engineering professional.\nThe successful candidate will have strong skills in SQL and Python, be familiar with ETL concepts, and have the ability to work with stored procedures in SQL Server.\nExposure to Snowflake and Tableau is a plus.\nExperience in AI/ML concepts or projects will also be considered a plus.\nReporting to: Head of Data Architecture and Reporting Strategy\nKey Responsibilities\nDevelop, optimize, and maintain SQL Server stored procedures, views, and queries.\nDesign, build, and maintain ETL/ELT pipelines to move and transform data from multiple sources.\nWrite and maintain Python scripts for data wrangling, automation, and reporting.\nEnsure data quality, integrity, and consistency across platforms.\nCollaborate with analysts and business stakeholders to analyze and interpret data.\nAssist in data warehouse development and pipeline optimization within Snowflake.\nDocument technical processes, workflows, and best practices.\nQualifications\nBachelor’s degree in Computer Science, Information Systems, Data Analytics, or equivalent experience.\nStrong proficiency in SQL, with the ability to write and troubleshoot complex queries.\nHands-on experience with Python for data processing and automation.\nSkilled in ETL concepts, data modeling, and data management best practices.\nKnowledge of SQL Server stored procedures and optimization techniques.\nFamiliarity with Snowflake or other cloud-based data warehouses (a plus).\nExperience with Tableau or other BI tools (a plus).\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and collaboration skills.\nPreferred Experience\nExposure to cloud platforms such as AWS, Azure, or GCP.\nExperience with Git or other version control systems.\nExposure to AI/ML concepts, frameworks, or projects (a plus).\nUnderstanding of data governance and documentation best practices.\nJob Type: Full-time\nPay: $100,000.00 - $150,000.00 per year\nBenefits:\n401(k)\nDental insurance\nHealth insurance\nPaid time off\nVision insurance\nWork Location: Hybrid remote in New York, NY 10017"
  },
  {
    "title": "Data Engineer",
    "company": "Sprout Social",
    "location": "Remote in Chicago, IL",
    "salary": "$133,056 - $199,584 a year",
    "url": "https://www.indeed.com/rc/clk?jk=51d87df22e512e8b&bb=DthfB3g5WpsJd_sByUBte0x952OYzEDIXut2nZ96lDQUBui1YcV-P07MmZrS7kxwxOOIcAzJZGs7yE3j5cKYHdNjX-d0bkgmvFl62jsSaiiHTnE6Sn4UKyE6YhhY3Pcewa6YrG9bKUOlrPL-tYWk-Q%3D%3D&xkcb=SoCE67M3sjH0q2XzQp0LbzkdCdPP&fccid=9d8433ffeb95e981&vjs=3",
    "description": "Sprout Social is looking for a Data Engineer to join our Data Foundations team. This team builds the internal data infrastructure, pipelines, and products that empower analytics, data science, and business stakeholders across Sprout. While our software engineers are focused on delivering customer-facing platform features, our data engineers specialize in ensuring data is reliable, well-modeled, and accessible to fuel smarter decisions and internal innovation.\nWhy join Sprout Social's Data Engineering team?\nSprout Social empowers businesses worldwide to harness the immense potential of social media in today's digital-first world. Processing over one billion social messages daily, our platform delivers insights and actionable intelligence to more than 30,000 brands. These insights guide strategic decisions, drive growth, and foster deeper connections with customers.\nOur Data Foundations team plays a critical role in this by enabling Sprout's internal stakeholders—analytics, product, finance, sales, and beyond—to work with trustworthy, scalable, and reusable data. You'll be helping build the pipelines, curated datasets, and data products that unlock value across the business and extend Sprout's data-driven culture.\nWhat you'll do\nDesign and maintain ETL/ELT pipelines and data workflows that enable reliable, timely, and scalable data flows.\nCollaborate with analysts, scientists, and business stakeholders to deliver curated datasets and internal data products.\nImplement best practices in schema design, data modeling, and metadata management.\nOwn and evolve internal data infrastructure for quality, monitoring, and discoverability.\nPartner with software engineering teams where application data intersects with internal pipelines—ensuring business-critical data is clean, structured, and usable.\nWhat you'll bring\nWe're looking for a data engineer with a strong foundation in data infrastructure and a passion for enabling others to succeed through high-quality data.\nThe minimum qualifications for this role include:\n2+ years of professional experience in data engineering or at least 2 years of hands on experience building, deploying and maintaining production-grade data infrastructure and pipelines\nDemonstrated proficiency in SQL (e.g. MySQL, PostgreSQL) and data modeling; with experience in a variety of business domains. Proficiency in Python.\nExperience working in relational and non-relational databases\nHands on experience with ELT + transformation frameworks (e.g., dbt) and with orchestrators (e.g., Airflow, dbt. dagster).\nExperience building internal data products (curated datasets, semantic layers, or reusable modeling frameworks).\nProven experience applying standard software development practice to data engineering, including testing, version control, code reviews, incident management, incident CI/CD, documentation, and observability for data.\nPreferred qualifications for this role include:\nExperience building and maintaining data infrastructure using transformations with dbt or similar tools(and managing semantic layers for Business intelligence (BI) dashboards (e.g. Tableau, Hex, Looker).\nExperience implementing data quality frameworks, testing methodologies, and monitoring practices to ensure data integrity.\nHands-on experience with event-driven or streaming frameworks (Kafka or NSQ, Kinesis, Pub/Sub).\nProven ability to collaborate withcross-functional teams and effectively communicate complex technical concepts to non-technical stakeholders.\nDirect experience with cloud infrastructure (AWS, GCP, or Azure) and implementing cost optimization strategies for data platforms.\nHow you'll grow\nWithin 1 month, you'll plant your roots, including:\nComplete Sprout's New Hire onboarding program and meet peers across Data Foundations, Data Science, Engineering.\nLearn the team's existing data stack, pipelines, and modeling frameworks.\nShadow teammates to understand how internal stakeholders use curated datasets today.\nPartner with your manager to scope your first pipeline or modeling task to own.\nWithin 3 months, you'll start hitting your stride by:\nDelivering your first production-ready pipeline, dataset, or internal data product.\nCollaborating with analysts and data scientists on requirements for reusable modeling layers.\nGaining deeper familiarity with Sprout's data warehouse(s) and orchestration environment—and starting to suggest improvements.\nWithin 12 months, you'll make this role your own by:\nTaking technical ownership of a set of pipelines or data products relied on by stakeholders across the business.\nProposing and implementing improvements to our pipelines for scalability, reliability, or cost efficiency.\nActing as a mentor to newer members of the Data Foundations team while continuing to grow your own expertise.\nHelping shape the team's roadmap and long-term data foundations strategy.\nOf course what is outlined above is the ideal timeline, but things may shift based on business needs and other projects and tasks could be added at the discretion of your manager.\nOur Benefits Program\nWe're proud to regularly be recognized for our team, product and culture. Our benefits program includes:\nInsurance and benefit options that are built for both individuals and families\nProgressive policies to support work/life balance, like our flexible paid time off and parental leave program\nHigh-quality and well-maintained equipment—your computer will never prevent you from doing your best\nWellness initiatives to ensure both health and mental well-being of our team\nOngoing education and development opportunities via our Grow@Sprout program and employee-led diversity, equity and inclusion initiatives.\nGrowing corporate social responsibility program that is driven by the involvement and passion of our team members\nBeautiful, convenient and state-of-the-art offices in Chicago's Loop and downtown Seattle, for those who prefer an office setting\nWhenever possible, Sprout wants to provide our team with the flexibility to work in the location that makes the most sense for them. Sprout maintains a remote workforce in many places in the United States. However, we are not set up in all states, so please look at the drop-down box in our application to see whether your state is listed. Few roles require an office setting. If your position requires a physical presence in a Sprout office, it will be evident in the job listing and your offer letter.\n\nIndividual base pay is based on various factors, including work location, relevant experience and skills, the responsibility of the role, and job duties/requirements. In the United States, we have two geographic pay zones. You can confirm the pay zone for your specific location with your recruiter during your interview process. For this role, our current base pay ranges for new hires in each zone are:\nZone 1 (New York, California, Washington): $133,056 (min), $166,320 (mid), $199,584 (max) USD annually\nZone 2 (All other US states): $121,000 (min), $151,200 (mid), $181,400 (max) USD annually\nThe listed ranges represent the full earning potential in this position. Starting salaries for well-qualified new hires are typically around the midpoint of the range. These ranges were determined by a market-based compensation approach; we used data from trusted third-party compensation sources to set equitable, consistent, and competitive ranges. We also evaluate compensation bi-annually, identify any changes in the market and make adjustments to our ranges and existing employee compensation as needed.\nBase pay is only one element of an employee's total compensation at Sprout. Every Sprout team member has an opportunity to receive restricted stock units (RSUs) under Sprout's equity plan. Employees (and their dependents) are covered by medical, dental, vision, basic life, accidental death, and dismemberment insurance, and Modern Health (a wellness benefit). Employees are able to enroll in Sprout's company's 401k plan, in which Sprout will match 50% of your contributions up to 6% with a maximum contribution. Sprout offers \"Flexible Paid Time Off\" and ten paid holidays. We have outlined the various components to an employee's full compensation package here to help you to understand our total rewards package.\nSprout Social is proud to be an Equal Opportunity Employer and an Affirmative Action Employer. We do not discriminate based on identity- race, color, religion, national origin or ancestry, sex (including sexual identity), age, physical or mental disability, pregnancy, veteran or military status, unfavorable discharge from military service, genetic information, sexual orientation, marital status, order of protection status, citizenship status, arrest record or expunged or sealed convictions, or any other legally recognized protected basis under federal, state, or local law. Learn more about our commitment to diversity, equity and inclusion in our latest DEI Report.\nIf you require a reasonable accommodation for any part of the interview process or to submit your application, please email us at accommodations@sproutsocial.com. Include the nature of your request and your preferred contact information. We'll do everything we can to support your success during our recruitment process while upholding your privacy. Please note that only inquiries regarding accommodations will receive a response from this email address; other inquiries will not be addressed (e.g., you send your resume but are not requesting an accommodation).\nFor more information about our commitment to equal employment opportunity, please click here (1) Equal Opportunity Employment Poster (2) Sprout Social's Affirmative Action Statement (3) Pay Transparency Statement.\nAdditionally, Sprout Social participates in the E-Verify program in certain locations, as required by law.\n#LI-REMOTE\nSprout Social Inc. and its subsidiaries process personal data submitted through your application to assess your qualifications for employment and to inform our hiring decision and, where applicable, for required governmental reporting. For more information, please review Sprout's Global Applicant Privacy Notice."
  },
  {
    "title": "Azure Data Engineer",
    "company": "Fusion Global Solutions LLC",
    "location": "Hybrid work in Fort Worth, TX 76102",
    "salary": "$126,458.77 - $152,294.44 a year",
    "url": "https://www.indeed.com/rc/clk?jk=1a3fb0f98dcff09c&bb=DthfB3g5WpsJd_sByUBte6LuZdmfP0vRQGuo49ukFMvjGkO8_ClQ3A0MOdyfgNcgYWb4wlIODQpT0XxTGeizHifP977QYWxd_RLzfnnf4JiHMz_Spi7T1yJvOUH4VW4y2WwMJqvPdN3Xy8h2YdcPHw%3D%3D&xkcb=SoDK67M3sjH0q2XzQp0BbzkdCdPP&fccid=ff6a723de373b509&cmp=Fusion-Global-Solutions-LLC&ti=Data+Engineer&vjs=3",
    "description": "Azure Data Engineer\nFort Worth TX for 12+ Months\nBachelor’s degree in Computer Science, Computer Engineering, Technology, Information Systems (CIS/MIS), Engineering or related technical discipline, or equivalent experience/training\n3 years software solution development using agile, DevOps, operating in a product model that includes designing, developing, and implementing large-scale applications or data engineering solutions\n3 years Data Engineering experience using SQL\n2 years of cloud development (prefer Microsoft Azure) including Azure EventHub, Azure Data Factory, Azure Databricks, Azure DevOps, Azure Blob Storage, Azure Power Apps and Power BI.\nCombination of Development, Administration & Support experience in several of the following tools/platforms required:\nScripting: Python, PySpark, Unix, SQL\nData Platforms: Teradata, SQL Server\nAzure Data Explorer. Administration skills are a plus\nAzure Cloud Technologies: Azure Data Factory, Azure Databricks, Azure Blob Storage, Azure Power Apps, and Azure Functions\nCI/CD: GitHub, Azure DevOps, Terraform\nBI Analytics Tool Stack – Cognos, Power BI\nJob Type: Contract\nPay: $126,458.77 - $152,294.44 per year\nWork Location: Hybrid remote in Fort Worth, TX 76102"
  },
  {
    "title": "",
    "company": "",
    "location": "",
    "salary": "",
    "url": "https://www.indeed.com/viewjob?jk=890abcdef0123456",
    "description": "None"
  },
  {
    "title": "Insider Threat Program Database Engineer",
    "company": "Leidos",
    "location": "Washington, DC 20090 \n(South West area)",
    "salary": "$126,100 - $227,950 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CZUO70VSdYKA8PR3jfrSh5ljhqJhfDt0PzQCMubt8cRosbSX75HEeA45s9n5xrWx0SzpMdBSYPrH-wIe9lyirGfHVFTielh8eVNkObifx5ILcDfbrk2mhFkppOZN1gxpvel8g7ohjNzfCds1AGswLivQRaO2eGflpZW_dXfAV9eAhKjGIX6iM6qqdchdGCpkzStyQY2SOacxQceyelDH4Belak5L2C6IQXic2H9R_x3s4Rzu5txaxsJRs1wBY-yVPdTgfNSSq-_8cX3y35KHH-1BhSp3VTg_znMGMoY_BHAp33ARDcuII23LmvtZTiKzwk37fgcLTVQ1oYLke-DHAlzZoeVGw0weKxi7ntyecMrC5TVje-LF5P3jjRp7XgrQD13Z3H44oGTakg05cUX0Qs9X0RPQlRUsitSjTo590m_j8EevUPxPIcjr3xmq943F2GCCTFSRK8tEqqCXNEoB-ZIpME8Xzcdf6AQx9NGQljkbH0ziN8CyajL07HrpxQPCiqptx1PXWx9PcxxEDsYrfM5hvFSuu6aMaVVv10O_zHIE-qFNSoXqnTIpHub0hSpbJEkhAzZfsH760spVD7gQ9jUA-fBDOERC55TaWBJB5bTc9m3vdr635MUtGW6ptkSjaccJKmJ-RNqYwxbLWNfZCVjwpaDeQmZ3XHEmm1LuwW-mca7BtTsUiq3uH0WSg-2LQyN4FFhcSmPAPqxv2He-tQAy3GsO7S4shSvIli39Wvx7zetvOPIphrwFi7mAxumOs=&xkcb=SoAg6_M3sjYKsXygMx0DbzkdCdPP&camk=ethIe0s0hecezcnkRTMseg==&p=7&fvj=0&vjs=3",
    "description": "Description\nLeidos is seeking a skilled Database Engineer to support the DHS Insider Threat Program (ITP) under the HEITS Contract. This is a dynamic opportunity to apply your expertise in designing, sustaining, and evolving the database infrastructure that powers one of DHS’s most critical missions—identifying and mitigating insider threats through advanced analytics, monitoring, and data correlation.\n\nResponsibilities\nAs the Database Engineer, you will:\nMaintain and support the ITP database backend infrastructure\nPerform routine maintenance: reindexing, backups, transaction log truncation, cluster management\nDesign and implement database encryption solutions (e.g., Transparent Data Encryption - TDE)\nDocument and maintain configuration baselines\nDevelop and execute system recapitalization plans to support capacity growth\nServe as the SME for all database infrastructure\nCreate and optimize stored procedures, queries, and views\nSupport data warehouse features and integrations\nOptimize database schemas for referential integrity, cascading transactions, indexing, and performance\n\nBasic Qualifications\nBachelor’s degree with 12+ years of relevant experience, or Master’s degree with 10+ years\nExpertise in Oracle database management, upgrades, and maintenance\nExperience with SQL, PostgreSQL, and other database types\nAbility to integrate diverse data types into cloud containers (e.g., AWS S3)\nStrong understanding of DBMS and database design principles\nExperience with database migrations and data lake/repository connectors\nEligibility to obtain DHS EOD SCI clearance\n\nPreferred Qualifications\nMaster’s degree in IT Management, Engineering, or related field\n10+ years of experience in IT service delivery management\nFamiliarity with User Activity Monitoring platforms\nExperience with Everfox High Speed Guard Platform\nProficiency with Oracle, Microsoft SQL, PostgreSQL, or other DBMS platforms\nCome break things (in a good way). Then build them smarter.\nWe're the tech company everyone calls when things get weird. We don’t wear capes (they’re a safety hazard), but we do solve high-stakes problems with code, caffeine, and a healthy disregard for “how it’s always been done.”\nOriginal Posting:\nOctober 2, 2025\n\nFor U.S. Positions: While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.\n\nPay Range:\nPay Range $126,100.00 - $227,950.00\nThe Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law."
  },
  {
    "title": "Core Processor Application Dev - Data & Analytics Engineer",
    "company": "MISSION FEDERAL CREDIT UNION",
    "location": "Hybrid work in San Diego, CA 92131",
    "salary": "$95,000 - $125,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0BmpNkHALyw7SPSmIFt_Bmxi7kg9utXhkUQERbJpS4GVLO4FmohB78kPaNUeYAkC3sGbhjYkjenyqLrORFO1PeLdg8c6-m5bdvnOks70iXC4Jy7k5-XpFQwyvnT4cu2YClJzJ8XlkG6HmUHNECTMfMAjext4OF01SArty213RXPS4n24UpBuyxOJmZsVNQewtev3GA50Ub1IwjpuK_mFP9l3if375OaBQLuVrr9jJ1PHB6w6kUAWVKMtwJUUeqhrNb2SmXkChOvUKaPK0nwBaYErlu-B2-Q9woicSmmi-jrDV0Oxhs9vPc_W-ZKHOXhrjVky430j0YOH7R1qyCJv3Jljx5vuS8AVUkzxEy2JM8zQYuzlAS9qgRGQD5y7HUdUjTA_2e-8-I02iX2Sq-7A_L-foaS6aBVzh5LmO3JLt1fBnj61x2bGUhBqn2-hkcvIYnFuoUAdh0pQ3Tq3HiDtfD9rgn7Rsz-mHsPaLuZFw7HLzAx-7s31SzJiVgxYEuZetuUFHDEQBRi0qghkjDHCgLzo-QzZvt7tbymO4ssZ7nwgtSQO2rGT2iaKFdGibUO1_40zcQiSuh21b3RfwXvdtxET-TKcgfkRbpAcvYHWEMPTzHMOkNOSrdV8mflsrR2AsWHYuH-Gj_zi7HNTjoq68n98bVJWch_ViBe3w10bFprpA==&xkcb=SoAz6_M3sjYKsXygMx0HbzkdCdPP&camk=CPChkbYSGj8lsgwe1hfudg==&p=11&fvj=0&vjs=3",
    "description": "Are you passionate about designing, developing, testing and deploying data reporting and systems? Do you like to work collaboratively with other programmers, departments and stakeholders to manage deployments? Are you the subject matter expert in Power BI? If so, this Data & Analytics Engineer position might be the role for you!\n\nIf this sounds interesting to you, below are a few more details.\nSQL Server experience is critical to be successful in this role.\nExperience in both data engineering and analytics is a plus! Must be detailed oriented when switching gears to the SSIS and SSRS side of your responsibilities.\nBeing knowledgeable and comfortable in Power BI with back-end semantics data modeling is required.\nBeing end-user focused when developing software is crucial.\nLove being in-the-know by keeping up with the current trends and issues in the domains of data engineering.\nIf you are interested in career progression and are looking for great leaders to mentor and train you, we have a great learning environment to support you towards the next level of your career!\nHere is what we are looking for:\n5 years of data engineering experience is preferred.\nBachelor’s Degree in Computer/Software Development, Information Technology or related discipline is a must. However, two years of equivalent experience can substitute for every one year of education.\nWhat we offer:\nGreat team!\nYou can’t beat a role in sunny San Diego!\n18 days of PTO in your first year plus 12 holidays a year!\n6% 401(k) match\nFull benefits package including medical, dental, vision, life insurance, etc.\nIf this sounds like an amazing opportunity to you (because ya, it is!), we want to hear from you!\n\nBase Pay/Salary: *$95,000 - $125,000 per year\n\nActual base pay within this range will be determined by several components, including but not limited to, relevant experience, internal equity, skills, qualifications, and other job-related factors permitted by law.\nYour privacy is very important to Mission Federal Credit Union. The California Consumer Privacy Act (“CCPA”)/ California Privacy Rights Act (CPRA) requires Mission Federal Credit Union to inform California residents, including job applicants, of the categories of personal information we collect and the purpose for which the personal information will be used. This job applicant notice and the CCPA/CPRA notice provides the disclosures required by the CCPA/CPRA and applies only to applicants who are subject to the CCPA/CPRA.\nMission Federal Credit Union is an Equal Opportunity Employer. All applicants will receive consideration without regard to race, sex, color, creed, religion, age, marital status, sexual orientation, national origin, physical or mental disability, veteran status, or any other class protected by law. INDMF #LI-Hybrid"
  },
  {
    "title": "Principal Data Engineer",
    "company": "SiTime Corporation",
    "location": "Santa Clara, CA 95054",
    "salary": "$131,000 - $181,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=3aded212abb2af8c&bb=-pk6i8MrsVJui5CucMaHlCxepMd1OMnmdh5jYA54a60XNXIuSPLF5QZUSY_DGgyRaAJ0MYzyX3BlVaXlCjPc7liobLSvMhD-0IUdqKLNMphml0cdcYOrAdg8ciQMPmo--weO7sQFlFASb1qyq3NfTA%3D%3D&xkcb=SoCg67M3sjYKsXygMx0PbzkdCdPP&fccid=a960d054c593b9d2&vjs=3",
    "description": "About SiTime\nSiTime Corporation is the precision timing company. Our semiconductor MEMS programmable solutions offer a rich feature set that enables customers to differentiate their products with higher performance, smaller size, lower power and better reliability. With more than 3 billion devices shipped, SiTime is changing the timing industry. For more information, visit www.sitime.com.\nResponsibilities:\nData Architecture & Strategy:\nDesign and architect scalable data flows between multiple business systems, ensuring seamless integration and optimal performance\nDefine data architecture standards and best practices across the organization\nLead technical decision-making for data infrastructure investments and technology stack evolution\nInfrastructure & Pipeline Management:\nBuild, maintain, and optimize data infrastructure including databases, data warehouses, and data lakes with focus on scalability and reliability\nDevelop and maintain robust ETL/ELT pipelines from various sources into analytics-ready formats\nEngineer solutions that support both operational reporting and advanced analytics use cases\nData Quality & Governance:\nEstablish comprehensive data quality frameworks including validation, cleansing, and monitoring processes\nImplement data security controls, access policies, and compliance measures for sensitive information\nDrive data governance initiatives and documentation standards\nMonitor data systems and pipelines for performance bottlenecks, troubleshoot issues, and implement optimizations to enhance efficiency and reliability.\nUnderstanding of data governance principles and the ability to implement relevant policies.\nCross-Functional Leadership:\nPartner with data scientists, analysts, and business stakeholders to translate requirements into technical solutions\nMentor junior team members and provide technical guidance across data initiatives\nServe as the primary technical liaison for data-related projects and decisions\nQualifications & Requirements (Education must be included):\nTechnical Expertise:\n10-15 years of experience in data engineering/architecture roles\nAdvanced proficiency in Python, SQL, and modern data stack technologies\nDeep expertise with Snowflake, AWS cloud services, and big data architectures\nExperience integrating Oracle ERP and Salesforce CRM systems\nLeadership & Communication:\nProven track record of designing and implementing data solutions for growing organizations\nExceptional communication skills with ability to explain complex technical concepts to non-technical stakeholders\nStrong analytical and problem-solving skills to address complex data challenges and design effective solutions.\nExperience working in fast-paced, resource-conscious environments where wearing multiple hats is essential\nPreferred Qualifications\nExperience with AI/ML data pipeline requirements and model deployment workflows\nBackground in semiconductor, HiTech or similar regulatory/compliance environments\nExperience with data visualization tools and self-service analytics platforms\nDesired Characteristics & Attributes:\nStrategic thinker who understands how data infrastructure decisions impact business outcomes\nContinuous learner with intellectual curiosity about data trends, business applications, emerging data technologies and industry best practices\nSystems thinking approach with ability to see the big picture while managing technical details\nCustomer service mindset when supporting data consumers across the organization\nThe actual wage offered may vary depending on work location, experience, education, training, external market data, internal pay equity, or other bona fide factors.\nSiTime compensation packages includes base salary, bonus based on achieving your innovation goals and equity.\nBenefits offered : 401k plan, health and wellness that includes medical, dental, vision, life, parental leave, legal services, and time off plans.\nSiTime is an Equal Opportunity Employer. We treat each person fairly and we do not tolerate discrimination or harassment against anyone on the basis of any protected characteristics, including race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, pregnancy, political affiliation, protected veteran status, protected genetic information, or marital status or other characteristics protected by law. SiTime participates in the E-Verify program.\nLearn More about SiTime: Review the Get to Know SiTime section of our career page to explore our culture, values, and what makes us unique.\nInnovation on Top – Philosophies of Innovation with Rajesh Vashist\nFabrication Knowledge – An Interview with Rajesh Vashist\nSiTime Corporation – YouTube\n#LI-SITIME"
  },
  {
    "title": "Guidewire Policy Center Principal Data Engineer",
    "company": "Liberty Mutual",
    "location": "Boston, MA",
    "salary": "$117,000 - $221,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DepOg7TDxGKZPUDK5aMJwXQn2YTYIL3PVVUU2ENsp1lRUjI0nRYCSPuDtxtiPgr7SvzHq9HqNatUVUgXdekRLFslWDJzRWNtW5gSNGWouuypNrPL52sUO7DSZjepwmPqaNjtpn3dmT-N2DtS6ZTMenyrQx3Xf_rQ_IoeX0SnzRdC8LsBR0b19W1ShMvtVzacNHVAYX0s5tAXtStpuG7eF43SM5NMS5GpVggD-DxXPr2ixmdsZbxpvZeE9EBgZGoZWKcRTG9IHlQuNQfhG8IgNsL9VpMSeMVmmUOSZMK9wbNvvS6S7c4xDF-BpVBk5a7SQ7CjlIWb7xz7itiKlCpqs114jyqWX1rkrpcr3UqZhfV2qrXpYV8xeobnK0ZyLeZaUl5U5qhDhaLN5d_skzNBOltHr9j4VxctcRIaP6LprqnPezyqVeJ52yD2HsmztC2sYtcqI2IlaMHVdJ7wc7Q40aYrjOuaDBXHvuRecUgkJJ2oVU3sadvYuIjXON_Lv93GWvbootgYo4JC0e-Dc5MqalEwpAYJmy0P5wALvyItfa4oASYkoiCs7T2kghDIeuzC03rtamD6fK7YuNcVz2KfOgeybbmkh9CPJG5ZlEkvpfvJXTsnwcxQXknPYEpILn8unUSEGki9lD1Yn9K9P7Zhkn_gkiOWC4EcK3g1bltH6XWbrFD21qDkriF3H1L1tCPUbt1_WRc1jTT4_5ugNykzB6-p3Eti2jlC482_qxInKtYTcJlRFywvWni8kX_HQcas8qxJkP0ElHP5c1O7bL5dA9dMcFysUq5XkcfvYtH7WUd5Gdz7R7_CBZ_AuOku4v-N-2RCRqCfwonWvwbtwT8UVzF8rHWyU_-SbXb2rkhglkuVAsBf35Xezf4lYbrLrJBmIgy8KHZskY6Kq3pTZeI92w1SwOQRj85el6YXZ7dshK2NaAAyaJqDCdj_qnsMAxvQiVYi-LyBZOJQ==&xkcb=SoAa6_M3sjYKsXygMx0FbzkdCdPP&camk=nUmJqO2E8rhk7fk8d7xYKA==&p=13&fvj=0&vjs=3",
    "description": "Description\nGuidewire Policy Center Principal Data Engineer\nAt Liberty Mutual, technology isn't just a part of our business, it's what drives us forward. We deliver our customers peace of mind every day by helping them protect what they value most. Our passion for placing the customer at the center of everything we do is driving a transformational shift at Liberty Mutual. Operating as an Agile team within a Fortune 100 company, we are on the front edge of an IT transformation for how people work and deliver solutions.\nLiberty Mutual United States Retail Market Data and Analytics Engineering is actively searching for a highly productive member of a distributed, dynamic agile team to serve as a Guidewire Policy Center technical expert designing, developing, analyzing & testing innovative Data Lakehouse reporting and analytics solutions. This Principal Data Engineer will join an energetic and engaged Business Data Solutions Engineering team focused on delivering exceptional value to our Product, Pricing and Underwriting business partners. As a Principal Data Engineer, you will work collaboratively in an agile squad to design and build data pipelines & workflows, ingest, curate & provision data workflows in a Cloud-based environment as well as own responsibility of thorough end-to-end testing.\nThis is a fast-paced environment providing rapid delivery for our business partners. You will be working in a highly collaborative environment that values speed and quality,\nwith a strong desire to drive change and foster a positive work environment. You will have the opportunity to help lead this change as we grow this culture, mindset and capability.\nWe encourage you to apply if this interests you:\nWork as ONE team committed to excellence\nModel and promote a Data First attitude\nHelp advance Data Engineering operations into the future\nWork with a modern tech stack\nIn this role you will:\nWork in a dynamic and exciting agile environment with Scrum Masters, Product Owners, and team members to develop creative data-driven solutions with our ETL pipeline that meet business and technical initiatives.\nPromote and contribute to a High Performing Engineering Culture.\nDesign and develop solutions to support ingestion, curation and provisioning of complex enterprise data to achieve analytics, reporting, and data science\nAnalyze, develop and execute data integration solutions, to manage the information lifecycle needs of an organization.\nActively participates in and often leads peer development and code reviews, with focus on test driven development and Continuous Integration and Continuous Development (CICD).\nDesigns and builds data provisioning workflows/pipelines, physical data schemas, extracts, data transformations, and data integrations and/or designs using ETL and API microservices\nBuilds data architecture and applications that enable reporting, analytics, data science, and data management and improve accessibility, efficiency, governance, processing, and quality of data.\nImprove speed to market by focusing on current data needs as well as building out long-term strategic data solutions using AWS, Snowflake, SQL, Informatica, as well as other modern data technologies\nDemonstrate an open-minded and collaborative approach to creating innovative technical solutions\nContinuously learn to maintain strong knowledge of technology enablers\nMentor new and junior developers\nProvide successful deployment and provisioning of data solutions to production or other required environments.\nAnalyze complex technical problems and is expected to recommend process improvements that address complex technology gaps within a single business process and improve data reliability, quality, and efficiency\nQualifications\nBachelor's or Master's degree in technical or business discipline or equivalent experience, technical degree preferred.\nGenerally, 5+ years of professional data engineering experience\nHighly proficient in data engineering languages and tools, and strong proficiency in general programming languages and frameworks; ability to develop on multiple platforms.\nRequired - Experience integrating Guidewire systems creating Data warehousing solutions in Snowflake for reporting, analytics, or data warehousing purposes, often using APIs and ETL tools.\nExtensive understanding of agile data engineering concepts and processes, such as CICD, pipelines, and iterative development and deployments\nDemonstrated experience delivering data solutions via agile methodologies on AWS, S3, Athena, Snowflake, IDMC ETL\nRequires critical thinking, data analysis, and data modeling experience\nMust be proficient in SQL and experience with Python and JavaScript\nExperience with ETL (Informatica PowerCenter/IDMC) and knowledge of variety of data platforms and ingestion patterns including eventing & streaming\nDemonstrate leadership and active pursuit of optimizing data, CI/CD process and tools, test frameworks & practices\nMust be proactive and self-driven, demonstrate initiative and be a logical thinker.\nStrong leadership, communication, collaboration skills with a track record of taking solution ownership\nThorough knowledge in the following areas: Personal Line P&C insurance, general IT concepts, strategies and methodologies, thorough knowledge of new data architecture principles and concepts, thorough understanding of layered systems architectures, extensive knowledge of business operations, strategies and objectives.\n\nAbout Us\nPay Philosophy: The typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. The full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. Some roles at Liberty Mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role.\n\nAs a purpose-driven organization, Liberty Mutual is committed to fostering an environment where employees from all backgrounds can build long and meaningful careers. Through strong relationships, comprehensive benefits and continuous learning opportunities, we seek to create an environment where employees can succeed, both professionally and personally.\n\nAt Liberty Mutual, we believe progress happens when people feel secure. By providing protection for the unexpected and delivering it with care, we help people embrace today and confidently pursue tomorrow.\n\nWe are dedicated to fostering an inclusive environment where employees from all backgrounds can build long and meaningful careers. By actively seeking employee feedback and amplifying the voices of our seven Employee Resource Groups (ERGs), which are open to all, we create an environment where every individual can make a meaningful impact so we continue to meet the evolving needs of our customers.\n\nWe value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits\n\nLiberty Mutual is an equal opportunity employer. We will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran's status, pregnancy, genetic information or on any basis prohibited by federal, state or local law.\n\nFair Chance Notices\nCalifornia\nLos Angeles Incorporated\nLos Angeles Unincorporated\nPhiladelphia\nSan Francisco"
  },
  {
    "title": "Cyber Data Analytics Engineer",
    "company": "CACI",
    "location": "St. Louis, MO 63118 \n(Benton Park area)",
    "salary": "$75,200 - $158,100 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DeKde-pU_olD4YUrw_gjyRI1n-4QX0HDStiN2-PsFXQb5rgC9OPV9OahcntPMV-TDdWnWRtrIJgHo5OQj1cK6fNjL6ItIZSJiWIRbXdQov4Mh6N4agx08-IlutnSw8AOYsM2CK2Zg0OwLli8Tg4TJ4qCjZ7i8vWFp2Je8kLtTw92-f8lv7L10Rpm59VnbNa4AQkWZ6eZpSGtUgK2iOc54RMUNPli0qv8Xo-NEXaXqtUsquSykYv1SHYwK0EAv-Ov462jVgEqB5TI8hCzi_kEdIcARpqCoxA3lB8nDo3vbB4oh3NP2AHJs6emmc4x1_EzhyNMV6gbbXE7aqy8RoKSg3LUFuqHibJQ98ZuGSOk177ynoxMOeoPHVEScjwi8sgwngn53ReNO_fX4cvr9DfVJKu-T0BVL3HTwbMpFO9odOkgKaLY5hjVqSSEZMyy4oxsjsFtHQmwLaveBTNYTHLdFJifzAbK8hJj1ehE6bzuvmn1Jy5_cUA4aAG8girqzWcuf-ybFRWPjfEkHwK89q4-jd58JKv2O_-2jKQaD7jXAo_Eqm8MfXU_cCbledwXOxAwYUD8vYU8hti4uZC-feDVasoR3_jdnx8YmEGW5VJu6cKEH-ryQhyDT6RV2zss0Ebk_yM-PCi_9wxMPSvK27aI4CYn9O9D1wE8UDfq2X2r-kJgjpjV3holAokMcNnFb8bmG65fnp1XEx2322nfRd3e5zH44EqQI96RkA0JrID5l2B4Rd4WqtEdBKyLAkAaC2K1mIV8xrJzg-SdkxMjhSbIy6QqXB0cL8oAs=&xkcb=SoC96_M3sjYKsXygMx0AbzkdCdPP&camk=nUmJqO2E8rjnJ4m4lapFsg==&p=10&fvj=0&vjs=3",
    "description": "Cyber Data Analytics Engineer\nJob Category: Information Technology\nTime Type: Full time\nMinimum Clearance Required to Start: TS/SCI\nEmployee Type: Regular\nPercentage of Travel Required: Up to 10%\nType of Travel: Local\n* * *\nThe Opportunity:\nResponsible for building/maintaining data-pipelines for associated information used for cybersecurity investigation within the enterprise. In this role you will have the opportunity to contribute to one or more areas including (but not limited to) data ingest, data normalization, SIEM management, Linux/Windows host administration, virtual machine (VM) management, and cloud asset management. To support our team, you will need to be experienced, driven, and have strong Linux, Windows, and/or networking experience. You will be collaborating closely with peers and customers which means you need to be an active listener, detail oriented, and a clear communicator.\n\nResponsibilities:\nResolve escalated issues and perform root cause analysis for complex issues\nHave ability to communicate with program SMEs as well as other customers with less technical backgrounds\nDemonstrate a high attention to detail, examining every aspect of the system\nBe able to multi-task, working with several different customers in various stages of onboarding process\nApply Configuration Management disciplines to maintain hardware/software revisions, security patches, hardening, and documentation\nCoordinate and conducts event collection, log management, event management, compliance activities, and identity monitoring activities for the customer's system\nWorks with other Service Providers to support areas of common interest\nProvide all preventative and corrective maintenance to ensure consistent, reliable, and secure service availability\nMaintain system availability and reliability with a threshold of 99.99%\nDetect and ticket degradations (volume/velocity) of all SIEM data flows within 60 minutes of the start of the degradation\nPerform day-to-day maintenance, and specific scheduled maintenance activities that result from manufacturers recommended service intervals, alerts, bulletins, available patches, and updates according to agency approved change management processes\nExecute emergency maintenance actions with sufficient urgency to preclude unacceptable outage durations, approved by the Government prior to execution, and coordinated through and approved by CSOC and ESC government management\nPerform all development, engineering, testing, integration, and implementation actions necessary for major vendor revisions\nRetain documentation regarding loss of event logs (e.g. June 5-7th DNS logs were not ingested from SBU and are lost)\nConfigure all assets assigned to this service within the Government Furnished Information - Software Tools list in accordance with all Federal, DoD, IC, and NGA laws, directives, orders, polices, guidance, procedures etc.\nUtilize agency approved ticketing systems to document, track, assign, update, and coordinate all engineering, integration, configuration, and maintenance actions\n\nQualifications:\nRequired:\n5+ years of Systems Engineer or similar experience\nIAT II certification and obtain CSSP Infrastructure Support certification 120 days of hire\nPossesses a strong work ethic, be self-directed, and be a detail-oriented professional\nWilling to learn and adapt to new, cutting-edge technologies\nPossess excellent time management skills and the drive to work unsupervised\nDemonstrated ability to use problem solving techniques such as root cause analysis to resolve issues\nAdvanced Linux proficiency\nKnowledge of network communication principles, common infrastructure components (IPAM, DNS, DHCP), load balancers, firewalls, virtual and physical infrastructure design\nExperience with hypervisors such as VMware ESXi, Citrix XenServer, Microsoft Hyper-V\nAdvanced knowledge of systems engineering principles, methods, and techniques\n\nDesired:\nExperience with public clouds such as AWS, Google, Rackspace.\nExperience with private clouds such as VMWare, OpenStack.\nStrong background in Unix, or Windows servers.\nExperience with SIEM technologies such as Elastic, Splunk, and/or ArcSight\nFamiliarity with Cribl data aggregation/normalization technology\nScripting experience with Python, Bash, and/or Powershell\n\n________________________________________________________________________________________\nWhat You Can Expect:\n\nA culture of integrity.\nAt CACI, we place character and innovation at the center of everything we do. As a valued team member, you’ll be part of a high-performing group dedicated to our customer’s missions and driven by a higher purpose – to ensure the safety of our nation.\n\nAn environment of trust.\nCACI values the unique contributions that every employee brings to our company and our customers - every day. You’ll have the autonomy to take the time you need through a unique flexible time off benefit and have access to robust learning resources to make your ambitions a reality.\nA focus on continuous growth.\nTogether, we will advance our nation's most critical missions, build on our lengthy track record of business success, and find opportunities to break new ground — in your career and in our legacy.\n\nYour potential is limitless. So is ours.\nLearn more about CACI here.\n________________________________________________________________________________________\nPay Range: There are a host of factors that can influence final salary including, but not limited to, geographic location, Federal Government contract labor categories and contract wage rates, relevant prior work experience, specific skills and competencies, education, and certifications. Our employees value the flexibility at CACI that allows them to balance quality work and their personal lives. We offer competitive compensation, benefits and learning and development opportunities. Our broad and competitive mix of benefits options is designed to support and protect employees and their families. At CACI, you will receive comprehensive benefits such as; healthcare, wellness, financial, retirement, family support, continuing education, and time off benefits. Learn more here.\nThe proposed salary range for this position is:\n$75,200-$158,100\nCACI is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, age, national origin, disability, status as a protected veteran, or any other protected characteristic."
  },
  {
    "title": "Databricks Engineer",
    "company": "Fusion Global Solutions LLC",
    "location": "Remote",
    "salary": "$119,848.36 - $144,333.52 a year",
    "url": "https://www.indeed.com/rc/clk?jk=f19268f402e833bb&bb=-pk6i8MrsVJui5CucMaHlOQy-5E14ULzP_6tbFnfv2FeKTVz21bBZpUL9UdWDWl3E7jkl8tBx2gVMSVjhFliwSixdw8S6_YbThFOmB6AXQuMLj1VJ8vLk9MUEFTDcoBze6ALwFTm4tNX0Rg8UI86Kg%3D%3D&xkcb=SoAH67M3sjYKsXygMx0KbzkdCdPP&fccid=ff6a723de373b509&cmp=Fusion-Global-Solutions-LLC&ti=Data+Engineer&vjs=3",
    "description": "Databricks Engineer\nLOCATION\nRemote (EST hours)\nDURATION\n3+ Months\nPAY\nREQUIRED SKILLS\nLooking for a specialized Databricks candidate to help close out the year on some projects (related to building the data lakehouse).\nSomeone who can reorganize datalakes, rewrite pipelines, etc.\n5-8 years of Databricks experience (not looking for architect level).\nUnity Catalog\nDLT (Delta Lives Tables)\n5+ years of SQL\nJob Type: Contract\nPay: $119,848.36 - $144,333.52 per year\nWork Location: Remote"
  },
  {
    "title": "Senior Data Engineer (Databricks)",
    "company": "CGI Group, Inc.",
    "location": "Salt Lake City, UT",
    "salary": "$78,400 - $137,100 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CmPt6JXytAhZscz-5ZOP53MMQ49Xi4hmwETo1lvmuAlRL2OAxsFRorduI7SZmWktWu4VPPr1h7Ng3XhUxVclsohjH5ujt3EllaGffD2fzSrsQ1ed8Jsuu_I1SB8Yf10xckH5RqeNcXaMVq5AwzDLTtqNwLELU_UrHq4tOiyK-JsSVYhoSqPDLlv21rUFUXxI_Asqz4CmBAFi1QrgdY7dpGWXJohBVQDpc2G6WVq24_LdM0y6BEsb_O2Kdlmkq1quR76ogtdC_cmTFUO7gGKb6aU12qgYWUG7s0WLT6wkH76A2rQfW0Rz7--Dn3q8ppvbUAOtIQWKcUck8a0FJIPsYt5kWS4Mq9-jzu3R_FtaUVAA2AbybFhiHouW4IQ9Q2t3dT3-ziH_znniTClZLERf9VsGsZ_oyLgVhmX4Dv7hoX_-uK2CGp_IMoqnVapXqDvrC-IkdCuJdpV3NcVH2zPToYUG7mthpS_17QcZ3h5zw4GV45JXkshY8Y_DsGs-DEqypmz67k1kKRhSdPaymfqEmLuWyjwAfUeF67nntFBPlnWNe_P7C8cLq5dMeI0mXT-77_SwgHZKkSkxJaFHiMW39H_26fzzTqpCU7id5ZkheJ5TA3umQKOcz--clILoW0ZUG9KsbHd7UVGT2alKMIq5cFVLWRBTPbAcBigWiSQut41g==&xkcb=SoAJ6_M3sjYKsXygMx0BbzkdCdPP&camk=4HOcmqOLYrD3MDZWBKj-EA==&p=9&fvj=0&vjs=3",
    "description": "U.S. - Technology as a force for good\nBy playing this video you consent to Google/YouTube processing your data and using cookies Learn more.\nPosition Description:\nDo you want to take your career to the next level? Are you ready for the responsibility of working with high-profile clients? CGI is looking for a talented, driven, and experienced Senior Data Engineer (Databricks) with a passion for solving business problems to join our team in Salt Lake City, UT.\n\nThis position is located at our client site in Salt Lake City, UT.\n\nAt CGI, you will solve challenging business and technical problems as a full-time consultant serving local, enterprise clients. Youll be part of a team of smart, dedicated people like yourself and make an impact with both internal and client stakeholders. You also can work on cutting edge technologies and cloud native development. Tired of the same old thing? Take your talents to a world class consulting firm that inspires personal and professional growth and values your ideas.\nYour future duties and responsibilities:\nHow you'll make an impact\n\nPlay key role in establishing and implementing migration patterns for the Data Lake Modernization project\n\nActively migrate use cases from our on-premises Data Lake to Databricks on GCP\n\nCollaborate with Product Management and business partners to understand case requirements and reporting\n\nAdhere to internal development best practices/lifecycle (e.g. Testing, Code Reviews, CI/CD, Documentation)\n\nDocument and showcase feature designs/workflows\n\nParticipate in team meetings and discussions around product development\n\nStay up to date on industry, the latest trends and design patterns\nRequired qualifications to be successful in this role:\nWhat you'll bring\n\n5+ years of development experience with Spark (PySpark), Python and SQL\n\nExtensive knowledge building data pipelines\n\nHands on experience with Databricks Development\n\nStrong experience developing on Linux OS\n\nExperience with scheduling and orchestration (e.g. Databricks Workflows,airflow, prefect, control-m)\n\nSolid understanding of distributed systems, data structures, design principles\n\nComfortable communicating with teams via showcases/demos\n\nAgile Development Methodologies (e.g. SAFe, Kanban, Scrum)\n\nExperience with Databricks Unity Catalogue\n\nExperience in developing metadata driven framework on Databricks\n\nBachelors in computer science, Computer Engineering or related field\n\nDesired qualifications/non-essential skills:\n3+ years experience with GIT\n\n3+ years experience with CI/CD (e.g. Azure Pipelines)\n\nExperience with streaming technologies, such as Kafka and Spark\n\nExperience building applications on Docker and Kubernetes\n\nCloud experience (e.g. Azure, Google)\n\nOther Information:\nCGI is required by law in some jurisdictions to include a reasonable estimate of the compensation range for this role. The determination of this range includes various factors not limited to skill set, level, experience, relevant training, and licensure and certifications. To support the ability to reward for merit-based performance, CGI typically does not hire individuals at or near the top of the range for their role. Compensation decisions are dependent on the facts and circumstances of each case. A reasonable estimate of the current range for this role in the U.S. is $78,400.00 - $137,100.00.\n\nCGIs benefits are offered to eligible professionals on their first day of employment to include:\nCompetitive compensation\nComprehensive insurance options\nMatching contributions through the 401(k) plan and the share purchase plan\nPaid time off for vacation, holidays, and sick time\nPaid parental leave\nLearning opportunities and tuition assistance\nWellness and Well-being programs\n\n#LI-RS2\nSkills:\nAzure DevOps\nKubernetes\nPython\nSQL\nWhat you can expect from us:\nTogether, as owners, lets turn meaningful insights into action.\n\nLife at CGI is rooted in ownership, teamwork, respect and belonging. Here, youll reach your full potential because\n\nYou are invited to be an owner from day 1 as we work together to bring our Dream to life. Thats why we call ourselves CGI Partners rather than employees. We benefit from our collective success and actively shape our companys strategy and direction.\n\nYour work creates value. Youll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise.\n\nYoull shape your career by joining a company built to grow and last. Youll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons.\n\nCome join our teamone of the largest IT and business consulting services firms in the world.\n\nQualified applicants will receive consideration for employment without regard to their race, ethnicity, ancestry, color, sex, religion, creed, age, national origin, citizenship status, disability, pregnancy, medical condition, military and veteran status, marital status, sexual orientation or perceived sexual orientation, gender, gender identity, and gender expression, familial status or responsibilities, reproductive health decisions, political affiliation, genetic information, height, weight, or any other legally protected status or characteristics to the extent required by applicable federal, state, and/or local laws where we do business.\n\nCGI provides reasonable accommodations to qualified individuals with disabilities. If you need an accommodation to apply for a job in the U.S., please email the CGI U.S. Employment Compliance mailbox at US_Employment_Compliance@cgi.com. You will need to reference the Position ID of the position in which you are interested. Your message will be routed to the appropriate recruiter who will assist you. Please note, this email address is only to be used for those individuals who need an accommodation to apply for a job. Emails for any other reason or those that do not include a Position ID will not be returned.\n\nWe make it easy to translate military experience and skills! Click here to be directed to our site that is dedicated to veterans and transitioning service members.\n\nAll CGI offers of employment in the U.S. are contingent upon the ability to successfully complete a background investigation. Background investigation components can vary dependent upon specific assignment and/or level of US government security clearance held. Dependent upon role and/or federal government security clearance requirements, and in accordance with applicable laws, some background investigations may include a credit check. CGI will consider for employment qualified applicants with arrests and conviction records in accordance with all local regulations and ordinances.\n\nCGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGIs legal duty to furnish information."
  },
  {
    "title": "GCP Data Engineer",
    "company": "Seven Hills Group Technologies inc.",
    "location": "Tampa, FL 33602 \n(Channel District area)",
    "salary": "Contract",
    "url": "https://www.indeed.com/rc/clk?jk=582f59a1014b8810&bb=-pk6i8MrsVJui5CucMaHlGD4GiAliisJBuf1Q2X6ZLPpJlfs-Gb-GzHWRsjpNTzg6DoXb1NzTllXzCByur-SWqPZH988BCDtf2QfmKKJ1Trtn1xcxna2ZPIIHsxMy11zqxbxancnGJOCIIZxywAsnw%3D%3D&xkcb=SoAU67M3sjYKsXygMx0ObzkdCdPP&fccid=7d60bc39362dcaa7&cmp=Seven-Hills-Group-Technologies-Inc&ti=Data+Engineer&vjs=3",
    "description": "Responsibilities\nDesign, develop, and maintain data pipelines on GCP.\nImplement data storage solutions and optimize data processing workflows.\nEnsure data quality and integrity throughout the data lifecycle.\nCollaborate with data scientists and analysts to understand data requirements.\nMonitor and maintain the health of the data infrastructure.\nTroubleshoot and resolve data-related issues.\nStay updated with the latest GCP features and best practices.\nQualifications\nBachelor's degree in Computer Science, Engineering, or related field.\nProven experience as a Data Engineer with expertise in GCP.\nStrong understanding of data warehousing concepts and ETL processes.\nExperience with BigQuery, Dataflow, and other GCP data services.\nExcellent problem-solving and analytical skills.\nStrong communication and collaboration abilities.\nSkills\nGoogle Cloud Platform (GCP)\nBigQuery\nDataflow\nSQL\nPython\nETL processes\nData warehousing\nData modeling\nApache Beam\nCloud Storage\nThanks and Regards\nNick Awasth\nJob Type: Contract\nExperience:\nGCP: 5 years (Preferred)\nBigQuery: 4 years (Preferred)\nWork Location: In person"
  },
  {
    "title": "Staff Data Engineer",
    "company": "Tubi",
    "location": "Hybrid work in San Francisco, CA",
    "salary": "$212,000 - $302,800 a year",
    "url": "https://www.indeed.com/rc/clk?jk=005ae356c73b152f&bb=-pk6i8MrsVJui5CucMaHlBJVIEUQy5gClDBXvCmFoMHR8gDnLv1EK7GUKqz1hGYmFtDtD0lahcJujQT7Hc12jypH0V5b430UAolgaT0NOLh_9aPAe7Nzc9cMSA9WZ6G_DPbFZ8HeFzpDNY8BA17HCw%3D%3D&xkcb=SoCa67M3sjYKsXygMx0JbzkdCdPP&fccid=037150182eaec9aa&vjs=3",
    "description": "About Tubi:\nBoldly built for every fandom, Tubi is a free streaming service that entertains over 100 million monthly active users. Tubi offers the world's largest collection of Hollywood movies and TV shows, thousands of creator-led stories and hundreds of Tubi Originals made for the most passionate fans. Headquartered in San Francisco and founded in 2014, Tubi is part of Tubi Media Group, a division of Fox Corporation.\nAbout the Role:\nAs a Staff Data Engineer at Tubi, you'll be instrumental in developing and maintaining robust data processing pipelines that underpin our reporting, analytics, and performance insights. You'll operate as an embedded data engineer within a specific data product vertical, shaping the culture, best practices, and overall approach to data engineering within the team. This position will focus on building pipelines to assist the dynamic Growth Marketing team in building data pipelines to track Tubi's marketing spend and attribution. Knowledge of attribution frameworks such as Adjust and Kochava will be nice to have.\nIn this role, you will also be responsible for supporting your vertical with data-related automation, mentoring junior engineers if need be, and contributing to the strategic direction of data engineering across the company. Each product team presents unique big data challenges, requiring you to be adaptable, curious, and possess a strong foundation in big data and engineering principles. Your responsibilities will span from constructing efficient pipelines and facilitating data access for data scientists and analysts, to building specialized data products that empower non-engineering teams with deeper insights into their datasets.\nThis role is a hybrid role based out of the San Fransisco office. You must be willing to travel to our San Fransisco office 2 days/week.\nWhat You'll Do:\nBe the primary owner of the data needs of the Growth Marketing team. That means everything from raw-data ingestion, end-user analysis, to light management duties\nAbility to manage other DE resources to achieve your team's goals\nAbility to plan and execute on general goals with limited management input\nBuild intuitive, easy-to-use, and high quality datasets using Spark and DBT\nTrack down data quality issues when they arise, and then set appropriate data quality monitors and alerts to help prevent future incidents\nParticipate in the occasional on-call rotation (12-hr day time shifts, ~1-2/month)\nUnderstand your product vertical's datasets, with an ability to document and educate your team on how certain tables / fields are meant to be used\nBe the liaison between your product vertical and the core data infrastructure team to make sure business needs are met in a computationally efficient manner\nYour Background:\nBSc/MSc in Computer Science or related field\n10+ years of industry experience, at least 7 of which are in a data engineering (or data-centric software engineering) role\nExperience in management of a small team\nTrack record of building and operating scalable, flexible, and always-on data pipelines\nA desire and ability to truly understand and serve the business problems and use-cases you will be working with\nFluent in data manipulation and SQL\nStrong knowledge of Spark, Python (libraries like pandas and polars, helpful)\nStrong experience with Databricks (SQL Warehouses, Jobs Clusters, and Serverless)\nStrong familiarity with DBT and Airflow\nExperience with cloud providers and cloud storage (AWS preferred)\nExperience with efficiently working with datasets at TB scale\nExperience with attribution pipelines such as Adjust and Kochava a plus\nService and data quality oriented\nA passion for shipping production quality code with good test coverage\nYour Benefits:\nWorking with a talented, tight-knit team of passionate people with the mandate to make premium content accessible to everyone\nUse the latest data engineering technologies and techniques to solve engineering problems at petabyte-scale\nAutonomy and end-to-end ownership of what you create\nYour choice of hardware to work with\nOpportunity for internal growth\nWork with other fellow AVOD enthusiasts\n#LI-Hybrid\n#LI-CL1\nTubi is a division of Fox Corporation, and the FOX Employee Benefits summarized here, covers the majority of all US employee benefits. The following distinctions below outline the differences between the Tubi and FOX benefits:\nFor US-based non-exempt Tubi employees, the FOX Employee Benefits summary accurately captures the Vacation and Sick Time.\nFor all salaried/exempt employees, in lieu of the FOX Vacation policy, Tubi offers a Flexible Time off Policy to manage all personal matters.\nFor all full-time, regular employees, in lieu of FOX Paid Parental Leave, Tubi offers a generous Parental Leave Program, which allows parents twelve (12) weeks of paid bonding leave within the first year of birth, adoption, surrogacy, or foster placement of a child in addition to applicable government leave program(s) and FOX's short-term disability policy. This time is 100% paid through a combination of any applicable state, city, and federal leaves and wage-replacement programs in addition to contributions made by Tubi.\nFor all full-time, regular employees, Tubi offers a monthly wellness reimbursement.\nWe are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, gender identity, disability, protected veteran status, or any other characteristic protected by law. We will consider for employment qualified applicants with criminal histories consistent with applicable law."
  },
  {
    "title": "Data Center Design Engineer (Job Code: TX0725DCE)",
    "company": "CHT Global, Inc.",
    "location": "Grapevine, TX 76051",
    "salary": "Full-time",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0Cal4f9zgutyUcAoBnZchqenviuiEzunv1lx9CiUs9LzK1ccrqbIpMiDrmElO5EMLXx6FpMbWcq5WI-lS6sImkTMNkVyYXoW5u451IPjSd1hSh3pQTYYPoYPLTa1w_nUqyXwG0zU7xkXUrt01rmOHMRDfP1WpuUzf_0GJvyaY907QHpUfxRehfdnsQjQq1oPqF-p57S4gjgCxp_i3TCAISp-HvBDzt2ct7Oole_9WwFS4scW0yANcI3o_Vb7X2-ZlCLUqLEKiSAWcYQr_T5aSMshVjAHPYDWMOMpm3ZxyWqZSVfUWynrNEbjIjSOlXVgH8Ld9lPVZxFX-S6pCJf1C_3EPnbtTMUeZ24dRic_4jGU7z8XJk3YH5trNi9Kmb9igSR1ahqVKe-k2CoVcOZQIcZIK3vBo_Rshucd9rUhrTAMqYY6l_p8BaTbXcSL0jNQZOIT3cMXutVOSA1nowLljjHcpO7M-FP_fzfVoDvx2XF4AdGNMuQFBn7BymqzMKXMGRl45qQ4T7iVATLzx6mBPLgXl8H_GJLoBo1y0TuTOFkxye4G-RIbHmhTEexvbuZnyvgMy6DpObM7FwdQtMmnGA-IIwUKNoa_ZcVv9oPYmJUa6YjG8Jzn0--z8oqXs2q3f0dLIkEgnp40cpio6BZQZR4G3naDEr7_9RseQaHUls4w-MfB7r_Bk5nrhBoUogT6vU=&xkcb=SoCu6_M3sjYKsXygMx0EbzkdCdPP&camk=ethIe0s0heccWvTgKz8HyA==&p=14&fvj=0&vjs=3",
    "description": "We are seeking a talented and motivated AI Data Center Design Engineer to join our team in designing, building, and maintaining state-of-the-art data centers supporting AI workloads. This role is critical to ensuring the stability, reliability, and security of our IT infrastructure, applications, and data. The responsibilities include:\nDesigning and implementing the backbone infrastructure of AI data centers, including power, cooling, and mechanical systems, to meet the high-performance computing requirements of AI workloads.\nDeveloping and deploying monitoring and management systems for AI cluster provisioning, hardware health, and environmental control.\nOverseeing network, electrical, and mechanical system surveillance, ensuring stable and reliable operation of AI and cloud environments in the US and globally.\nCollaborating closely with data center electrical and mechanical engineering teams to ensure system stability, safety, and adherence to design requirements.\nProviding technical support to internal and external customers, including solutions design, architecture proposals, and technical guidance.\nEnsuring successful project execution, including design validation, installation, configuration, documentation, and system integration.\nServing as an on-site field engineer during quality control processes at customer sites, ensuring that all project phases meet required standards through final commissioning.\nPerforming other duties as assigned by the Company.\nQualifications\nBachelor’s degree in Electrical Engineering, Computer Science, Mechanical Engineering, or a related field.\nExperience in data center design, including power distribution, cooling systems, and mechanical/electrical system integration.\nStrong understanding of electrical and mechanical engineering principles, especially in high-density computing environments.\nFamiliarity with AI hardware (GPUs, TPUs, high-density servers) and their integration into large-scale data center environments.\nBilingual in English and Mandarin Chinese is preferred.\nUS Citizen, US Permanent Resident, or individuals with OPT (Optional Practical Training) status.\nJob Type: Full-time\nBenefits:\n401(k)\n401(k) matching\nDental insurance\nHealth insurance\nLife insurance\nPaid time off\nVision insurance\nSchedule:\n8 hour shift\nWork Location: In person"
  },
  {
    "title": "Senior Specialist - Data Engineering",
    "company": "LTIMindtree",
    "location": "Hybrid work in Philadelphia, PA",
    "salary": "$110,000 - $120,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0DGQGqBDMx6A8AmUwMHqwH1Bji_O9JKiQMSo1T_WGQXVPGsOjwWPlvWTJlM47EieU5MTopU2W99RY3ypL85gKjGj62NugtyxzW0h7cX_hYZXy1azfWz09wzalWN-kFtw9eq3xf_GDv1Kj6BuEFwRTvv1CgV8P2tU2T1OtgcUCcVlOhnlCWGh-QzQP_YRQS746oNlJwHka_mkA-TWoW1PH5hlQ6tQaazEMub4cdyoVosT7nNPcadgrQ9byzT9G7i2jovkxWCzEoJ4mpvZRjF2e-MEhGNeG5-WbVY0IFLGX9tvGuk9P6nA4ZzKBAtbeyc-DG9xtkTjTZGATw76jQO14Re3RPCkXx7eYFJ9kmZYCRRvCjibtrzHbAG8HSUbTw6vgws_y5CSIC5KJK21K52FZT2RT87saSmxR1Ptn_BHuvuRR66Myv6bAqCU6kUM6j-b402o8GGXNtNYVGofF0iO-EyX7SipijZ823EZVqYp-aDROU6UK79-alqavZQzADNu9tTgGe4pbjrrO9boE2PtCRs-Q9sFoFagG101FN3GXNKyLIxbmfTCNnJQdmuLVc_zPAuSsO9GfvCk5mG6UMuUb4W2i-b8xMCerpaf5ZbLR5oZJe_MQaC_qHPXYadsNCltrn48F9fCW9lwk3GzvCoHWXJzle4bkC4N6JyL73IayqTsf0Vj6GHF_-slx6GPUc_XsIVMBXsf6uxsFUG505WxV4bwmrlgcn43F3CBTXuTC04AMx9n5VIEQEKdCC6mGH2QDI=&xkcb=SoCU6_M3sjYKsXygMx0CbzkdCdPP&camk=ethIe0s0hefEvqHCLxk3yw==&p=8&fvj=0&vjs=3",
    "description": "Role description\n\nCloud Data Engineering\n3 years of handson experience with GCP services BigQuery PubSub DataflowApache Beam AirflowCloud Composer Cloud Functions and Cloud Storage\nStrong understanding of data lakes data warehouses and analytics platforms at scale\nProgramming Development\nProficiency in Java Python and SQL for data processing and transformation\nExperience with DevOps pipelines CICD including automated testing and deployment\nInfrastructure Architecture\nDesign and deploy scalable faulttolerant systems on GCP\nManage cloud infrastructure using Compute Engine App Engine Kubernetes Engine and Cloud Functions\nAutomate infrastructure with Terraform or Google Cloud Deployment Manager\nMonitoring Security\nUse GCP Operations Suite Stackdriver for performance monitoring\nImplement IAM roles service accounts and access policies to ensure data security\nPreferred Qualifications\nGCP certification eg Professional Data Engineer\nExperience with agile methodologies and containerization Docker Kubernetes\nFamiliarity with CICD tools like Jenkins GitHub Actions and Cloud Build\nBachelors degree in Computer Science IT or related field\nLocation Work Model\nLocation Bangalore Hybrid minimum 3 days onsite\nEmployment Fulltime\nShift Willingness to work latenight shifts if required\n\nSkills\nMandatory Skills : GCP Storage,GCP BigQuery,GCP DataProc,GCP Cloud Composer,GCP DMS,Apache airflow,Java,Python,Scala,GCP Datastream,Google Analytics Hub,GCP Workflows,GCP Dataform,GCP Datafusion,GCP Pub/Sub,ANSI-SQL,GCP Dataflow,GCP Data Flow,GCP Cloud Pub/Sub,Big Data Hadoop Ecosystem\n\nOther details\n\nBenefits/perks listed below may vary depending on the nature of your employment with LTIMindtree (“LTIM”):\nBenefits and Perks:\nComprehensive Medical Plan Covering Medical, Dental, Vision\nShort Term and Long-Term Disability Coverage\n401(k) Plan with Company match\nLife Insurance\nVacation Time, Sick Leave, Paid Holidays\nPaid Paternity and Maternity Leave\nThe range displayed on each job posting reflects the minimum and maximum salary target for the position across all US locations. Within the range, individual pay is determined by work location and job level and additional factors including job-related skills, experience, and relevant education or training. Depending on the position offered, other forms of compensation may be provided as part of overall compensation like an annual performance-based bonus, sales incentive pay and other forms of bonus or variable compensation.\nDisclaimer: The compensation and benefits information provided herein is accurate as of the date of this posting.\nLTIMindtree is an equal opportunity employer that is committed to diversity in the workplace. Our employment decisions are made without regard to race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), gender identity or expression, national origin, ancestry, age, family-care status, veteran status, marital status, civil union status, domestic partnership status, military service, handicap or disability or history of handicap or disability, genetic information, atypical hereditary cellular or blood trait, union affiliation, affectional or sexual orientation or preference, or any other characteristic protected by applicable federal, state, or local law, except where such considerations are bona fide occupational qualifications permitted by law.\n\nBenefits\nCompensation range: $110,000.00 to $120,000.00 per year\nAbout LTIMindtree\nLTIMindtree is a global technology consulting and digital solutions company that enables enterprises across industries to reimagine business models, accelerate innovation, and maximize growth by harnessing digital technologies. As a digital transformation partner to more than 700 clients, LTIMindtree brings extensive domain and technology expertise to help drive superior competitive differentiation, customer experiences, and business outcomes in a converging world. Powered by 83,000+ talented and entrepreneurial professionals across more than 40 countries, LTIMindtree — a Larsen & Toubro Group company — solves the most complex business challenges and delivers transformation at scale. For more information, please visit https://www.ltimindtree.com/."
  },
  {
    "title": "Senior Data Engineer",
    "company": "Ten Mile Square Technologies",
    "location": "Remote in Arlington, VA 22203",
    "salary": "$140,000 a year",
    "url": "https://www.indeed.com/rc/clk?jk=7678c9200586d72e&bb=-pk6i8MrsVJui5CucMaHlA_4N6GfgHvhP53Ae8JrcLrJ3iXwYMRPfD_8mIpEJ5lbyv2MOudJ1EHPoN4T-C9vrB5R1esv1JnNaa493aMHWko5wyx5GIx0FP5dMf3PjFGjMp15AGmV7pijGx85cEwBcg%3D%3D&xkcb=SoCJ67M3sjYKsXygMx0NbzkdCdPP&fccid=9e2d156ef0f235c9&vjs=3",
    "description": "Company Description\n\nTen Mile Square is a high-end technology consulting firm where we are routinely called upon to solve some of the most challenging problems in computer science and software development. We are a cadre of experienced technology professionals who thrive on solving complex problems for our clients, from brand names to venture-funded companies, in the healthtech, media, fintech, and enterprise software segments. We believe that a company of exceptional people demands an exceptional culture.\nIf you have a solid grounding in data and software engineering, continuous delivery, and computer science, and are looking for a career opportunity to grow and leverage those skills to the fullest while working on cutting edge technologies – Ten Mile Square is the place to be!\n\nJob Description\n\nCompensation (Base + Bonus at plan): $161K ($140K + $21K)\nJob Responsibilities:\nPerform quality-side setup, teardown, and transform data for test setups and demos.\nDig into both DB2 and Postgres data, with a primary focus on Postgres.\nWork with a variety of data platforms including Snowflake and S3.\nMigrate and transform data to support various project needs, including database refreshes, copies, S3 reports, and PDFs.\nDevelop tooling using Typescript to enable development teams to manage their data needs more efficiently.\nUse your DevOps experience to support your role and ensure smooth data operations.\nLeverage AI to force-multiply your work, finding innovative ways to automate and optimize data processes.\n\nQualifications\nSerious expertise in data migration and transformation.\nDeep understanding of data and it's relationship to the SDLC.\nStrong working knowledge of databases, specifically DB2 and Postgres.\nExperience with modern data platforms such as Snowflake and S3.\nTypescript background and experience building internal tools.\nDevOps experience.\nDemonstrated ability to use AI to improve efficiency and workflow.\nMore importantly,\nBe passionate about what you do\nDemonstrated ability to learn new technologies and concepts quickly\nDetail-oriented\nHigh integrity and quality of work ethic\nSelf-motivated to proactively identify and solve problems\nExcellent communication skills – within team and with clients\n\nAdditional Information\n\nEXCEPTIONAL PEOPLE DESERVE EXCEPTIONAL BENEFITS\nTen Mile Square Technologies benefits package currently includes:\n100% company paid medical, dental, and vision insurance (including family)\n100% company paid individual short- and long- term disability\n100% company paid vision insurance\n3% company contribution to 401(k)\n15 days paid time off per year + 1 day/year up to 20 days\n10 training days per year\n8 company holidays + 2 personal days\nIndividual performance-based quarterly cash incentive\nCombined individual and company based annual cash incentive\nPublic transit reimbursement\nAdditional Details:\nJob Type: Full-time, not C2C\nJob Location: Remote, US based only"
  },
  {
    "title": "Insider Threat Program Database Engineer",
    "company": "Leidos",
    "location": "Washington, DC 20090 \n(South West area)",
    "salary": "$126,100 - $227,950 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0CZUO70VSdYKA8PR3jfrSh5ljhqJhfDt0PzQCMubt8cRosbSX75HEeA45s9n5xrWx0nRVA3TIrsdvxG6LQgLxaqXq14FUiajWS7mEyhiJDdohtRb_McTbrEtPib-iR5yhDCp8onx__0hZ4dphUq-1eg7hGbzcCkLYgHv1RXSNKG44f9ya69KHAYrqvkBfkIIHJXR0qsPyoDqw4izRRfC1iNJhIQKiDQ0C6QtXFDk81KIBuSnWr0WWxgYO5cXCEB_3xsk3jlIpOQw6RhirnLw3u3UmDh6piZop2Q9DC8SK7MZyirQ0sf-dUJ7DPXuTJY5ao4zfU4YBhya4NZ4Ep_Cqh1j2I9xdngzHk4jmPAio4BiRebqQqbnJlSPukDe_BQUMRWo-TNmcbUcp1RPvH4tiDN-lzxSIFM0mGL-awUaXo4L1qCRUZ6ElhVQZbXTAwdprx3kcVWbUuAdsUhdgdaVqn9aKNDXx0MK0KAIAE4z6vDByaBTIWF3Tc7RiHD6XWI6ozGNMIg7f0Mq9Q7HpyZcSGWxdqaIMZX9-d_5RTzyZBF35V6ZkYLNJdNziIiZWeAyfyeY34BO9A3fFOvVA7mXFqEQJHgnraWl7242FxKkgxXoYfCIaZeaRhU2RXhEkGkmRyApmROdDvPxSXuS9Yyzs6BR99ijdTI46uy0WLETcRqey4JO1soCjntqOLPtCQIt5xlkU4O9AoD0QleDFVbfCSIf6upishlk2gQzg80vtBgK01Ybwmn8bnrjPl9beqR1Ek7eZeZ6EOxdg==&xkcb=SoDJ6_M3sjYKsXygMx0MbzkdCdPP&camk=ethIe0s0hecezcnkRTMseg==&p=6&fvj=0&vjs=3",
    "description": "Description\nLeidos is seeking a skilled Database Engineer to support the DHS Insider Threat Program (ITP) under the HEITS Contract. This is a dynamic opportunity to apply your expertise in designing, sustaining, and evolving the database infrastructure that powers one of DHS’s most critical missions—identifying and mitigating insider threats through advanced analytics, monitoring, and data correlation.\n\nResponsibilities\nAs the Database Engineer, you will:\nMaintain and support the ITP database backend infrastructure\nPerform routine maintenance: reindexing, backups, transaction log truncation, cluster management\nDesign and implement database encryption solutions (e.g., Transparent Data Encryption - TDE)\nDocument and maintain configuration baselines\nDevelop and execute system recapitalization plans to support capacity growth\nServe as the SME for all database infrastructure\nCreate and optimize stored procedures, queries, and views\nSupport data warehouse features and integrations\nOptimize database schemas for referential integrity, cascading transactions, indexing, and performance\n\nBasic Qualifications\nBachelor’s degree with 12+ years of relevant experience, or Master’s degree with 10+ years\nExpertise in Oracle database management, upgrades, and maintenance\nExperience with SQL, PostgreSQL, and other database types\nAbility to integrate diverse data types into cloud containers (e.g., AWS S3)\nStrong understanding of DBMS and database design principles\nExperience with database migrations and data lake/repository connectors\nEligibility to obtain DHS EOD SCI clearance\n\nPreferred Qualifications\nMaster’s degree in IT Management, Engineering, or related field\n10+ years of experience in IT service delivery management\nFamiliarity with User Activity Monitoring platforms\nExperience with Everfox High Speed Guard Platform\nProficiency with Oracle, Microsoft SQL, PostgreSQL, or other DBMS platforms\nCome break things (in a good way). Then build them smarter.\nWe're the tech company everyone calls when things get weird. We don’t wear capes (they’re a safety hazard), but we do solve high-stakes problems with code, caffeine, and a healthy disregard for “how it’s always been done.”\nOriginal Posting:\nOctober 2, 2025\n\nFor U.S. Positions: While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above.\n\nPay Range:\nPay Range $126,100.00 - $227,950.00\nThe Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law."
  },
  {
    "title": "Data Engineer - Onsite Position",
    "company": "Adhesives Research Inc.",
    "location": "Glen Rock, PA 17327",
    "salary": "Full-time",
    "url": "https://www.indeed.com/rc/clk?jk=5e8bd81220105e3d&bb=-pk6i8MrsVJui5CucMaHlOkII90j-bGrJ61k2G2dt9myZ75InBYTB1H3KzYbVt8Mqpx-yIWacZCjEOot6gWDVIcHrrltY0AWG1jzKwt9upYn0pfWI6ww1ACoy2-x0yjz8usUUKl5_pltZ43JlpmXJA%3D%3D&xkcb=SoAu67M3sjYKsXygMx0IbzkdCdPP&fccid=de37bf62c25001de&vjs=3",
    "description": "Job Summary:\nThe Data Engineer will design, develop, and maintain scalable data solutions that support enterprise analytics, reporting, and system integration. This role combines traditional database administration (DBA) responsibilities with modern data engineering, including ETL/ELT processes, data warehousing, and performance optimization. The ideal candidate will ensure data quality, security, availability, and contribute to the organization’s BI and data strategy by enabling robust and efficient data pipelines.\n\nPlease note - this position is 100% onsite. Business is located in Glen Rock, PA.\nEssential Functions:\nDesign, develop, and maintain ETL pipelines using tools such as Azure Data Factory, or equivalent.\nManage the lifecycle of on-prem and cloud-based databases (SQL Server, Oracle, or similar).\nMonitor, tune, and optimize database performance, indexing, and storage.\nDevelop and publish dashboards, datasets, and data models using Power BI.\nMaintain enterprise data warehouse structures and support reporting platforms (e.g., Power BI).\nDevelop and implement data governance, validation, and data quality frameworks.\nWork with application teams to define data requirements and integrations from ERP, MES, CRM, and other enterprise platforms.\nAutomate recurring tasks using scripting tools (PowerShell, Python, etc.).\nDefine and maintain data dictionaries, data lineage, and technical documentation.\nEnsure high availability, backups, and disaster recovery for all data systems.\nSecure sensitive data in compliance with company policies and frameworks like NIST, ISO 27001, and 21 CFR Part 11.\nCollaborate with business analysts and developers to support analytics use cases and dashboards.\nAdditional Responsibilities:\nProvide Tier 3 support and troubleshooting for data-related issues.\nDocument data architecture, transformations, and design patterns.\nEvaluate and recommend tools or platforms for data integration and visualization.\nParticipate in audits and assessments related to data privacy and compliance.\nCollaborate with cross-functional teams to ensure alignment between business systems and operational needs.\nSupport compliance with IT policies, procedures, and regulatory requirements.\nParticipate in IT projects and system implementations.\nPerforms other duties and responsibilities as assigned.\nJob Specifications:\nBachelor’s degree in Computer Science, Information Systems, Data Engineering, or a related field\n5+ years of experience as a Data Engineer or DBA in enterprise environments\nHands-on experience with SQL Server, SSIS, and/or Azure Data Factory.\nStrong experience in relational databases, data modeling, and performance tuning.\nSolid understanding of data warehousing concepts and dimensional modeling.\nExperience working with ERP (preferably Oracle EBS), MES, and CRM systems (e.g., SugarCRM) for data extraction and integration.\nProficiency with scripting languages such as PowerShell or Python.\nFamiliarity with cloud platforms (Azure or AWS) and hybrid architecture.\nExperience with compliance frameworks like 21 CFR Part 11, ISO 27001, and NIST.\nDesirable:\nCertifications in Azure Data Engineer, Oracle DBA, or project management (e.g., PMP).\nExperience with:\nCustom connectors and external API integration\nIntegration of CRM with ERP, email, or marketing platforms\nITSM tools and structured change management processes\nWorking in validated or regulated environments\nAbility to work cross-functionally with application, infrastructure, and support teams.\nPhysical Requirements\nMust be able to lift 50 lbs, bend, and climb stairs as needed.\nAbility to travel to company sites as required."
  },
  {
    "title": "Data Pipeline Reliability Engineer (Secret Clearance)",
    "company": "DCI Solutions",
    "location": "Washington, DC 20007 \n(Georgetown area)",
    "salary": "$160,000 - $200,000 a year",
    "url": "https://www.indeed.com/pagead/clk?mo=r&ad=-6NYlbfkN0AT3av5_RQjo6mG1Ya7TZfZ6XSK9fb5zJoLKCbAm2YwhNlxXgFDtnYmiY8AwUKSRxj10qtppTswOs4mrxp4slOu6DiLgs8XNaxJAtm-KZQ35tdIQz-s2-l-E_sByRY0B5Uqg9BS5PlTB1nAwbbOHpJY1_XCkyP6ilQOl0h09HknuG6tspBNYS4hgdjTqHG0YzjMBNlvQpshjcnk1rusgjXyNuJG4aymXvAeCm08SsKQ7kINcK0qs333yRwGCwzsLovfIke3I0MWmXFTIB5qVef_G_oBPq7AdieVzi51ahTQjgao8jpffcKX-s9T83ZLc6Rd6r1-Ukx659c_0QK0KVuMtKabYMbGZjKqzn-olUcNQo2vCzGhuRiC7Av0tfA5zOibwhfCqTovzoqfOqCxQ9Uwbc97w7BKtCMLQbPC9CPC3krP3qq6GhDxondnyRfWlW3wfp8DRMbN8GFQTbPEn-neE7Bh-JFlYHFefRwgrnctK7WlYvRrjvheRA839AUqBWsCo0JLWa1qLd5fFaQ96c6oDF2m_zNB_xcUMwqh3N-_zV5X1Kw8ABuW8RvjcvJCMqlCP0ulJNHBh46wkssAw-1KAiKbqtqrRBIgHJHtnxzt13wg2Er0MDG64dYuqf2IEj90Ckqb5IGoSkQY_aBTzyPBlTtISpZSnFk8JmTUHPeHReZbnfREmbiW3D_OanIbUn4=&xkcb=SoCH6_M3sjYKsXygMx0GbzkdCdPP&camk=4HOcmqOLYrBK20tkunRUpg==&p=12&fvj=0&vjs=3",
    "description": "DCI Job Requirement for:\nData Pipeline Reliability Engineer\nLocation: Washington, D.C.\nJob Description:\nServe as a Data Pipeline Reliability Engineer (DPRE) on a cross-functional team\nHelp to ensure our customers’ missions are supported with updated and accurate data\nBuild, optimize, and maintain data pipelines to improve their efficiency and resilience.\nServe as a first responder\nTriaging, troubleshooting, and coordinating the resolution of technical issues\nDiagnose, resolve, and prevent issues encountered in the field\nImplement/maintain automated monitoring to detect data quality issues\nEmbed with business teams to minimize risks associated with product deployments\nCollaborate with customer-facing teams to increase reliability of data pipelines\nImprove performance/stability of production data pipelines by:\nInstalling data health metrics and automated alerts\nDocument strategies for responding to incidents\nQualifications:\nStrong engineering background\nProficiency with programming languages such as Java, C++, Python, or JavaScript\nBasic parallel data processing experience\nBasics understanding of optimizing Spark jobs\nExperience performing root cause analysis and documentation of findings\nUnderstanding/experience with data concepts such as:\nData warehousing\nData Lakes\nData governance\nData Liniage\nUnderstanding of networking concepts (DNS, VPNs, Load Balancing)\nExperience with the following tools:\nObservability tools (Ex. Grafana)\nData Pipeline tools (Ex. Airflow)\nCloud tools (Ex: AWS, Azure, Google Cloud)\nIaC tools (Ex. Terraform)\nRequired: Active Secret Security Clearance\nJob Type: Full-time\n$160,000-$200,000\nJob Type: Full-time\nPay: $160,000.00 - $200,000.00 per year\nBenefits:\n401(k)\n401(k) matching\nDental insurance\nFlexible schedule\nFlexible spending account\nHealth insurance\nHealth savings account\nLife insurance\nPaid time off\nParental leave\nProfessional development assistance\nRelocation assistance\nRetirement plan\nTuition reimbursement\nVision insurance\nExperience:\ndata engineering: 5 years (Preferred)\npython coding/programming : 5 years (Preferred)\nFoundry: 2 years (Preferred)\nbuilding and managing data pipelines : 2 years (Preferred)\nSecurity clearance:\nSecret (Required)\nWork Location: In person"
  },
  {
    "title": "",
    "company": "",
    "location": "",
    "salary": "",
    "url": "https://www.indeed.com/viewjob?jk=a1b2c3d4e5f67890",
    "description": "None"
  }
]