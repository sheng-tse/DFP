{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3c8a91",
   "metadata": {},
   "source": [
    "# # Main Scraper\n",
    "# Execute the complete scraping workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d93130",
   "metadata": {},
   "source": [
    "## Load All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb496d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Test URL: https://www.indeed.com/jobs?q=data+analyst&l=New+York+NY\n",
      "✓ Utility functions loaded successfully\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Test URL: https://www.indeed.com/jobs?q=data+analyst&l=New+York+NY\n",
      "✓ Utility functions loaded successfully\n",
      "✓ Scraping functions loaded successfully\n",
      "  - get_job_basic_info()\n",
      "  - get_job_description()\n",
      "  - process_job_with_description()\n"
     ]
    }
   ],
   "source": [
    "%run config.ipynb\n",
    "%run utils.ipynb\n",
    "%run scraping_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863957c",
   "metadata": {},
   "source": [
    "## User Input Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69dd6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job search parameters\n",
    "job_title = input(\"Enter job title: \")\n",
    "city = input(\"Enter city: \")\n",
    "state = input(\"Enter state: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1528624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from page 1\n",
      "Will scrape 1 pages\n"
     ]
    }
   ],
   "source": [
    "# Pagination settings\n",
    "start_page_input = input(\"Enter starting page (0 for first page, 1 for second page, etc.): \")\n",
    "start_page = int(start_page_input) if start_page_input.strip() else 0\n",
    "print(f\"Starting from page {start_page + 1}\")\n",
    "\n",
    "pages_input = input(\"Enter number of pages to scrape (default 1): \")\n",
    "num_pages = int(pages_input) if pages_input.strip() else 1\n",
    "print(f\"Will scrape {num_pages} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d66d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 15 parallel threads\n"
     ]
    }
   ],
   "source": [
    "# Threading settings\n",
    "threads_input = input(f\"Enter number of parallel threads (default {DEFAULT_THREADS}, max {MAX_THREADS}): \")\n",
    "max_workers = int(threads_input) if threads_input.strip() else DEFAULT_THREADS\n",
    "max_workers = min(max_workers, MAX_THREADS)\n",
    "print(f\"Using {max_workers} parallel threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1e9d6",
   "metadata": {},
   "source": [
    "## Main Scraping Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d1829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search URL: https://www.indeed.com/jobs?q=Machine+Learning+Engineer&l=San+Jose+CA\n"
     ]
    }
   ],
   "source": [
    "# Generate initial URL\n",
    "url = get_url(job_title, city, state, start_page)\n",
    "print(f\"\\nSearch URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b04e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup WebDriver\n",
    "driver = create_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149adb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRAPING PAGE 1\n",
      "================================================================================\n",
      "Found 16 jobs on page 1\n",
      "\n",
      "Phase 1: Collecting basic job information...\n",
      "  Collecting job 16/16...\n",
      "✓ Collected 16 job listings\n",
      "✓ Closed listing page browser\n",
      "\n",
      "Phase 2: Fetching job descriptions (15 parallel threads)...\n",
      "[Thread] Processing job 1/16: Staff Machine Learning Engineer, AI Platform at General Motors\n",
      "[Thread] Processing job 2/16: Staff Machine Learning Engineer – AI Research at General Motors\n",
      "[Thread] Processing job 3/16: Lead Machine Learning Engineer - ESPN+ Personalization at Disney Entertainment and ESPN Product & Technology\n",
      "[Thread] Processing job 4/16: Google ADK AI Engineer at Infosys\n",
      "[Thread] Processing job 5/16: Data Analytics Engineer (7432) at Advantest\n",
      "[Thread] Processing job 6/16: Applied Scientist II, RBKS AI at Amazon.com Services LLC\n",
      "[Thread] Processing job 7/16:  at \n",
      "[Thread] Processing job 8/16: Staff ML Engineer - Offboard Embodied AI at General Motors\n",
      "[Thread] Processing job 9/16: Staff ML Engineer, ML Orchestration at General Motors\n",
      "[Thread] Processing job 10/16: Principal Machine Learning Engineer - AI Research at General Motors\n",
      "[Thread] Processing job 11/16: Staff ML Engineer, Inference Platform at General Motors\n",
      "[Thread] Processing job 12/16: Principal Machine Learning Engineer - Ad Platforms at Disney Entertainment and ESPN Product & Technology\n",
      "[Thread] Processing job 13/16: Staff ML Engineer, ML Orchestration at General Motors\n",
      "[Thread] Processing job 14/16: Full Stack AI Engineer at CuraFi\n",
      "[Thread] Processing job 15/16: Senior ML Compiler Engineer at General Motors\n",
      "[Thread] Completed job 12/16: Principal Machine Learning Engineer - Ad Platforms\n",
      "[Thread] Processing job 16/16: Delivery Consultant- GenAI/ML & Data Science, Professional Services, AWS Industries at Amazon Web Services, Inc.\n",
      "[Thread] Completed job 15/16: Senior ML Compiler Engineer\n",
      "[Thread] Completed job 10/16: Principal Machine Learning Engineer - AI Research\n",
      "[Thread] Completed job 5/16: Data Analytics Engineer (7432)\n",
      "[Thread] Completed job 4/16: Google ADK AI Engineer\n",
      "[Thread] Completed job 2/16: Staff Machine Learning Engineer – AI Research\n",
      "[Thread] Completed job 9/16: Staff ML Engineer, ML Orchestration\n",
      "[Thread] Completed job 3/16: Lead Machine Learning Engineer - ESPN+ Personalization\n",
      "[Thread] Completed job 11/16: Staff ML Engineer, Inference Platform\n",
      "[Thread] Completed job 14/16: Full Stack AI Engineer\n",
      "[Thread] Completed job 16/16: Delivery Consultant- GenAI/ML & Data Science, Professional Services, AWS Industries\n",
      "[Thread] Completed job 13/16: Staff ML Engineer, ML Orchestration\n",
      "[Thread] Completed job 1/16: Staff Machine Learning Engineer, AI Platform\n",
      "[Thread] Completed job 8/16: Staff ML Engineer - Offboard Embodied AI\n",
      "[Thread] Completed job 6/16: Applied Scientist II, RBKS AI\n",
      "[Thread] Completed job 7/16: \n",
      "\n",
      "✓ Completed 16 jobs in 20.81 seconds\n",
      "  Average: 1.30 seconds per job\n",
      "\n",
      "✓ Total jobs collected so far: 16\n"
     ]
    }
   ],
   "source": [
    "# Store all job records\n",
    "records = []\n",
    "next_page_url = None\n",
    "\n",
    "try:\n",
    "    for page_num in range(num_pages):\n",
    "        current_page = start_page + page_num\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SCRAPING PAGE {current_page + 1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Navigate to URL\n",
    "        if page_num == 0:\n",
    "            driver.get(url)\n",
    "        else:\n",
    "            if next_page_url:\n",
    "                driver.get(next_page_url)\n",
    "            else:\n",
    "                url = get_url(job_title, city, state, current_page)\n",
    "                driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        time.sleep(random.randint(PAGE_LOAD_MIN, PAGE_LOAD_MAX))\n",
    "        WebDriverWait(driver, WEBDRIVER_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"job_seen_beacon\"))\n",
    "        )\n",
    "        \n",
    "        # Find all job postings\n",
    "        posts = driver.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
    "        print(f\"Found {len(posts)} jobs on page {current_page + 1}\")\n",
    "        \n",
    "        # Phase 1: Collect basic info quickly\n",
    "        print(\"\\nPhase 1: Collecting basic job information...\")\n",
    "        job_basics = []\n",
    "        for i, post in enumerate(posts):\n",
    "            print(f\"  Collecting job {i + 1}/{len(posts)}...\", end=\"\\r\")\n",
    "            basic_info = get_job_basic_info(post)\n",
    "            if basic_info:\n",
    "                job_basics.append(basic_info)\n",
    "        \n",
    "        print(f\"\\n✓ Collected {len(job_basics)} job listings\")\n",
    "        \n",
    "        # Save next page URL before closing\n",
    "        next_page_url = None\n",
    "        if page_num < num_pages - 1:\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n",
    "                next_page_url = next_button.get_attribute(\"href\")\n",
    "                print(f\"✓ Next page URL saved\")\n",
    "            except NoSuchElementException:\n",
    "                print(\"⚠ No next page button found\")\n",
    "        \n",
    "        # Close browser to avoid detection\n",
    "        driver.quit()\n",
    "        print(\"✓ Closed listing page browser\")\n",
    "        \n",
    "        # Phase 2: Fetch descriptions in parallel\n",
    "        print(f\"\\nPhase 2: Fetching job descriptions ({max_workers} parallel threads)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_job = {\n",
    "                executor.submit(process_job_with_description, job_data, i, len(job_basics)): job_data \n",
    "                for i, job_data in enumerate(job_basics)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_job):\n",
    "                try:\n",
    "                    record = future.result()\n",
    "                    records.append(record)\n",
    "                except Exception as e:\n",
    "                    safe_print(f\"Error processing job: {e}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\n✓ Completed {len(job_basics)} jobs in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"  Average: {elapsed_time/len(job_basics):.2f} seconds per job\")\n",
    "        print(f\"\\n✓ Total jobs collected so far: {len(records)}\")\n",
    "        \n",
    "        # Create new driver for next page\n",
    "        if page_num < num_pages - 1 and next_page_url:\n",
    "            print(\"\\nPreparing for next page...\")\n",
    "            driver = create_driver()\n",
    "            time.sleep(random.randint(PAGE_SWITCH_MIN, PAGE_SWITCH_MAX))\n",
    "        elif page_num < num_pages - 1:\n",
    "            print(\"⚠ No next page available, stopping pagination\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd456f5",
   "metadata": {},
   "source": [
    "## Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c1ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRAPING COMPLETE\n",
      "================================================================================\n",
      "Total jobs collected: 16\n",
      "\n",
      "First 3 jobs preview:\n",
      "\n",
      "--- Job 1 ---\n",
      "Title: Principal Machine Learning Engineer - Ad Platforms\n",
      "Company: Disney Entertainment and ESPN Product & Technology\n",
      "Location: San Francisco, CA 94105 \n",
      "(Financial District area)\n",
      "Salary: \n",
      "Description: Technology is at the heart of Disney’s past, present, and future. Disney Entertainment and ESPN Product & Technology is a global organization of engin...\n",
      "\n",
      "--- Job 2 ---\n",
      "Title: Senior ML Compiler Engineer\n",
      "Company: General Motors\n",
      "Location: Remote in Mountain View, CA\n",
      "Salary: \n",
      "Description: Job Description\n",
      "Remote : This role is based remotely but if you live within a 50-mile radius of Mountain View, you are expected to report to that loca...\n",
      "\n",
      "--- Job 3 ---\n",
      "Title: Principal Machine Learning Engineer - AI Research\n",
      "Company: General Motors\n",
      "Location: Mountain View, CA\n",
      "Salary: \n",
      "Description: Job Description\n",
      "Our AI Research team, reporting directly to the Chief AI Officer, is pioneering how cutting-edge machine learning can transform the wa...\n",
      "\n",
      "... and 13 more jobs\n",
      "\n",
      "================================================================================\n",
      "Next step: Run save_data.ipynb to export your data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SCRAPING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total jobs collected: {len(records)}\")\n",
    "print(f\"\\nFirst 3 jobs preview:\")\n",
    "\n",
    "for i, record in enumerate(records[:3], 1):\n",
    "    print(f\"\\n--- Job {i} ---\")\n",
    "    print(f\"Title: {record[0]}\")\n",
    "    print(f\"Company: {record[1]}\")\n",
    "    print(f\"Location: {record[2]}\")\n",
    "    print(f\"Salary: {record[3]}\")\n",
    "    print(f\"Description: {record[5][:150]}...\" if len(record[5]) > 150 else f\"Description: {record[5]}\")\n",
    "\n",
    "if len(records) > 3:\n",
    "    print(f\"\\n... and {len(records) - 3} more jobs\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Next step: Run save_data.ipynb to export your data\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e888b7",
   "metadata": {},
   "source": [
    "## Store Records for Later Use\n",
    "# This saves the 'records' variable so you can access it in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fbfc13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'records' (list)\n",
      "\n",
      "✓ Stored 16 records in Jupyter storage\n",
      "You can now access this in save_data.ipynb using: %store -r records\n"
     ]
    }
   ],
   "source": [
    "%store records\n",
    "print(f\"\\n✓ Stored {len(records)} records in Jupyter storage\")\n",
    "print(\"You can now access this in save_data.ipynb using: %store -r records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
