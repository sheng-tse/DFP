{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3c8a91",
   "metadata": {},
   "source": [
    "# # Main Scraper\n",
    "# Execute the complete scraping workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d93130",
   "metadata": {},
   "source": [
    "## Load All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb496d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Test URL: https://www.indeed.com/jobs?q=data+analyst&l=New+York+NY\n",
      "✓ Utility functions loaded successfully\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 10s\n",
      "✓ Test URL: https://www.indeed.com/jobs?q=data+analyst&l=New+York+NY\n",
      "✓ Utility functions loaded successfully\n",
      "✓ Scraping functions loaded successfully\n",
      "  - get_job_basic_info()\n",
      "  - get_job_description()\n",
      "  - process_job_with_description()\n"
     ]
    }
   ],
   "source": [
    "%run config.ipynb\n",
    "%run utils.ipynb\n",
    "%run scraping_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863957c",
   "metadata": {},
   "source": [
    "## User Input Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69dd6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job search parameters\n",
    "job_title = input(\"Enter job title: \")\n",
    "city = input(\"Enter city: \")\n",
    "state = input(\"Enter state: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1528624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from page 1\n",
      "Will scrape 1 pages\n"
     ]
    }
   ],
   "source": [
    "# Pagination settings\n",
    "start_page_input = input(\"Enter starting page (0 for first page, 1 for second page, etc.): \")\n",
    "start_page = int(start_page_input) if start_page_input.strip() else 0\n",
    "print(f\"Starting from page {start_page + 1}\")\n",
    "\n",
    "pages_input = input(\"Enter number of pages to scrape (default 1): \")\n",
    "num_pages = int(pages_input) if pages_input.strip() else 1\n",
    "print(f\"Will scrape {num_pages} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d66d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 15 parallel threads\n"
     ]
    }
   ],
   "source": [
    "# Threading settings\n",
    "threads_input = input(f\"Enter number of parallel threads (default {DEFAULT_THREADS}, max {MAX_THREADS}): \")\n",
    "max_workers = int(threads_input) if threads_input.strip() else DEFAULT_THREADS\n",
    "max_workers = min(max_workers, MAX_THREADS)\n",
    "print(f\"Using {max_workers} parallel threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1e9d6",
   "metadata": {},
   "source": [
    "## Main Scraping Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d1829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search URL: https://www.indeed.com/jobs?q=Machine+Learning&l=san+jose+CA\n"
     ]
    }
   ],
   "source": [
    "# Generate initial URL\n",
    "url = get_url(job_title, city, state, start_page)\n",
    "print(f\"\\nSearch URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8b04e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup WebDriver\n",
    "driver = create_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149adb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRAPING PAGE 1\n",
      "================================================================================\n",
      "Found 16 jobs on page 1\n",
      "\n",
      "Phase 1: Collecting basic job information...\n",
      "  Collecting job 16/16...\n",
      "✓ Collected 16 job listings\n",
      "✓ Closed listing page browser\n",
      "\n",
      "Phase 2: Fetching job descriptions (15 parallel threads)...\n",
      "[Thread] Processing job 1/16: Senior Machine Learning Engineer at TETRAMEM INC\n",
      "[Thread] Processing job 2/16: Senior Machine Learning Engineer, Agentic at Robinhood\n",
      "[Thread] Processing job 3/16: Member of Technical Staff, Machine Learning Engineer at Reinforce Labs, Inc.\n",
      "[Thread] Processing job 4/16: Machine Learning Researcher / Engineer (Foundational Models) at Pathway\n",
      "[Thread] Processing job 5/16: Machine Learning Engineer, ML Runtime & Optimization at pony.ai\n",
      "[Thread] Processing job 6/16: Machine Learning Principal Architect at ASAP Talent Services\n",
      "[Thread] Processing job 7/16: Sr. Machine Learning - Compiler Engineer III, AWS Neuron, Annapurna Labs at Amazon.com Services LLC\n",
      "[Thread] Processing job 8/16: ML Engineer at Prospance Inc\n",
      "[Thread] Processing job 9/16: Mathematical Scientist at Pit.AI Technologies Inc.\n",
      "[Thread] Processing job 10/16:  at \n",
      "[Thread] Processing job 11/16: Machine Learning Research Scientist at Autoscience\n",
      "[Thread] Processing job 12/16: AI Agent Engineer at Tessera Labs\n",
      "[Thread] Processing job 13/16: Product Manager / Senior Product Manager, Machine Learning at Precision Neuroscience\n",
      "[Thread] Processing job 14/16: Forward-Deployed ML Engineer at WindBorne Systems\n",
      "[Thread] Processing job 15/16: Director, Machine Learning at Syntiant\n",
      "[Thread] Completed job 7/16: Sr. Machine Learning - Compiler Engineer III, AWS Neuron, Annapurna Labs | Salary: $151,300 - $261,500 a year\n",
      "[Thread] Processing job 16/16: Deep Learning Engineer at Matroid\n",
      "[Thread] Completed job 12/16: AI Agent Engineer | Salary: $200,000 - $230,000 a year\n",
      "[Thread] Completed job 6/16: Machine Learning Principal Architect | Salary: $215,000 - $265,000 a year\n",
      "[Thread] Completed job 8/16: ML Engineer | Salary: $70.78 - $80.15 an hour\n",
      "[Thread] Completed job 1/16: Senior Machine Learning Engineer | Salary: $110,000 - $300,000 a year\n",
      "[Thread] Completed job 15/16: Director, Machine Learning | Salary: $225,000 - $275,000 a year\n",
      "[Thread] Completed job 13/16: Product Manager / Senior Product Manager, Machine Learning | Salary: $150,000 - $200,000 a year\n",
      "[Thread] Completed job 3/16: Member of Technical Staff, Machine Learning Engineer | Salary: $130,000 - $200,000 a year\n",
      "[Thread] Completed job 5/16: Machine Learning Engineer, ML Runtime & Optimization | Salary: $140,000 - $250,000 a year\n",
      "[Thread] Completed job 11/16: Machine Learning Research Scientist | Salary: Full-time\n",
      "[Thread] Completed job 9/16: Mathematical Scientist | Salary: $150,000 a year\n",
      "[Thread] Completed job 2/16: Senior Machine Learning Engineer, Agentic | Salary: $187,000 - $220,000 a year\n",
      "[Thread] Completed job 4/16: Machine Learning Researcher / Engineer (Foundational Models) | Salary: $100,000 a year\n",
      "[Thread] Completed job 16/16: Deep Learning Engineer | Salary: $170,000 - $300,000 a year\n",
      "[Thread] Completed job 14/16: Forward-Deployed ML Engineer | Salary: $90,000 - $200,000 a year\n",
      "[Thread] Completed job 10/16:  | Salary: Not listed\n",
      "\n",
      "✓ Completed 16 jobs in 22.61 seconds\n",
      "  Average: 1.41 seconds per job\n",
      "\n",
      "✓ Total jobs collected so far: 16\n"
     ]
    }
   ],
   "source": [
    "# Store all job records\n",
    "records = []\n",
    "next_page_url = None\n",
    "\n",
    "try:\n",
    "    for page_num in range(num_pages):\n",
    "        current_page = start_page + page_num\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SCRAPING PAGE {current_page + 1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Navigate to URL\n",
    "        if page_num == 0:\n",
    "            driver.get(url)\n",
    "        else:\n",
    "            if next_page_url:\n",
    "                driver.get(next_page_url)\n",
    "            else:\n",
    "                url = get_url(job_title, city, state, current_page)\n",
    "                driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        time.sleep(random.randint(PAGE_LOAD_MIN, PAGE_LOAD_MAX))\n",
    "        WebDriverWait(driver, WEBDRIVER_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"job_seen_beacon\"))\n",
    "        )\n",
    "        \n",
    "        # Find all job postings\n",
    "        posts = driver.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
    "        print(f\"Found {len(posts)} jobs on page {current_page + 1}\")\n",
    "        \n",
    "        # Phase 1: Collect basic info quickly\n",
    "        print(\"\\nPhase 1: Collecting basic job information...\")\n",
    "        job_basics = []\n",
    "        for i, post in enumerate(posts):\n",
    "            print(f\"  Collecting job {i + 1}/{len(posts)}...\", end=\"\\r\")\n",
    "            basic_info = get_job_basic_info(post)\n",
    "            if basic_info:\n",
    "                job_basics.append(basic_info)\n",
    "        \n",
    "        print(f\"\\n✓ Collected {len(job_basics)} job listings\")\n",
    "        \n",
    "        # Save next page URL before closing\n",
    "        next_page_url = None\n",
    "        if page_num < num_pages - 1:\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n",
    "                next_page_url = next_button.get_attribute(\"href\")\n",
    "                print(f\"✓ Next page URL saved\")\n",
    "            except NoSuchElementException:\n",
    "                print(\"⚠ No next page button found\")\n",
    "        \n",
    "        # Close browser to avoid detection\n",
    "        driver.quit()\n",
    "        print(\"✓ Closed listing page browser\")\n",
    "        \n",
    "        # Phase 2: Fetch descriptions in parallel\n",
    "        print(f\"\\nPhase 2: Fetching job descriptions ({max_workers} parallel threads)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_job = {\n",
    "                executor.submit(process_job_with_description, job_data, i, len(job_basics)): job_data \n",
    "                for i, job_data in enumerate(job_basics)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_job):\n",
    "                try:\n",
    "                    record = future.result()\n",
    "                    records.append(record)\n",
    "                except Exception as e:\n",
    "                    safe_print(f\"Error processing job: {e}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\n✓ Completed {len(job_basics)} jobs in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"  Average: {elapsed_time/len(job_basics):.2f} seconds per job\")\n",
    "        print(f\"\\n✓ Total jobs collected so far: {len(records)}\")\n",
    "        \n",
    "        # Create new driver for next page\n",
    "        if page_num < num_pages - 1 and next_page_url:\n",
    "            print(\"\\nPreparing for next page...\")\n",
    "            driver = create_driver()\n",
    "            time.sleep(random.randint(PAGE_SWITCH_MIN, PAGE_SWITCH_MAX))\n",
    "        elif page_num < num_pages - 1:\n",
    "            print(\"⚠ No next page available, stopping pagination\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd456f5",
   "metadata": {},
   "source": [
    "## Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c1ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRAPING COMPLETE\n",
      "================================================================================\n",
      "Total jobs collected: 16\n",
      "\n",
      "First 3 jobs preview:\n",
      "\n",
      "--- Job 1 ---\n",
      "Title: Sr. Machine Learning - Compiler Engineer III, AWS Neuron, Annapurna Labs\n",
      "Company: Amazon.com Services LLC\n",
      "Location: Cupertino, CA 95014\n",
      "Salary: $151,300 - $261,500 a year\n",
      "Description: The Product: AWS Machine Learning accelerators are at the forefront of AWS innovation and one of several AWS tools used for building Generative AI on ...\n",
      "\n",
      "--- Job 2 ---\n",
      "Title: AI Agent Engineer\n",
      "Company: Tessera Labs\n",
      "Location: San Jose, CA\n",
      "Salary: $200,000 - $230,000 a year\n",
      "Description: Job Summary\n",
      "As we build to the next level, we’re looking for a top-quality AI engineer with a strong focus on AI agents - someone who knows how to lev...\n",
      "\n",
      "--- Job 3 ---\n",
      "Title: Machine Learning Principal Architect\n",
      "Company: ASAP Talent Services\n",
      "Location: Redwood City, CA\n",
      "Salary: $215,000 - $265,000 a year\n",
      "Description: Description\n",
      "Location: San Francisco, Bay Area, Melo Park\n",
      "ASAP Talent Services is a leading I.T. executive search firm. Our recruiting team has been re...\n",
      "\n",
      "... and 13 more jobs\n",
      "\n",
      "================================================================================\n",
      "Next step: Run save_data.ipynb to export your data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SCRAPING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total jobs collected: {len(records)}\")\n",
    "print(f\"\\nFirst 3 jobs preview:\")\n",
    "\n",
    "for i, record in enumerate(records[:3], 1):\n",
    "    print(f\"\\n--- Job {i} ---\")\n",
    "    print(f\"Title: {record[0]}\")\n",
    "    print(f\"Company: {record[1]}\")\n",
    "    print(f\"Location: {record[2]}\")\n",
    "    print(f\"Salary: {record[3]}\")\n",
    "    print(f\"Description: {record[5][:150]}...\" if len(record[5]) > 150 else f\"Description: {record[5]}\")\n",
    "\n",
    "if len(records) > 3:\n",
    "    print(f\"\\n... and {len(records) - 3} more jobs\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Next step: Run save_data.ipynb to export your data\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e888b7",
   "metadata": {},
   "source": [
    "## Store Records for Later Use\n",
    "# This saves the 'records' variable so you can access it in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbfc13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'records' (list)\n",
      "\n",
      "✓ Stored 16 records in Jupyter storage\n",
      "You can now access this in save_data.ipynb using: %store -r records\n"
     ]
    }
   ],
   "source": [
    "%store records\n",
    "print(f\"\\n✓ Stored {len(records)} records in Jupyter storage\")\n",
    "print(\"You can now access this in save_data.ipynb using: %store -r records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
