{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3c8a91",
   "metadata": {},
   "source": [
    "# # Main Scraper\n",
    "# Execute the complete scraping workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d93130",
   "metadata": {},
   "source": [
    "## Load All Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6cb496d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 60s\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 60s\n",
      "✓ Test URL: https://www.indeed.com/jobs?q=data+analyst&l=New+York+NY\n",
      "✓ Utility functions loaded successfully\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 60s\n",
      "✓ Configuration loaded successfully\n",
      "  - Default threads: 3\n",
      "  - Max threads: 15\n",
      "  - WebDriver timeout: 60s\n",
      "✓ Test URL: https://www.indeed.com/jobs?q=data+analyst&l=New+York+NY\n",
      "✓ Utility functions loaded successfully\n",
      "✓ Scraping functions loaded successfully\n",
      "  - get_job_basic_info()\n",
      "  - get_job_description()\n",
      "  - process_job_with_description()\n"
     ]
    }
   ],
   "source": [
    "%run config.ipynb\n",
    "%run utils.ipynb\n",
    "%run scraping_functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863957c",
   "metadata": {},
   "source": [
    "## User Input Section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69dd6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job search parameters\n",
    "job_title = input(\"Enter job title: \")\n",
    "city = input(\"Enter city: \")\n",
    "state = input(\"Enter state: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1528624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from page 1\n",
      "Will scrape 3 pages\n"
     ]
    }
   ],
   "source": [
    "# Pagination settings\n",
    "start_page_input = input(\"Enter starting page (0 for first page, 1 for second page, etc.): \")\n",
    "start_page = int(start_page_input) if start_page_input.strip() else 0\n",
    "print(f\"Starting from page {start_page + 1}\")\n",
    "\n",
    "pages_input = input(\"Enter number of pages to scrape (default 1): \")\n",
    "num_pages = int(pages_input) if pages_input.strip() else 1\n",
    "print(f\"Will scrape {num_pages} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "58d66d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 15 parallel threads\n"
     ]
    }
   ],
   "source": [
    "# Threading settings\n",
    "threads_input = input(f\"Enter number of parallel threads (default {DEFAULT_THREADS}, max {MAX_THREADS}): \")\n",
    "max_workers = int(threads_input) if threads_input.strip() else DEFAULT_THREADS\n",
    "max_workers = min(max_workers, MAX_THREADS)\n",
    "print(f\"Using {max_workers} parallel threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1e9d6",
   "metadata": {},
   "source": [
    "## Main Scraping Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "49d1829d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search URL: https://www.indeed.com/jobs?q=Data+Scientist&l=+\n"
     ]
    }
   ],
   "source": [
    "# Generate initial URL\n",
    "url = get_url(job_title, city, state, start_page)\n",
    "print(f\"\\nSearch URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e8b04e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup WebDriver\n",
    "driver = create_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "149adb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRAPING PAGE 1\n",
      "================================================================================\n",
      "Found 16 jobs on page 1\n",
      "\n",
      "Phase 1: Collecting basic job information...\n",
      "  Collecting job 16/16...\n",
      "✓ Collected 16 job listings\n",
      "✓ Next page URL saved\n",
      "✓ Closed listing page browser\n",
      "\n",
      "Phase 2: Fetching job descriptions (15 parallel threads)...\n",
      "[Thread] Processing job 1/16: Data Engineer/ Python/ ETL Developer at Combined Computer Resources\n",
      "[Thread] Processing job 2/16: Data Scientist at FanDuel\n",
      "[Thread] Processing job 3/16: Data Scientist II at AccuWeather Careers\n",
      "[Thread] Processing job 4/16: Data Scientist at Boosted.ai\n",
      "[Thread] Processing job 5/16: Data Scientist at Fulcrum Analytics\n",
      "[Thread] Processing job 6/16: Analytics Engineer at Virtuoso, Ltd.\n",
      "[Thread] Processing job 7/16: Applied Scientist – Research Products at Thomson Reuters\n",
      "[Thread] Processing job 8/16: Bioinformatics/Data Scientist at Axle\n",
      "[Thread] Processing job 9/16: Principal, Software Engineering at NIKE\n",
      "[Thread] Processing job 10/16: Data Scientist Advanced Development Program at Vanguard\n",
      "[Thread] Processing job 11/16:  at \n",
      "[Thread] Processing job 12/16: Machine Learning Scientist III - Causal Inference & CLV Strategy at Expedia Group\n",
      "[Thread] Processing job 13/16: Principal Associate, Data Scientist - US Card (New To Credit Team) at Capital One\n",
      "[Thread] Processing job 14/16: Signals Intelligence Analyst at Technosoft\n",
      "[Thread] Processing job 15/16: Research Engineer (Data Science) at Ataraxis AI\n",
      "[Thread] Completed job 7/16: Applied Scientist – Research Products | Salary: $108,500 - $201,500 a year\n",
      "[Thread] Processing job 16/16: Data Scientist at Cordia LLC\n",
      "[Thread] Completed job 10/16: Data Scientist Advanced Development Program | Salary: Not listed\n",
      "[Thread] Completed job 4/16: Data Scientist | Salary: $155,000 - $210,000 a year\n",
      "[Thread] Completed job 12/16: Machine Learning Scientist III - Causal Inference & CLV Strategy | Salary: $137,500 - $220,000 a year\n",
      "[Thread] Completed job 1/16: Data Engineer/ Python/ ETL Developer | Salary: $60 - $75 an hour\n",
      "[Thread] Completed job 6/16: Analytics Engineer | Salary: $115,000 - $130,000 a year\n",
      "[Thread] Completed job 3/16: Data Scientist II | Salary: Not listed\n",
      "[Thread] Completed job 13/16: Principal Associate, Data Scientist - US Card (New To Credit Team) | Salary: Full-time\n",
      "[Thread] Completed job 9/16: Principal, Software Engineering | Salary: $149,100 - $313,900 a year\n",
      "[Thread] Completed job 8/16: Bioinformatics/Data Scientist | Salary: $105,000 - $115,000 a year\n",
      "[Thread] Completed job 2/16: Data Scientist | Salary: $98,000 - $128,100 a year\n",
      "[Thread] Completed job 14/16: Signals Intelligence Analyst | Salary: From $87.03 an hour\n",
      "[Thread] Completed job 16/16: Data Scientist | Salary: $75,000 - $105,000 a year\n",
      "[Thread] Completed job 15/16: Research Engineer (Data Science) | Salary: $90,000 - $180,000 a year\n",
      "[Thread] Completed job 5/16: Data Scientist | Salary: $100,000 - $130,000 a year\n",
      "[Thread] Completed job 11/16:  | Salary: Not listed\n",
      "\n",
      "✓ Completed 16 jobs in 70.29 seconds\n",
      "  Average: 4.39 seconds per job\n",
      "\n",
      "✓ Total jobs collected so far: 16\n",
      "\n",
      "Preparing for next page...\n",
      "\n",
      "================================================================================\n",
      "SCRAPING PAGE 2\n",
      "================================================================================\n",
      "Found 16 jobs on page 2\n",
      "\n",
      "Phase 1: Collecting basic job information...\n",
      "  Collecting job 16/16...\n",
      "✓ Collected 16 job listings\n",
      "✓ Next page URL saved\n",
      "✓ Closed listing page browser\n",
      "\n",
      "Phase 2: Fetching job descriptions (15 parallel threads)...\n",
      "[Thread] Processing job 1/16: Data Science, Advisor at Peraton\n",
      "[Thread] Processing job 2/16: Staff Data Scientist (Director) at Atlas Air, Inc\n",
      "[Thread] Processing job 3/16: Sr. Data Scientist at Perfect Path, LLC, d/b/a Trajector Services\n",
      "[Thread] Processing job 4/16: Lead Engineer - Machine Learning at Macy’s\n",
      "[Thread] Processing job 5/16: Manager, Data Science - AI Foundations at Capital One\n",
      "[Thread] Processing job 6/16: Data Visualization Specialist /ERP Data Scientist at CGI Group, Inc.\n",
      "[Thread] Processing job 7/16: Staff Machine Learning Engineer – AI Research at General Motors\n",
      "[Thread] Processing job 8/16: Data Science Manager, Sales Analytics at Pharmavite\n",
      "[Thread] Processing job 9/16: Senior Cyber Data Scientist at Peraton\n",
      "[Thread] Processing job 10/16: Machine Learning Engineer at Apollo Professional Solutions, Inc.\n",
      "[Thread] Processing job 11/16: AI/ML Engineer (TS/SCI) {S} at ARKA Group, L.P.\n",
      "[Thread] Processing job 12/16:  at \n",
      "[Thread] Processing job 13/16: Director, Applied Generative AI at Bristol-Myers Squibb\n",
      "[Thread] Processing job 14/16: Sr. Transportation Data Analyst at Novolex\n",
      "[Thread] Processing job 15/16: Global Intelligence Analyst II at NIKE\n",
      "[Thread] Completed job 9/16: Senior Cyber Data Scientist | Salary: $176,000 - $282,000 a year\n",
      "[Thread] Processing job 16/16: Sr. Data Analyst at Virginia Farm Bureau\n",
      "[Thread] Completed job 15/16: Global Intelligence Analyst II | Salary: Full-time\n",
      "[Thread] Completed job 11/16: AI/ML Engineer (TS/SCI) {S} | Salary: $86,000 - $200,000 a year\n",
      "[Thread] Completed job 3/16: Sr. Data Scientist | Salary: $160,000 - $197,000 a year\n",
      "[Thread] Completed job 1/16: Data Science, Advisor | Salary: $146,000 - $234,000 a year\n",
      "[Thread] Completed job 4/16: Lead Engineer - Machine Learning | Salary: $151,400 - $252,100 a year\n",
      "[Thread] Completed job 8/16: Data Science Manager, Sales Analytics | Salary: $112,000 - $212,000 a year\n",
      "[Thread] Completed job 13/16: Director, Applied Generative AI | Salary: $192,253 - $251,602 a year\n",
      "[Thread] Completed job 7/16: Staff Machine Learning Engineer – AI Research | Salary: $198,000 - $250,000 a year\n",
      "[Thread] Completed job 2/16: Staff Data Scientist (Director) | Salary: $165,000 - $225,000 a year\n",
      "[Thread] Completed job 14/16: Sr. Transportation Data Analyst | Salary: $95,000 - $110,000 a year\n",
      "[Thread] Completed job 5/16: Manager, Data Science - AI Foundations | Salary: Full-time\n",
      "[Thread] Completed job 6/16: Data Visualization Specialist /ERP Data Scientist | Salary: $88,200 - $214,800 a year\n",
      "[Thread] Completed job 10/16: Machine Learning Engineer | Salary: $49 - $79 an hour\n",
      "[Thread] Completed job 16/16: Sr. Data Analyst | Salary: Full-time\n",
      "[Thread] Completed job 12/16:  | Salary: Not listed\n",
      "\n",
      "✓ Completed 16 jobs in 65.87 seconds\n",
      "  Average: 4.12 seconds per job\n",
      "\n",
      "✓ Total jobs collected so far: 32\n",
      "\n",
      "Preparing for next page...\n",
      "\n",
      "================================================================================\n",
      "SCRAPING PAGE 3\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.65)\nStacktrace:\n0   chromedriver                        0x0000000102bcb598 cxxbridge1$str$ptr + 2894960\n1   chromedriver                        0x0000000102bc34d4 cxxbridge1$str$ptr + 2861996\n2   chromedriver                        0x00000001026e95ec _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74324\n3   chromedriver                        0x00000001026c3198 chromedriver + 143768\n4   chromedriver                        0x00000001027593fc _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 532580\n5   chromedriver                        0x0000000102771fb8 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633888\n6   chromedriver                        0x0000000102725178 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318944\n7   chromedriver                        0x0000000102b8f2e4 cxxbridge1$str$ptr + 2648508\n8   chromedriver                        0x0000000102b928c0 cxxbridge1$str$ptr + 2662296\n9   chromedriver                        0x0000000102b6fd64 cxxbridge1$str$ptr + 2520124\n10  chromedriver                        0x0000000102b931a8 cxxbridge1$str$ptr + 2664576\n11  chromedriver                        0x0000000102b614d4 cxxbridge1$str$ptr + 2460588\n12  chromedriver                        0x0000000102bb2b94 cxxbridge1$str$ptr + 2794092\n13  chromedriver                        0x0000000102bb2d18 cxxbridge1$str$ptr + 2794480\n14  chromedriver                        0x0000000102bc3120 cxxbridge1$str$ptr + 2861048\n15  libsystem_pthread.dylib             0x0000000188308c08 _pthread_start + 136\n16  libsystem_pthread.dylib             0x0000000188303ba8 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchWindowException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Wait for page to load\u001b[39;00m\n\u001b[32m     23\u001b[39m time.sleep(random.randint(PAGE_LOAD_MIN, PAGE_LOAD_MAX))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWEBDRIVER_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mEC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjob_seen_beacon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Find all job postings\u001b[39;00m\n\u001b[32m     29\u001b[39m posts = driver.find_elements(By.CLASS_NAME, \u001b[33m\"\u001b[39m\u001b[33mjob_seen_beacon\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/DFP/ipynb_files/env/lib/python3.13/site-packages/selenium/webdriver/support/wait.py:129\u001b[39m, in \u001b[36mWebDriverWait.until\u001b[39m\u001b[34m(self, method, message)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         value = \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[32m    131\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/DFP/ipynb_files/env/lib/python3.13/site-packages/selenium/webdriver/support/expected_conditions.py:104\u001b[39m, in \u001b[36mpresence_of_element_located.<locals>._predicate\u001b[39m\u001b[34m(driver)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/DFP/ipynb_files/env/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:926\u001b[39m, in \u001b[36mWebDriver.find_element\u001b[39m\u001b[34m(self, by, value)\u001b[39m\n\u001b[32m    923\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby.root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43musing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/DFP/ipynb_files/env/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:458\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    455\u001b[39m response = cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m.command_executor).execute(driver_command, params)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/DFP/ipynb_files/env/lib/python3.13/site-packages/selenium/webdriver/remote/errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mNoSuchWindowException\u001b[39m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=141.0.7390.65)\nStacktrace:\n0   chromedriver                        0x0000000102bcb598 cxxbridge1$str$ptr + 2894960\n1   chromedriver                        0x0000000102bc34d4 cxxbridge1$str$ptr + 2861996\n2   chromedriver                        0x00000001026e95ec _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74324\n3   chromedriver                        0x00000001026c3198 chromedriver + 143768\n4   chromedriver                        0x00000001027593fc _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 532580\n5   chromedriver                        0x0000000102771fb8 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 633888\n6   chromedriver                        0x0000000102725178 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318944\n7   chromedriver                        0x0000000102b8f2e4 cxxbridge1$str$ptr + 2648508\n8   chromedriver                        0x0000000102b928c0 cxxbridge1$str$ptr + 2662296\n9   chromedriver                        0x0000000102b6fd64 cxxbridge1$str$ptr + 2520124\n10  chromedriver                        0x0000000102b931a8 cxxbridge1$str$ptr + 2664576\n11  chromedriver                        0x0000000102b614d4 cxxbridge1$str$ptr + 2460588\n12  chromedriver                        0x0000000102bb2b94 cxxbridge1$str$ptr + 2794092\n13  chromedriver                        0x0000000102bb2d18 cxxbridge1$str$ptr + 2794480\n14  chromedriver                        0x0000000102bc3120 cxxbridge1$str$ptr + 2861048\n15  libsystem_pthread.dylib             0x0000000188308c08 _pthread_start + 136\n16  libsystem_pthread.dylib             0x0000000188303ba8 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "# Store all job records\n",
    "records = []\n",
    "next_page_url = None\n",
    "\n",
    "try:\n",
    "    for page_num in range(num_pages):\n",
    "        current_page = start_page + page_num\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SCRAPING PAGE {current_page + 1}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Navigate to URL\n",
    "        if page_num == 0:\n",
    "            driver.get(url)\n",
    "        else:\n",
    "            if next_page_url:\n",
    "                driver.get(next_page_url)\n",
    "            else:\n",
    "                url = get_url(job_title, city, state, current_page)\n",
    "                driver.get(url)\n",
    "        \n",
    "        # Wait for page to load\n",
    "        time.sleep(random.randint(PAGE_LOAD_MIN, PAGE_LOAD_MAX))\n",
    "        WebDriverWait(driver, WEBDRIVER_TIMEOUT).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"job_seen_beacon\"))\n",
    "        )\n",
    "        \n",
    "        # Find all job postings\n",
    "        posts = driver.find_elements(By.CLASS_NAME, \"job_seen_beacon\")\n",
    "        print(f\"Found {len(posts)} jobs on page {current_page + 1}\")\n",
    "        \n",
    "        # Phase 1: Collect basic info quickly\n",
    "        print(\"\\nPhase 1: Collecting basic job information...\")\n",
    "        job_basics = []\n",
    "        for i, post in enumerate(posts):\n",
    "            print(f\"  Collecting job {i + 1}/{len(posts)}...\", end=\"\\r\")\n",
    "            basic_info = get_job_basic_info(post)\n",
    "            if basic_info:\n",
    "                job_basics.append(basic_info)\n",
    "        \n",
    "        print(f\"\\n✓ Collected {len(job_basics)} job listings\")\n",
    "        \n",
    "        # Save next page URL before closing\n",
    "        next_page_url = None\n",
    "        if page_num < num_pages - 1:\n",
    "            try:\n",
    "                next_button = driver.find_element(By.CSS_SELECTOR, \"a[data-testid='pagination-page-next']\")\n",
    "                next_page_url = next_button.get_attribute(\"href\")\n",
    "                print(f\"✓ Next page URL saved\")\n",
    "            except NoSuchElementException:\n",
    "                print(\"⚠ No next page button found\")\n",
    "        \n",
    "        # Close browser to avoid detection\n",
    "        driver.quit()\n",
    "        print(\"✓ Closed listing page browser\")\n",
    "        \n",
    "        # Phase 2: Fetch descriptions in parallel\n",
    "        print(f\"\\nPhase 2: Fetching job descriptions ({max_workers} parallel threads)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_job = {\n",
    "                executor.submit(process_job_with_description, job_data, i, len(job_basics)): job_data \n",
    "                for i, job_data in enumerate(job_basics)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_job):\n",
    "                try:\n",
    "                    record = future.result()\n",
    "                    records.append(record)\n",
    "                except Exception as e:\n",
    "                    safe_print(f\"Error processing job: {e}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\n✓ Completed {len(job_basics)} jobs in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"  Average: {elapsed_time/len(job_basics):.2f} seconds per job\")\n",
    "        print(f\"\\n✓ Total jobs collected so far: {len(records)}\")\n",
    "        \n",
    "        # Create new driver for next page\n",
    "        if page_num < num_pages - 1 and next_page_url:\n",
    "            print(\"\\nPreparing for next page...\")\n",
    "            driver = create_driver()\n",
    "            time.sleep(random.randint(PAGE_SWITCH_MIN, PAGE_SWITCH_MAX))\n",
    "        elif page_num < num_pages - 1:\n",
    "            print(\"⚠ No next page available, stopping pagination\")\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd456f5",
   "metadata": {},
   "source": [
    "## Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1ec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SCRAPING COMPLETE\n",
      "================================================================================\n",
      "Total jobs collected: 48\n",
      "\n",
      "First 3 jobs preview:\n",
      "\n",
      "--- Job 1 ---\n",
      "Title: Data Analyst\n",
      "Company: CGI Group, Inc.\n",
      "Location: Arlington, VA 22201 \n",
      "(Bluemont area)\n",
      "Salary: $50,800 - $119,200 a year\n",
      "Description: Position Description:\n",
      "This is an exciting full-time opportunity to work in a fast-paced environment with a team of passionate technologists. We take a...\n",
      "\n",
      "--- Job 2 ---\n",
      "Title: Data Governance Foundation Senior Analyst, AVP (Hybrid)\n",
      "Company: Citi\n",
      "Location: Tampa, FL 33601\n",
      "Salary: $87,280 - $130,920 a year\n",
      "Description: The Data Governance Foundation Senior Analyst is responsible for contributing to compliance of Citi Data Governance Policy with a focus on Data Concer...\n",
      "\n",
      "--- Job 3 ---\n",
      "Title: Associate eCommerce Data Analyst\n",
      "Company: Uline\n",
      "Location: Pleasant Prairie, WI 53158\n",
      "Salary: Full-time\n",
      "Description: Associate eCommerce Data Analyst\n",
      "Corporate Headquarters\n",
      "12575 Uline Drive, Pleasant Prairie, WI 53158\n",
      "As one of the largest e-commerce websites in the...\n",
      "\n",
      "... and 45 more jobs\n",
      "\n",
      "================================================================================\n",
      "Next step: Run save_data.ipynb to export your data\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SCRAPING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total jobs collected: {len(records)}\")\n",
    "print(f\"\\nFirst 3 jobs preview:\")\n",
    "\n",
    "for i, record in enumerate(records[:3], 1):\n",
    "    print(f\"\\n--- Job {i} ---\")\n",
    "    print(f\"Title: {record[0]}\")\n",
    "    print(f\"Company: {record[1]}\")\n",
    "    print(f\"Location: {record[2]}\")\n",
    "    print(f\"Salary: {record[3]}\")\n",
    "    print(f\"Description: {record[5][:150]}...\" if len(record[5]) > 150 else f\"Description: {record[5]}\")\n",
    "\n",
    "if len(records) > 3:\n",
    "    print(f\"\\n... and {len(records) - 3} more jobs\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Next step: Run save_data.ipynb to export your data\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e888b7",
   "metadata": {},
   "source": [
    "## Store Records for Later Use\n",
    "# This saves the 'records' variable so you can access it in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfc13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'records' (list)\n",
      "\n",
      "✓ Stored 16 records in Jupyter storage\n",
      "You can now access this in save_data.ipynb using: %store -r records\n"
     ]
    }
   ],
   "source": [
    "%store records\n",
    "print(f\"\\n✓ Stored {len(records)} records in Jupyter storage\")\n",
    "print(\"You can now access this in save_data.ipynb using: %store -r records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
